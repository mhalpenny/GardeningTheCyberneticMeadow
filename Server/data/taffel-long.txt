


    Perhaps the most commonly encountered application of scale derives
from cartography, whereby the term refers to the reduction of space by a
numerical factor, enabling vast areas to be graphically represented upon
maps; one centimetre represents a specific distance, denoting that within
cartography scale indicates a direct and quantifiable relationship which
exists between map and territory. This is also the sense of scale as size that
is employed in the notion of scalability or scaling up, whereby tech start-ups
seek to dramatically increase their size without effecting wider structural
changes, a concept that is usefully critiqued by Anna Tsing  as
originating in the abstract world of mathematics but which applies poorly
to the complex and specific ecological world.
    Contrastingly, we find relational approaches to scale, such as its
employment within the geologic time scale (GTS), which divides the earth’s
history into nested chronological units that are not based upon equal
temporal durations but indicate alterations to geological strata corresponding
to significant geological or palaeontological events (Gradstein et al. .
For example, the boundary between the Permian and Triassic periods is the
Permian–Triassic extinction event, the most severe mass-extinction event
found within the geological record, which denotes an abrupt change in
the fossil record. What is important to grasp about the GTS, then, is that
scale does not demarcate a linear series of measurements but particular
relational aspects between neighbouring periods. A related application of
scale as defining relations between entities can be found within music. The
difference between a major and minor scale may only be a single note, but
this relatively small alteration between the set of notes being played is heard
and understood as comprising a significant shift in qualities of the acoustic
arrangement. Scale in this case is not about quantitative change but refers to
the configuration of relationships between actors within sets, and grasping
this difference between scale as quantitatively measurable size and scale as
relational configuration is key to grasping the way which scale is used here
to explore digital media.
    Scale has been heavily discussed within cultural geography, a discipline in
which the cartographic connotations of scale as size have for a considerable
time existed in tension alongside relational variants closer to the application
of scale in the GTS. There has, however, been considerable debate over the
precise meaning of scale within cultural geography, ranging from Richard
Howitt’s  , .suggestion that scale is a foundational disciplinary
concept alongside space, place and culture, through to claims that scale
presents a highly problematic concept which should be eliminated from
geographical discourse (Marston, Jones and Woodward . Marston
and colleagues apply theoretical insights from Deleuze and Bruno Latour
surrounding a flat ontology to argue that scale presents ontological
divisions, which, while adding nuance to a global/local distinction through
the addition of intermediaries such as the national, are ultimately founded
  


upon verticalism and hierarchy, thus inhibiting network-led approaches to
micropolitics. Consequently, they contend that ‘scale is a classic case of form
determining content, whereby objects, events and processes come pre-sorted,
ready to be inserted into the scalar apparatus at hand’ (Marston, Jones and
Woodward  23). This approach to scale, however, concentrates on
scale as size, whereas scale as relation is entirely absent.
   Additionally, this critique presupposes that scale addresses ontological
rather than epistemological phenomena, whereas various alternative
perspectives within cultural geography emphasize scale as an epistemological
tool rather than a series of ontological divisions (Hoefle  Moore .
Instead of representing rigid striations between hierarchically nested layers
of reality, ‘scale itself is a representational trope, a way of framing political
spatiality that in turn has material effects’ (Jones . There seems to be
no reason why an ontological framework whereby existence is comprised
of a single plane of immanence is incompatible with an epistemology which
explores phenomena across a range of relational scales, presenting numerous
overlapping apertures with which to comprehend entangled phenomena.
Indeed, Guattari’s framework of the three ecologies, derived from the
environmental/cybernetic epistemology of Gregory Bateson, exemplifies
such an approach whereby scale is understood as a relational quality
between elements of ecological systems occurring within a single ontological
register. Consequently, Guattari  2) argues that despite the apparent
separation of these relational scales, it is ‘quite wrong to make a distinction
between action on the psyche, the socius and the environment’. Here, scalar
ecologies are applied to provide overlapping modes of inquiry into entangled
phenomena, rather than hierarchically determined ontological divisions.
   Homologous to the three entangled ecologies of mind, society and
environment proposed by Bateson and Guattari, the digital entanglements
explored in this book focuses upon three relational scales of content,
software and hardware. While ultimately, digital content and software both
exist as magnetized regions or transistor states, and certain types of digital
content such as computer games are only operationalized as executable
code, the difference I draw between these entangled scales revolves around
the way they are encountered, hence the emphasis on their existence as
epistemological rather than ontological differences. Content is a scale
at which end users encounter digital media, it is the part of the media
assemblage that is most visible because it is designed to be experienced by
humans. Code is a scale at which there are encounters between humans and
machines. As we shall see, there are many levels of code, some of which
(such as binary) are not human-readable, but a key characteristic here is the
way that machines can be programmed to execute algorithms – step-by-step
instructions – that allow the automation of decision-making processes based
upon rules. While humans are today in almost constant contact with various
forms of code, this process is often invisible and unfelt. Finally, hardware is
1


the scale that addresses the material assemblages of silicon, copper, plastic
and countless other substances that afford the construction of code and
content. While digital infrastructures such as transoceanic fibre-optic cables,
satellite networks and data centres are often vast in size, the proliferation
of miniaturized thin client devices such as smartphones means that they are
rarely perceived and poorly understood.
   This is not to suggest that these are the only scales at which media
systems operate; we could, for example, equally think about media existing
through the scales of spectrum (Wi-Fi, WiMAX, Satellites, GPS, cellular
networks), earths (undersea and underground cables, mining metals and
minerals and fossil fuels) and users (user interfaces, screens, haptic devices,
cyborgs), or the layers of earth, cloud, cities, address, platform, interface
and users, proposed in Benjamin Bratton’s The Stack . My selection
of scales of content, code and hardware is partially designed to address
recent calls in post-Marxist critical theory for open digital infrastructures at
the levels of information, software and hardware (Hardt and Negri .
As with Bateson and Guattari’s three ecologies, these scales are not distinct
and separable layers, but entangled meshworks that cannot be functionally
isolated from one another. The utility in exploring them as scales lies in
grasping the differing types of relation highlighted by each aperture. While it
would be wholly wrong to ascribe particular qualities in a singular fashion
to each scale, such as claiming that meaning derives solely from content
and materiality entirely relates to hardware, there are important differences
in the way that particular relations manifest within different scales. As we
shall see, there are different materialities, agencies, meanings and power
relations that exist within images and sounds, algorithms and codecs, CPU
architectures and fibre-optic cables, and grasping how these differences
operate across and between entangled relational scales is an important task
for a political ecology of digital media.


        Digital infrastructures and inequalities
Politically and ethically orientated approaches to media and technology are
by no means new. My argument, however, is that the contemporary situation
requires a reappraisal of how we approach technocultural systems from
ethical and political perspectives. The demarcation that has traditionally
been drawn between ethics and politics is largely one of individual and
collective action, with ethics referring to a system for understanding positive
and negative behaviours, whereas politics denotes the collective actions
taken to enact ethical action. As we shall see, this is somewhat complicated
by an ecological approach to agency which blurs the boundaries between
individual and collective practices, decentring the traditional notion of the
unified subject into an evolving assemblage which is always composed of
  


multiple forms of organic and nonorganic life which escape the boundaries
of the skin, but which is still concerned with purposive collaborative action
and large-scale social and environmental change. Consequently, the line
between ethics and politics dissolves into thinking about how to enact
collective behaviours with ecologically beneficial impacts.
   The technocultural context for the book is composed of both the
backdrop of the Anthropocene and the ongoing changes to information
and communications technologies. The massive increase in volume, speed
and interconnection of information and communications technologies have
arguably formed a new stage of capitalist development, variously described
as informational capitalism, empire, cognitive capitalism, integrated
world capitalism, globalization, late capitalism, platform capitalism and
the network society. Although these concepts do not precisely describe a
homogeneous system of relations and structures, they present a series of
overlapping frameworks which examine differing aspects of contemporary
society.
   During this period, the cost of processing information using transistor-
based computers, fell from over US$18 billion for one Gigaflop (one billion
floating point operations per second) in to cents by , the
internet has grown from a handful of nodes to having several billion users,
global internet traffic has grown from 0.01 petabytes per year in to
 , ,  petabytes per year in (Press . and mobile phone
usage has risen from a handful of corporate executives to the majority of
the global population. Spanish sociologist Manuel Castells argues that in
this context

  electronic media have become the privileged space of politics. Not that all
  politics can be reduced to images, sounds, or symbolic manipulation. But,
  without it, there is no chance of winning or exercising power. … Because
  of the convergent effects of the crisis of traditional political systems and
  of the dramatically increased pervasiveness of the new media, political
  communication and information are essentially captured in the space of
  media. Outside the media sphere there is only marginality. (Castells :
   / )

These claims are supported by empirical evidence delineating the frequency
of media usage, such as Ofcom  8) finding that between and
 , sixteen- to twenty-four-year-olds have on average gone from spending
ten to twenty-eight hours a week online.
   It is important, however, to recognize that it is not merely the frequency of
our engagements with media that has changed over this period. As mobile,
locative and pervasive computing has been increasingly ubiquitous, our lives
have become increasingly bound up with the process of digital mediation.
From movements through space that are recorded via GPS and feed into
1


recommendation algorithm systems such as Google Now and governmental
surveillance systems such as the NSA’s Co-Traveller, to our heart rates and
quantities of sleep that are now measured and recorded by wearable devices
such as the Apple Watch and Fitbit bands, our lives have increasingly become
mediated.
   Rhetorics of the quantified self, smart home and smart city illustrate
that this mediatization of life permeates personal, domestic and social
scales, that this increased level of digitization and quantification cuts across
diverse domains (Greenfield . Following David Berry , we can
understand that (post)human existence has become increasingly enmediated;
that whereas twenty years ago media predominantly happened in particular
places and at specific times, our lives are now fundamentally bound up with
media. In this hegemonic technocultural space, approaching the political
ecology of digital culture becomes an important task for those interested in
processes of social change precisely because of the centrality of mediation to
the biopolitics of the twenty-first century.
   Understanding entanglements of technology, culture and mediation
requires ethical and political analyses that go beyond focusing upon the content
of mediated communications, additionally examining the infrastructures of
software and hardware upon which digital communications are predicated.
Most existing approaches to media ethics (e.g. Ess .focus upon issues
such as pornography, propaganda and representations of violence – all
of which are serious issues, but ones that primarily relate to the content
of media, its final communicational outputs. A political ecology of media
must additionally consider the relations that are embedded within and
propagated by the infrastructures which support the production of content
– the code, algorithms and programs which exist at the scale of software,
and the components, computers, cabling, cell towers and other entities
which comprise the scale of hardware. As Susan Leigh-Star  79)
explains, ‘Study an information system and neglect its standards, wires, and
settings and you miss essential aspects of aesthetics, justice and change.’ As
such, this mode of inquiry takes up the challenge posited by McKenzie Wark
 20), who contends that in the face of the increasingly algorithmic
and digital modes of social and ecological precarity, ‘the whole question of
what now constitutes an infrastructure must be revisited’.
   Built into the systems of software and hardware are a multiplicity of
affects, ethics and values which are frequently rendered invisible by
discourses surrounding virtuality, informationalization and immaterial
labour which surround ICTs. One of the aims of this text is to present
examples of how and where these ecological impacts arise in order to render
them visible and de-naturalized, while considering techniques and strategies
which present eco-ethical alternatives. As such, this book has affinities with
recent important recent works surrounding the materiality of hardware
(Gabrys  Maxwell and Miller  Parikka a; Starosielski
  


 ; Cubitt  Rossiter . while arguing that hardware should
be approached systemically alongside software and content. Rather than
presenting a materialist account of media as an alternative to discursive
approaches, media ecology is interested in the ways that discourse and
materiality are themselves entangled rather than oppositional. As we
shall see, infrastructures affect discourses which in turn feed back into
technological architectures. Whereas for German media theorist Friedrich
Kittler  there is no such thing as software (or for that matter digital
content), as ultimately all code exists as voltage differences, media ecology
contends that there are divergent affective and political consequences which
result from examining media systems across differing scales.
   We must also bear in mind that outside the preferentially connected nodes
of the network society, the consumer-led media society which is often taken
to be universal is still far from pervasive or homogenous. In , global
internet users exceeded four billion, meaning that over half of humanity
had internet access, but the forty-something per cent with no access who
are now, for the first time, a global minority should not be forgotten. While
there has been continuing growth in internet usage, particularly among users
of mobile broadband since , this must be understood within a context
of enduring social and economic divisions. According to the International
Telecommunications Union , while over per cent of individuals in
the developed world have internet access, this falls to around per cent
in developing nations, and just per cent in the least developed nations.
While these figures present a stark reminder that we must resist the cyber-
utopian ideology of digital telecommunications as affording a globalized
and universal realm, we should also be wary of addressing the digital divide
as a binary opposition between the connected and disconnected, insofar
as such accounts problematically assume that all modalities of access are
equal, reducing a series of complex sociotechnical issues to a simple on/off
dualism (Cisler  Warschauer  . In developing nations, where
cellular connectivity is fast outstripping the growth of fixed-line broadband,
the type of connectivity afforded by mobile phones is far removed from
the professional digital creation and consumption tools provided by some
other contemporary computing technologies. Equally, bandwidth is hugely
variable between and within nations, and this determines the types of
engagement that are possible.
   Digital divisions can only be understood within wider social contexts;
alongside digital divisions between rich and poor across and within nation-
states, there are corresponding health care divisions, education divisions,
human rights divisions and labour rights divisions, all of which require
attention if social justice is to be addressed. Consequently, the approach
to digital technoculture outlined here not only explores ways that media
systems affect those who are preferentially connected within the network
society but also considers how this technocultural system affects either
1


those disconnected from the communicational outputs of the information
revolution or those who have only minimal modes of connectivity. This
connects media ecology to key insights that arise from the political economy
of digital culture surrounding inequality and exploitation (Fuchs .
What emerges is a range of forms of exploitation, which often either echo
or update the dynamics of colonialism.


                         Chapter outlines
This book is organized into two parts. ‘Part 1: Theorising Media Ecologies’
is composed of two chapters that provide the conceptual background for
my reimagined approach to media ecology. Chapter begins by exploring
debates surrounding technology, media and agency. The central questions
addressed here are who and what can act, and how do these agential
capacities play out within digitally mediated systems. These questions are
explored through the example of algorithmic financial trading and the Flash
Crash of , which becomes a point of departure for considering how
concepts derived from cybernetics, complexity theory, self-organization and
ecology provide a conception of agency based upon assemblages rather than
individuals. The chapter concludes by following Sean Cubitt’s suggestion
that we reconsider our conceptualization of cybernetic organisms, viewing
corporations and financial systems as cyborgian assemblages that situate
human nodes within inhuman and destructive networks whose logic is
centred upon short-term economic profitability.
   Whereas Chapter focuses upon questions of how agency manifests
throughout technocultural systems, Chapter explores the values
associated with an ecological ethic and how this system may be mobilized
as an ecological politics. Such an ethic departs from the anthropocentrism
of deontology and consequentialism – the two major branches of Western
ethics – instead foregrounding ethics as something which is enacted rather
than merely theorized. A key part of the ethical model outlined here is
derived from the works of Gregory Bateson and Felix Guattari surrounding
the three entangled ecologies of mind, society and environment. The chapter
also explores several concepts generated through Guattari's collaborations
with Gilles Deleuze and contemporary works these concepts have inspired
which explore commons-based production in peer-to-peer systems and
ecological approaches to value.
   The theoretical focus upon the logic of the AND, assemblages, scale and
entanglement found in Part informs the structure of Part 2, ‘Ecologies of
Content Code and Hardware’, which is comprised of three chapters that in
turn consider content, software and hardware as entangled sites of conflict
within digital ecologies. Each of these chapters does not use the aperture
afforded by a single, unifying case study, instead they construct assemblages
  


which employ numerous examples that work across scales. Doing so is
designed to apply a logic of the AND that allows multiple converging and
diverging lines of conflict, hierarchy and resistance to be drawn out and
examined using the conceptual tools that are outlined within Part 1.
   Chapter examines flows of attention and information within the scale
of content. This is approached by examining phenomena ranging from
the attention economy and big data, through to particular responses on
climate blogs to the Climategate scandal. What emerges from this material
are questions surrounding how, why and if change occurs in response to
activist endeavour within the framework of what Jodi Dean has termed
‘communicative capitalism’. Critical insights taken from this chapter
include ways that the reception of networked media departs from rational–
critical approaches such as those associated with the public sphere, with
meaning instead often deriving from a combination of affect, technicity,
cognitive frames and economies of attention. The latter term is of particular
pertinence in mapping some of the novel forms of hierarchy which come
into being through the formations of networked digital media, which are
of central importance in considering what types of activist intervention are
likely to draw lines of flight towards other systemic configurations.
   Chapter focuses upon the ecology of software, exploring the discourses
and approaches associated with software studies and free and open source
software, before exploring a range of agential, ethical and political conflicts
which arise through practices ranging from jailbreaking Apple’s iOS
devices and firmware hacking Canon EOS digital cameras, to digital rights
management software, and the personalization algorithms used by search
engines and social media. The two key themes which recur across these
examples pertain to questions of control, openness and surveillance on the
one hand, and on the other foreground elements of nonhuman agency within
software, as evidenced by unplanned actions surrounding the behaviours of
search engine crawlers and security issues surrounding bugs and exploits
used to compromise, hack or otherwise alter computational systems.
   In Chapter 5, I examine a range of issues surrounding the life cycle of digital
hardware. The cases explored include the procurement of conflict minerals
for microelectronics, the manufacture of devices in Chinese sweatshops,
the carbon footprint of creating and powering the ever-expanding array of
digital media hardware, and their end-of-life as highly toxic e-waste which
is often manually treated by impoverished workers who unwittingly poison
themselves and local ecosystems. At every juncture within this process, the
chapter explores how various practices – which often involve the usage of
the offending microelectronics devices themselves – are being applied to
reduce the current impacts of these technologies. The chapter concludes by
considering how cradle-to-cradle production and open source hardware
may suggest radically different models for how we produce and consume
hardware, alongside how these models are being reterritorialized.
  

   Chapter provides initially presents two cases which traverse the
entangled scales of content, software and hardware, the mobile game Phone
Story and the Open Source Ecology project, that draw together some of the
key insights from the previous chapters. This is followed by some conclusions
that addresses the novel forms of hierarchy and reterritorialization that have
seen digital culture become synonymous with a handful of multibillion-
dollar corporations, alongside how we may enact ecological praxis and a
biopolitics of media in the Anthropocene.
   Re-thinking media ecology in an increasingly pervasive, digital culture is
important because it foregrounds the entanglements between two of the core
challenges that technocultures face in the twenty-first century: the politics
of technology and Anthropocenic ecological crises. Without concerted
efforts, the near future will see catastrophic climate change accompanied
by a vast reduction in biodiversity faced by a technoculture which
erects walls and boundaries around economic elites while perpetuating
systems of colonial, racial and economic privilege. Reformulating media
ecology is a way of trying to think through these challenges, of finding
ways of building commonwealth and democratically accountable public
infrastructure rather than commodities, of addressing the toxic drive
towards short-term profitability rather than ecological sustainability and
resilience, and dramatically reducing inequalities and hierarchies rather
than strengthening them.
  

Theorizing digital
   ecologies
                   and agency



On May , the US equity market experienced an extreme and novel
form of turbulence. Commonly known as the ‘Flash Crash’, around US$ 
billion was wiped off the value of stocks between 2:32 and 2:45 p.m., with
the vast majority of the fall taking place between 2:41 and 2:45. After trading
in Standard and Poor (S&P) E-Mini futures was paused for seconds, the
market began to recover, with over two-thirds of losses being recovered
by the close of the day. In the wake of this event, media coverage focused
on speculation surrounding the role that computer-based trading played in
the Flash Crash, how this new class of digital actor had demonstrated the
ability to impact financial systems in unpredictable ways that could lead to
catastrophic economic consequences.
    In this chapter, I will use the events of the Flash Crash as a way of examining
a range of questions surrounding technology, agency and complexity. The
concept of agency addresses who and what can and does act, how these
actions impact upon other entities, and how these agencies are distributed
between individuals and collectives, humans and nonhumans. While agency
has traditionally been invoked as a purely human and individual affair, it is
approached here as a distributed capacity that can never solely be attributed
to a single, isolated entity. Instead, agencies are understood as unstable,
relational and multiple rather than as the expression of an individual’s will.
In order to formulate an ecological model of mediation and political action,
addressing questions of how agency is distributed between human, organic
and technical systems is a necessary pre-requisite; conceptualizing how to
mobilize and enact change in response to ecological crises requires us to
first consider how agencies coalesce, cascade and erupt within complex,
nonlinear systems.
  

    The chapter begins by outlining the key actors involved in the Flash Crash,
and how their collective interactions shaped this event. This then leads to
a discussion of several concepts and genealogies that can help us to create
a map of how agency functions within complex, dynamical technocultural
systems. This begins by thinking through the multiple legacies of cybernetics,
which includes the tropes of information theory, feedback and nonlinearity.
I then move on to discuss several interrelated system theories that have been
influenced by cybernetics: autopoiesis, complexity theory, and Deleuze and
Guattari’s concepts of assemblages and abstract machines, all of which assist
in refining an ecological model of agency. Finally, I conclude the chapter
by employing discourses of cyborgs and posthumanism to consider the
inhuman agencies and politics of computer-based trading.


                       Algorithmic trading
To grasp what happened in the Flash Crash of , we must first become
familiar with the main entities that were involved in the event. This means
exploring several types of computer-based trading systems, alongside
structural changes that have occurred within financial markets which enable
computer-based trading. In particular, this section will explore algorithmic
trading (AT) and high-frequency trading (HFT) systems as two different
types of digital agents whose actions were central to both the onset and the
subsequent trajectory of the Flash Crash. Electronic trading is a relatively
new phenomenon, which in certain contexts has largely replaced human-
to-human transactions, to the point where now over half of all trades on
financial markets are executed by algorithms. Whereas financial trading
may still conjure up images resembling the human-led pit trading depicted
in films such as The Wolf of Wall Street, by this was no longer the
major form of exchange within digitized and globalized financial systems.
   Pathways towards electronic trading have not been smooth and
continuous. The evolution of automated trading systems is episodic and
discontinuous, with technological changes surrounding software and
hardware being accompanied by structural pressures arising from both
transnational financial markets and the specific local politics that surround
futures exchanges. While there clearly would be no automated trading
systems without computers, connectivity and code, a range of human
agencies also played key roles in these events. For example, the formation of
the Globex trading system – which in became the first fully electronic
trading system, when it was introduced as part of the Chicago Mercantile
Exchange (commonly known as the Merc) – was largely a response to the
perceived threat of futures markets in London and Hong Kong that were
open while trading pits were closed in the United States. Donald Mackenzie
 60) documents how ‘electronic trading shifted from being an
  

unimportant adjunct to the pit to becoming a replacement for it’, with
the introduction of the S&P E-Mini stock market index futures contract
in .1 Shortly after its introduction, the new E-Mini began to outsell
the pit-traded ‘big’ S&P contract, in part because the near-instantaneous
electronic Globex system meant that the inherent delays of embodied open
outcry pit trading could be avoided.
   Electronic trading does not have an unproblematic history. A prominent
early example of the destabilizing impact of electronic trading surrounds
the severity of the market crash commonly known as ‘Black Monday’.
This event was partially attributed to portfolio insurance, an early form of
electronic trading that was ‘designed to protect individual investors from
losses, but when used by many investors simultaneously … helped make
the fall in prices a systemic event with a feedback loop’ (Carlson  5).
While this high-profile, high-impact case where electronic trading played
a prominent role in a stock market crash dampened the enthusiasm for
computer-based trading in the late twentieth century, subsequent increases
in computational processing power accompanied by the introduction of
more algorithmically complex forms of electronic trading subsequently
led to computer-based trading performing the majority of trades on global
markets.
   The final report of the Foresight project, which was commissioned by the
UK government following the Flash Crash to explore the future of computer-
based trading in financial markets, argues that there are two fundamental
classes of computer-based trading system in operation today. The first is AT
systems that perform trades that would have been undertaken by humans
in the past, and the second is HFT systems ‘doing jobs that no human could
ever hope to attempt’ (Beddington et al.  3). HFT systems execute
trades at speeds grossly exceeding those which human traders are capable of.
They execute huge volumes of these trades, with each individual transaction
only designed to produce a fraction of a cent in profit. Over time, however,
the massive quantities of miniscule amounts add up to a significant source of
revenue, while vastly increasing the overall volume of exchanges in financial
markets. This expedited pace of exchange can be grasped through the fact
that whereas in US stocks were held for an average of four years, by
this had decreased to a mere seconds (Toscano . HFTs are
designed not to build any significant portfolios of stocks, so most assets
are traded moments after they are acquired, and portfolios are not held
overnight. While proponents claim that under most circumstances HFTs



1
 Each E-Mini contract is valued at times that of the S&Pstock index, as opposed to the
original S&P futures contract that was valued attimes the index. The ‘big’ S&P contract
was subsequently reduced totimes the value of the index due to the popularity of the
E-Mini.
  

add liquidity to markets due to the increased volume of transactions, as we
will see, there are also situations in which they have contributed to sudden
shortages of liquidity and thus to the formation of Flash Crashes.
   While HFT has been a central figure in press coverage and popular debates
surrounding the Flash Crash, the Securities and Exchange Commission
(SEC) report into the Flash Crash identifies an AT program as the instigating
factor for the event (SEC . At 2:32 p.m., a large trader initiated an
automated execution algorithm designed to sell a total of 75,  E-Mini
contracts, one of the three largest single-day sell programs executed on the
E-Mini within a twelve-month period. AT programs can be instructed to
take price, time and volume as variables into account when completing the
order, but in this specific case, the AT program was set to sell orders at a rate
of per cent of the volume of trades occurring over the previous minute
without specifying price or time as additional variables.
   As the AT program began to sell large numbers of E-Mini contracts,
many were initially bought by HFTs; however, as HFTs are designed not to
hold large numbers of contracts at any time, as they began to accumulate
contracts in a declining market, at 2:41 HFTs began aggressively selling
contracts. The large AT sell program’s response to this increased volume of
trades was to increase the rate at which it sold contracts, as the only variable
it was using to govern its activity was the volume of transactions, thereby
adding further pressure to the market which had seen orders on the buy-side
fall to less than per cent of that morning’s level (SEC  . This caused
a liquidity crisis, and consequently, AT systems that had been instructed to
buy or sell particular stocks without price variables in some cases executed
trades at irrational prices of either cent or $ , , ‘stub quotes’, or
placeholders that are never intended to actually be traded. At 2:45:28 p.m.,
E-Mini trading was paused for seconds by the Merc to prevent a further
cascade of declining prices and irrational trades. This brief break in trading
allowed the sell-side pressure to relieve, and when trading resumed prices
stabilized and then recovered.
   What does this event tell us about the relationships between technology,
agency and complexity? On the one hand, the Flash Crash confronts
us with the scope of particular forms of nonhuman agency within the
ecology of partially automated, digital financial trading. The AT and HFT
systems provide definite advantages in terms of speed when contrasted to
human traders and that temporal advantage entails that in a competitive
marketplace there is a strong rationale for replacing human traders with
automated systems. Indeed, when it comes to HFT, we see strategies that
would be impossible for humans to execute being highly profitable. However,
these nonhuman decision-making entities also have the potential to behave
in unpredictable ways that can amplify the impacts of crises and crashes,
generating systemic instabilities which are highly undesirable. Furthermore,
some of these behaviours, such as buying stub quotes at $ ,  each, the
  

highest price that can be listed, are forms of irrationality that would almost
certainly not occur with human traders.
   Within the internal logic of financial markets though, these unwanted
impacts are insufficient for investment banks, hedge funds and other trading
entities to consider jettisoning computer-based trading; this would leave
these actors at a competitive disadvantage. The broader system therefore has
a structural role in determining the agencies of individual trading entities,
and while periodic instability may be unwelcome, the advantage of trading at
the speeds of networked computational systems rather than those at which
human bodies and communication acts function is perversely understood
to outweigh these systemic risks. As Foresight conclude, a consequence of
exchanges being conducted at speeds which outcompete human traders and
prevent human oversight in real time from removing structural risk is that
‘computer based (and therefore mechanical) trading is almost obligatory,
with all of the system-wide uncertainties that this gives rise to’ (Zigrand,
Cliff and Hendershott  .
   Moving beyond this general understanding that computer-based trading
has some form of agential capacity within contemporary financial markets,
and that this both increases profitability and systemic risk, requires exploring
how nonlinear systems function in some depth. We have seen that during
the Flash Crash, HFTs created a feedback loop which amplified the risks
and issues that emanated from the large AT sell program. In order to map
these issues surrounding nonlinearity, technology and agency, I next turn
to the history of understanding processes of control and feedback whose
genealogy can be traced to the formation of the interdisciplinary field of
cybernetics in the mid-twentieth century, before examining how processes
of feedback and homeostasis that emerged in cybernetics have subsequently
been reformulated within complexity theory, systems biology and ecology.
Engaging with these theoretical and historical accounts allows us a more
detailed and nuanced way of grasping how relational and distributed agencies
flow through open systems, which not only are key to comprehending the
specific case of computer-based trading but are pervasive within digital
media ecologies.


                        Nonlinear agencies
Cybernetics emerged as a field of academic inquiry during the s and
 s from the collaboration of a transdisciplinary group of academics
including Norbert Wiener, Warren Weaver, Gregory Bateson, John von
Neumann and Margaret Mead. The term ‘cybernetics’ was coined by Wiener
   in Cybernetics: Or Control and Communication in the Animal and the
Machine, and as the title denotes, cybernetics aimed to explore mechanisms
of control and communication alongside organizational and configurative
  

patterns common to living and nonliving systems. From its inception then,
cybernetics muddies the distinction between living and nonliving systems
(George  . In addition to examining biological and technical
entities, cyberneticists recognized that ‘it is certainly true that the social
system is an organization like the individual, that it is bound in a system
of communication, and that it has dynamics in which circular processes of
feedback play an important part’ (Wiener  4), additionally blurring
the boundaries between individuals and collectives.
   The histories and legacies of cybernetics are not only relevant in terms
of their demarcation of feedback and nonlinear dynamics though; the
branch of cybernetics associated with Claude Shannon and Warren Weaver’s
information theory, and Jon von Neumann’s work around digital computers
are pivotal to the technological genealogies that manifest today as pervasive
networks of digital devices. Equally, as we will see later, the paradigm of
control and communication has been advocated as the fundamental logic
or diagram that defines contemporary societies, for example, in Deleuze’s
work surrounding societies of control. Conversely, later strands of systems
theories that pay a genealogical debt to cybernetics include systems biology,
complexity theory and Earth Systems theory, which are key fields for
science-led comprehensions of the Anthropocene. As a consequence of
cybernetics’ influences upon contemporary discourses of technology, control
and ecology – the central themes of this book – it is useful to recount various
strands of cybernetic praxis in order to elucidate how they came to be so
influential, in addition to contrasting the models of agency that arise from
these differentiated and often contradictory models.
   Cybernetics effectively formed as a discipline from a series of conferences
held in the United States between and , commonly referred to as
the Macy conferences, that were formally titled ‘Feedback Mechanisms and
Circular Causal Systems in Biological and Social Systems’. Feedback loops
occur when elements are causally connected so that an initial causal factor
circulates around the system, so that effects feed back to the start of the
loop. Whereas in a linear chain of causality A effects B which effects C which
effects D, in a system with circular causality (feedback) A effects B which
effects C which effects A. Wiener uses the example of a man steering a boat
as an example of a feedback-based system; the steersman’s job is to visually
assess any deviation from the desired course and compensate by moving
the ship’s rudder to counter-steer. This may even overcompensate, in which
case the steersman reassesses the situation and alters direction. As such,
the steersman navigates through a process of continuous feedback. Indeed,
the term cybernetics originates from the Greek word kybernetes, meaning
steersman, as cybernetics studies processes of control or steersmanship.
   Early cybernetics research explored a diverse array of feedback-based
systems: biological systems, such as human coordination in walking or
picking up cigarettes; mechanical systems, such as the thermostat and the
  

governor of a steam engine; and systems which link living and nonliving
components such as the steersman. In all of these examples, ‘the feedback
tends to oppose what the system is already doing, and thus is negative’
(Wiener  11). Negative feedback is self-corrective or homeostatic;
feedback counteracts systemic perturbations. Positive feedback, by contrast,
involves feedback which reinforces change, leading to vast alterations given
only minute changes to inputs, as difference becomes iteratively magnified.
Although cyberneticists discovered the equations governing positive
feedbacks, they were largely conceptualized as undesirable noise which
led to systems rapidly becoming unpredictable. Consequently, positive
feedbacks were neglected by cybernetics research, which was characterized
by minimizing noise while explicating processes of homeostatic balance.
Later, however, positive feedback was found to be crucial within systems
biology, complexity theory and self-organization. Indeed, the behaviour of
AT and HFT in iteratively amplifying structural risks in financial markets
are an example of a positive feedback.
   Feedback loops denote configurative relationships that are found within
systems ranging from biology, to social structures, to machines. The circular
nature of feedback loops is not a physical structure but a nonlinear pattern
of causality which is found across heterogeneous biological, technological
and physical structures. This circular causal process is key to grasping the
nonlinear dynamics that occur across open systems ranging from algorithmic
financial trading and trending social media content to climate change and
ecosystem population dynamics. It is important to grasp that linearity in this
formulation departs from how the term is usually employed within media
studies. In the fields of mathematics, cybernetics, nonlinear dynamics and
complexity theory, the term ‘nonlinear’ demarcates an equation or function
pertaining to a system, whereby changes in output are not proportional to
changes to inputs. Whereas within a linear system, a small increase in input
will lead to a small increase in output, within a nonlinear system, a small
increase in input may result in no difference or exponential change.
   This definition of nonlinearity, which is inexorably related to processes of
feedback, diverges from how it is typically employed within media studies,
whereby linearity refers to a unidirectional flow of information (Rosenberg
 ). Thus defined, linear forms of media include audio cassettes or
television programmes, where media can only be accessed as a single
predefined temporal flow, through a single interface. Nonlinear material,
by contrast, can be accessed in numerous ways, often through various
interfaces. The importance of introducing the notion of nonlinearity used
in cybernetics is that if we are interested in mapping the entanglements of
digital technocultural systems, then we must have a grasp of the fundamental
mechanisms and processes through which complex, dynamical entities act.
   Feedback and nonlinearity relate to agency through the way that these
concepts decouple media ecology from any positivist or physicalist notion
3


of simply being able to add together factors to accurately map a complex
system. Exploring nonlinear agencies within flows of symbols, electrons,
images, voltages and sound waves found in media ecosystems entails
not merely examining difference but differences that make a difference
(Bateson  87), interventions that are able to substantively alter the
trajectory of systems. Within digital culture we encounter the notion of
content going viral; escaping the long tail of social media platforms such as
YouTube and Twitter through being picked up by those platforms’ trending
algorithms which form a positive feedback loop, creating more views, more
likes and more shares which result in further attention for that material.
Understanding feedback and nonlinearity is crucial to comprehending how
social media newsfeeds, search engine rankings and other digital algorithms
affect the types and forms of content people encounter.
   The history of cybernetics involves multiple interwoven lineages of
thought that run across some of the disciplinary and political fault lines
that characterized the formation of the discipline at the Macy conferences.
One prominent way of categorizing cybernetic thought has been the
separation of first- and second-order cybernetics. In this schema, first-order
cybernetics is associated with Wiener, von Neumann and the Shannon/
Weaver model of information theory, which sought to reduce questions
of communication to mathematical problems of transmitting accurate
information through a noisy channel. Furthermore, first-order cybernetics
is associated with a tendency towards dematerialization through the
abstraction of informational patterns from material structures, and so
allegedly ‘holds onto humanist and idealist dualisms that describe the world
in terms of an equivocal dialectics of matter and form, of substance and
pattern, in which the immaterial wrests agency away from the embodied’
(Clarke and Hansen  .
   These claims are particularly associated with Wiener’s  03/ )
suggestion in The Human Use of Human Beings:

     The fact that we cannot telegraph the pattern of a man from one place
     to another seems to be due to technical difficulties, and in particular,
     to the difficulty of keeping an organism in being during such a radical
     reconstruction. The idea itself is highly plausible. As for the problem of
     the radical reconstruction of the living organism, it would be hard to
     find any such reconstruction much more radical than that of a butterfly
     during its period as a pupa.

Wiener’s focus on the transference of an informational pattern is problematic,
especially with regard to its proximity to vernacular transhumanist
discourses that suggest we will soon be able to upload the human mind
into digital neural networks, thereby falling prey to the seduction of an
immaterialist digital immortality (Hayles .
  

    We should, however, remind ourselves that digital computation affords
the apparent transference of the informational pattern of multiple media
formats – books, music, film, photographs and so on – into the universal
language of binary code. While this transformation is not the complete
substitution of one entity for another, as the specific media in question have
differing material affordances, such as the compression artefacts in digital
imaging or the different modes of temporal degradation that occur with
deteriorating celluloid and bitrot, the informational encounter between a
human user and Pink Floyd record or the same song as an FLAC file has clear
parallels in terms of informational content, even if they are not congruent.
Similarly, the use of genetically modified agrobacterium to modify the DNA
of plants is an example of transferring informational patterns between living
entities. This process is again far from immaterial, depending on advanced
scientific knowledge, tools and processes allied with the specific materiality
of the agrobacterium, which transfers DNA from itself to the plant. In both
cases then, the material properties, whether of silicon, digital storage and
compression algorithms, or agrobacterium, DNA and plants are key to
grasping these informational transferences.
    Using Wiener’s own example, the material transformation from caterpillar
into butterfly requires the larval form to release enzymes that digest all
its tissues, excepting the imaginal discs that use the resulting protein-rich
soup to fuel the process of metamorphosis. Despite this near-total process
of bodily dissolution and reformation, evidence exists that memories
from the experience of the caterpillar can be recalled by adult moths and
butterflies (Blackiston, Casey and Weiss . further demonstrating
how informational patterning can under certain material conditions be
transferred. While these examples are far removed from the hypothetical
notion of transferring human consciousness into or through machines,
they do at the very least sketch the embodied material basis for thinking
seriously about the transference of informational patterns across biological
and technical systems. In each case, however, there are key questions
surrounding the specific material processes of mediation which afford these
transferences, rather than a mystical process of dematerialization.
    Second-order cybernetics, sometimes referred to as the cybernetics of
cybernetics – which was the title of a book chapter by Margaret Mead
   and a collection edited by Heinz von Foerster  – is widely
understood to be centrally concerned with the recursive and reflexive
interactions between systems and environments, drawing on the work of
Gregory Bateson, Humberto Maturana and Francisco Varela alongside
Mead and von Foerster. One of the key concepts associated with second-
order cybernetics is that of autopoiesis, or self-making, which Maturana
and Varela  posit as key to grasping how biological organisms are
operationally closed but thermodynamically open systems. Viewed this way,
life is composed of bounded entities that maintain their structure at points
3


balanced away from equilibrium, and this process of self-making necessarily
requires these structurally closed entities to be open to transferences of
energy, primarily those associated with ingesting food, drinking water or
photosynthesizing sunlight.
   While there are numerous differences between the writings of Wiener
and Shannon in the s and s, and those of Maturana and Varela in
the s and s, the apparent schism between first- and second-order
cybernetics is more complex and nuanced than this reductive binary would
suggest. Key authors associated with second-order cybernetics, such as
Bateson, Mead and von Foerster were all present at the Macy conferences,
indeed, von Foerster was the editor of transactions of the Macy conferences
from to . Bateson and Mead have argued that the fundamental
split was not between first- and second-order cybernetics but between the
engineers, typified by Weaver and Shannon, and the general systems group,
in which they ally themselves with Wiener (Brand, Bateson and Mead .
As shown in   Bateson’s sketch foregrounds the difference as being
that the systems group includes the observer (Winer, Bateson and Mead)
within the model, foregrounding the reflexive, feedback-led process through
which the observer engages with the phenomenon being observed, denoting
that the reflexivity and role of the observer that is frequently attributed
to second-order cybernetics was also present within the heterogeneous
tradition of first-order cybernetics.
   Additionally, there were important political differences between the
engineers and systems theory groups within first-order cybernetics, which




FIGURE 1.1 Bateson’s cybernetic models.
  

are elided through a first-/second-order opposition. The engineers were
closely associated with military research and development; indeed, von
Neumann was a principal member of the Manhattan project and a key
proponent of the implosion-type nuclear weapons used during the Second
World War. He was also a member of the target selection committee which
decided that Hiroshima and Nagasaki would be targeted by nuclear
weapons. Conversely, while Wiener worked on anti-aircraft tracking
weapons during the Second World War, by the s he was strongly
opposed to accepting military-linked work, arguing that following the
use of nuclear weapons against urban populations, ‘it is clear that to
provide scientific information is not a necessarily innocent act and may
entail the gravest consequences’ (Wiener  XVII). In The Human
Use of Human Beings, Wiener’s position approaches the cybernetic
eco-philosophy of Bateson (which will be explored in detail in the next
chapter), arguing that the pace of ecological transformation associated
with industrial societies which appears as increased human control over
nature simultaneously involves an enhanced dependence upon the finite
resources of the planet:

  The more we get out of the world the less we leave, and in the long run
  we shall have to pay our debts at a time that may be very inconvenient for
  our own survival. … We have modified our environment so radically that
  we must now modify ourselves in order to exist in this new environment.
  We can no longer live in the old one. Progress imposes not only new
  possibilities for the future but new restrictions.  6)

Given this outlook, it is hard to square Wiener’s politics with the claims
levelled by critics such as N. Katherine Hayles  that first-order
cybernetics homogeneously turns machines into people and people
into machines, a process that effectively empowers the subjugation and
domination of de-humanized people. This position would not, however, be
an unfair characterization of the politics of von Neumann.
   As is often the case, the either/or first/second opposition turns out to be a
rather reductive and misleading way of approaching the differences between
the various traditions and texts associated with cybernetics. Second-order
cybernetics is perhaps better understood as the continuation and refinement
of the systems theory strand’s interest in reflexivity and observation than an
epistemic break with first-order cybernetics, as has been claimed in recent
texts that have sought to characterize a fundamental schism between the
‘bad’, first-order cybernetics of dematerialization and the ‘good’ second-order
cybernetics of reflexive embodiment. That said, the concept of autopoiesis
and its delineation of coupling between systems that are organizationally
closed but thermodynamically open and their environments does represent
an important shift away from the model of distributed cognition that Bateson
3


proposes, where the organism-in-environment forms a single cognitive
circuit, entailing that the lines between the organism and its environment
are fictitious, a lie that the ego tells the organism:

     Let us consider for a moment the question of whether a computer thinks.
     I would state that it does not. What thinks and engages in trial and error
     is the man plus the computer plus the environment. And the lines between
     man, computer and environment are purely artificial, fictitious lines. They
     are lines across the pathways along which information or difference is
     transmitted. They are not boundaries of the thinking system. What thinks
     is the total system which engages in trial and error, which is man plus
     environment. (Bateson  91)

In this formulation, it is not the human actants which provide agency, but
their concrete relational situation within material networks. Agency is
not innate but is an emergent property arising from complex interactions
within assemblages which encompass human and nonhuman elements.
When we apply this to media systems, we end up with a model of agency
which contends that technologies have agential capacities, but these are not
congruent to those exercised by human actors.
   For Maturana and Varela on the other hand, cognition is dependent
upon an organism’s embodied observations of an environment which
‘may be carried out only on the basis of self-referential closure, but that
closure, because it produces both environmental complexity and semantic
overburdening, produces more possibilities for connection, more openness’
(Wolfe  14). This interplay between organizational closure and
entropic openness within bounded autonomous biological systems, and
between the cognitive biological entity and the environment it senses and
observes in response to the need for sustenance, leads to an embodied or
enactivist model of cognition whereby living systems do not simply access
the world to construct an accurate depiction of it, they ‘enact a world’
(Stewart et al. . actively constructing and bringing forth a world
through a perspective generated via their interactions with the environment.
Consequently, within autopoiesis we have a coupling between system – the
bounded biological organism – and environment – everything outside of
that system’s boundaries – which marks a fundamental difference from
Bateson’s single cognitive circuit.
   Correspondingly, there are significant differences in the construction of
agency between Bateson’s cybernetic epistemology and Varela’s autopoiesis.
Whereas for Bateson agency is a relational property that exists distributed
across the total system, autopoiesis suggests that we can only discuss the
presence of a teleological agent as ‘a self-constructed unity that engages the
world by actively regulating its exchanges with it for adaptive purposes
that are meant to serve its continued viability’ (Di Paolo  43).
  

While the exchange between system and environment is still key to this
agential formulation, and the model of autopoietic agency does extend
agency beyond the human to other biological organisms, the limitation
of agency to purposive behaviours conducted by self-constructed unities
appears deeply problematic when applied to technocultural systems
such as AT and HFT algorithms. While these technical agents do act in
a purposeful way, they do not meet the test for autopoietic systems as
defined by Varela, who argued against the transposition of the concept
to technical and social systems, bluntly stating, ‘I am absolutely against
all extensions of autopoiesis, and also against the move to think society
according to models of emergence’ (quoted in Protevi . Consequently,
despite the issues present in Bateson’s total circuit that does not distinguish
between organism and environment, the model of distributed agency he
posits avoids the reduction of agency to biological entities that we find in
autopoietic accounts.
   Recently, Donna Haraway’s  critique of the Anthropocene has
substantively engaged with theories of autopoiesis, suggesting that the focus
on autonomous organisms coupled to environments can be read as part
of a misplaced focus upon individual autonomy that she characterizes as
pervasive within Anthropocenic and Anthropocentric approaches where
competitive individualism tends to dominate narratives of agency. Alongside
autopoiesis, Haraway posits the concept of sympoiesis, of making-together,
drawing upon Lynne Margulis’s  work on symbiogenesis which
demonstrates that the origins of complex multicellular life involved the
symbiosis of multiple organisms. For Haraway  1), autopoiesis
and sympoiesis are not an opposition, but foreground different elements
of how biological systems function as a generative enfolding. This focus
upon multispecies becomings, sympoiesis and symbiogenesis stands as an
important corrective to the individual cognitive entities found in autopoietic
discourse.
   Multispecies becoming does not mean returning to the undifferentiated
cognitive totality posited by Bateson, but it does productively undermine
the notions of closure and autonomy found in autopoietic accounts. As
the anthropologist Anna Tsing argues with reference to the sympoietic
multispecies assemblage of pine wilt nematodes, sawyer beetles and trees,
organisms must immerse themselves in webs of coordination that exceed
the autonomous cognitive spheres of autopoiesis. Addressing individual
biological actors as bounded cognitive entities necessarily fails to recognize
the inherent entanglement of collectives. For Tsing  57), ‘if we want
to know what makes places liveable we should be studying polyphonic
assemblages, gatherings of ways of being. Assemblages are performances of
liveability’. I next turn to Deleuze and Guattari’s highly influential concept
of assemblages in order to further address questions of emergence and
agency in complex systems.
3


                                  Assemblages
Assemblages are a way of describing the process by which collective
entities of humans, nonhuman biological organisms and nonliving actors
(such as technologies) are composed. As such, assemblages destabilize the
boundaries that have traditionally been drawn between humans, technology
and ‘nature’, instead forming what Deleuze and Guattari describe as the
machinic phylum, a single lineage that combines these three classes of entity.
According to Manuel DeLanda  .the term ‘assemblage’s’ translation
from the French word agencement loses the duality of meaning through
which the original term describes both the resulting collective entity and
the constitutive process of assembling, of bringing the heterogeneous
components together.
   This process of assembling presents a useful way of thinking beyond
systems as either holistic totalities, as we saw in Bateson’s cybernetic
epistemology, or in terms of autopoietic individuals.

     What is an assemblage? It is a multiplicity which is made up of many
     heterogeneous terms and which establishes liaisons, relations, between
     them, across ages, sexes and reigns – different natures. Thus the
     assemblage’s only unity is that of a co-functioning: it is a symbiosis, a
     ‘sympathy’. It is never filiations which are important, but alliances, alloys;
     these are not successions, lines of descent, but contagions, epidemics, the
     wind. (Deleuze and Parnet  2)

Unlike the process of autopoiesis in which bounded entities make themselves
and produce offspring, thinking in terms of assemblages asks us to consider
collectives as emergent sympoietic systems, as contagions, as epidemics, as
dynamic, mutable flows of becoming-together rather than discrete, sculpted
and solidified individuals. Thinking in terms of assemblages means going
beyond isolated objects-in-themselves, instead studying the configurative
relationships between entities. Homologous to Karen Barad’s theorization
of entanglement, the basic unit of existence in this schema is not an isolated
subatomic particle but relational phenomena. Things do not exist alone, or
as connected individuals, but as entangled, intra-active assemblages.2
   This position is held in contradistinction to physicalism, the dominant
metaphysical framework of classical physics, which contends that all
knowledge proceeds from the laws which govern subatomic particles,
that complex structures can be deconstructed into smaller systems which


2
 Barad suggests the term ‘intra-active’, rather than the more commonly employed ‘interactive’,
as the latter suggests a coming together between two pre-formed, stable entities. Intra-action
then designates the formative nature of the encounter and the dynamism of things.
  

maintain all the information necessary to analyse larger structures. Within
physicalism, causality is always attributed to the micro-level and can be
traced upwards as a linear set of determining forces. By isolating and
studying parts of systems, a great volume of information can be gleaned, and
this methodology has produced the majority of technological and scientific
advances that have shaped contemporary societies. Despite these successes,
however, during the twentieth century, scientists encountered numerous
phenomena where micro-causality fails to provide an adequate description
of events, as by isolating and analysing phenomena, physicalism divorces
them from the context in which they occur, the way that the components of
assemblages collectively produce emergent properties.
   We can understand emergence as situations whereby the system as a whole
demonstrates qualities that are more than the sum of its constituent parts.
Consider the case of the relative stability of species’ populations within an
ecosystem as an example of emergence: while each population can grow
exponentially, within the framework of the ecosystem the agglomeration of
species exhibits a state of dynamic balance through various feedback loops
such as symbiotic and predator/prey relationships. The relative stability
of an ecosystem, which is necessary for the survival of species within it,
is an emergent property where the feedback structures do not belong to
any individual species but to the community as a whole. Similarly, we can
consider emergence within board games such as chess. We cannot merely add
together the number of individual pieces in play to comprehend which player
holds an advantage; the value of the pieces depends on their relations to one
another. One player may have several extra pieces, but this is irrelevant if his
opponent has an imminent checkmate. The emergent game state therefore
not only depends upon the internal properties of the pieces but additionally
requires an assessment of how they are connected to one another, what
we can understand as their capacities or affordances. We can understand
emergent properties, then, as being dependent upon the specific interacting
entities, but being irreducible to those entities in isolation. Alongside its
internal proprieties, each individual entity possesses a range of capacities or
affordances, properties that are only realized through connections to other
entities and whose realization depends upon the specificities of the entities
and their intra-actions.
   Emergence precludes the formation of unifying and transcendental laws,
instead requiring detailed analysis of the specific relations between systems’
constituent parts at various scales to account for behaviours. Emergence,
then, provides a framework which rejects dualism and essentialism, but
avoids the reductionism evident in physicalist accounts. The potential
pitfall of an emergentist approach, however, is that while it removes
atomistic (micro-level) reductionism, it can lead to holistic (macro-level)
reductionisms whereby systems are posited as fully determining their
constituent parts. Within numerous disciplines, analytical methodologies
3


have been predicated upon precisely this mode of macro/micro dualism:
micro-/macroeconomics, political economy/reception studies within media
studies, global/local concerns within geography or structure/agency debates
within sociology. In order to escape this dualistic ontology, the concept of
assemblages proves useful.
   Assemblages are entities formed by historical (evolutionary) processes
whereby a heterogeneous array of elements is combined with individual
entities. The activities and capacities of the assemblage are, however, not
fully determined by either the properties of the parts (micro-reductionism)
or the totality of the assemblage (macro-reductionism). Instead, the emergent
properties of the assemblage are a consequence of the relational capacities
of the parts, potential states which exist but are not actualized until parts
enter into relation with other parts. Consequently, thinking in terms of
assemblages allows us to conceptualize emergence in terms of the material
interactions between relational capacities, rather than as a problematic form
of holism:

     Unlike wholes in which ‘being part of the whole’ is a defining characteristic
     of the parts, that is, wholes in which the parts cannot subsist independently
     of the relations they have with each other (relations of interiority) we
     need to conceive of emergent wholes in which the parts can retain a
     relative autonomy, so they can be detached from one whole and plugged
     into another one. (DeLanda  –4)

Consequently, thinking in terms of assemblages does not present holistic
totalities but posits a series of nested systems whereby the parts of particular
assemblages are themselves assemblages of parts that exist as smaller scales.
   Consider the man-horse-stirrup assemblage that Deleuze and Guattari
employ as an exemplar; this entity brings forth new forms of activity
through the way that the technical part allows the mounted rider lateral
stability, allowing the human to hold weapons differently (such as tucking
a lance under one arm), thereby altering military strategies, battlefield
outcomes and subsequent regimes of territorial governance. Here we begin
to envision how assemblages exert causal forces ‘upwards’ upon larger
assemblages. Simultaneously though, we can conceive of each part as itself
being an assemblage; while humans commonly think of themselves as single,
bounded wholes, research into the sympoietic human assemblage argues:

     The microbes that live inside and on us (the microbiota) outnumber our
     somatic and germ cells by an estimated 10-fold. The collective genomes of
     our microbial symbionts (the microbiome) provide us with traits we have
     not had to evolve on our own. If we consider ourselves to be a composite
     of microbial and human species, our genetic landscape a summation of
     the genes embedded in our human genome and microbiome, and our
  

  metabolic features a coalescence of human and microbial traits, the self-
  portrait that emerges is one of a ‘human supraorganism’. (Turnbaugh
  et al.  04)

The human, then, is an emergent multiplicity rather than a singular whole.
Just as assemblages exert determining pressures upwards towards larger
collectives, they exert pressures ‘downwards’ on their constituent parts.
For example, the human-fire-meat assemblage altered the physiological
make-up of humans by externalizing the work of digestion and chewing,
allowing the human’s digestive system to become substantively smaller than
similar-sized primates (Pollan . Assemblages therefore exert causal
pressures both upwards and downwards, existing in the middle and moving
outwards, rather than macro/micro dualisms that move from the top down
or bottom up.
   I propose that the field of ecology and Haraway’s more recent work
surrounding sympoiesis begin to provide us with a language which explores
the differential forms of coupling between the various entities that form
technocultural assemblages. Perhaps the best-known relationships are the
two oppositional forms: predator/prey and competition between species.
Alongside these, however, there exist four modes of symbiotic relationship.
These are parasitism, which denotes a relationship in which one entity –
the parasite – receives benefits to the detriment of the host; commensalism,
an association where one party – the commensal – receives benefits while
the other entity remains unaffected; amensalism, whereby one organism
negatively impacts another while receiving no tangible benefit; and
mutualism, a phenomenon by which distinct species interact in ways which
provide benefit to both species.

  Mutualisms have often been neglected in the past compared to other
  types of interaction, yet mutualists compose most of the world’s biomass.
  Almost all of the plants that dominate grasslands, heaths and forests
  have roots that have an intimate mutualistic association with fungi. Most
  corals depend on the unicellular algae within their cells, many flowering
  plants need their insect pollinators and many animals carry communities
  of micro-organisms within their guts that they require for effective
  digestion. (Begon, Townsend and Harper  81/ )

The preponderance of mutualistic relationships reveals that far from
being composed of individual species competing against each other in
oppositional relationships – as implicit in the widely encountered dogma
that depicts life as the survival of the fittest – ecosystems are comprised of
assemblages of species which co-evolve, depending upon others for their
continued survival and proliferation. Fitness, understood in this manner is
not the social Darwinist construct of individual autonomy that draws upon
4


highly anthropomorphic notions of fitness as strength, a place from which to
exercise power over the weak, but instead denotes the ability of organisms
to adaptively situate themselves within an evolutionary ecological context,
their ability to ‘fit’ into a particular niche.
   Consequently, a micro-reductionism which seeks to isolate species from
their ecological community ignores vital aspects of evolutionary survival
strategies, including co-evolved features of other organisms. This is a crucial
point for an ecological model of agency as it gestures towards mapping the
confluence of agencies which flow through multi-scalar assemblages, rather
than designating that agency exists within any particular node, human or
otherwise. From this perspective, we are misled by consciousness to consider
individual humans as singular, complete, wholes. By means of illustrating
this point, we should recall that like all vertebrate forms of life, the human
gastrointestinal tract contains a community of mutualistic microbiota,
without which the host human would be unable to convert energy from
food. Like other backbone-bearing organisms, humans are dynamic
assemblages composed of many forms of life. Dissolving the subject into an
assemblage in this way de-centres social constructivist approaches, instead
positing agency as a distributed property.
   Although conscious agencies may play dominant roles in certain
circumstances, they are always constrained: an imbalance in the microbiota
in your guts or a proliferation of the influenza virus within your body exerts
limits and pressures to the types of behaviour that your body can perform,
the affective capacities it can actualize at any given moment. It is also vital to
emphasize that the boundary of the skin is not the boundary of the assemblage.
Technologies are, as McLuhan correctly asserted, crucial sites for how pace,
scale and pattern are constructed through a process of extension, although
there never was the stable pre-technical and sensorially harmonious human
that McLuhan’s thesis of extension is built upon. With specific reference to
digital media ecologies, there exist a range of technological agencies that
invite, permit, afford, constrain, suggest or otherwise impact the potential
capacities of individual and collective actions.
   Approaching agency in this way presents a departure from the neo-
Kantian version of agency that is attributable to the intentional or
purposive activity of humans, and its structural counterpart within macro/
micro sociological accounts. While structures are not understood to be
entirely passive, their role is limited to functioning as constraints upon
the active intentional behaviours that are construed as the sole preserve
of the human. In contrast to this dualistic model of anthropocentric
exceptionalism, the agential model of assemblages suggests that humans
and nonhumans ‘have always performed an intricate dance with one
another. There never was a time when human agency was anything other
than an interfolding network of human and inhumanity’ (Bennett :
31). In the contemporary conjuncture where we are surrounded by an ever-
  

expanding array of networked digital technologies, on the one hand, and a
growing realization that this technocultural arrangement has catastrophic
consequences for a broad range of biological and ecological systems, on
the other, the entanglement between humans and nonhumans has become
increasingly difficult to dismiss.
    As we saw in the introduction, the increasing mediatization of everyday
life entails that today we find that increasing volumes of our ‘selves’ –
such as our abilities to access ‘our’ memories, to contact others, traverse
spaces or access information – are mediated through digital assemblages
of hardware, code and content. In this situation, our ability to connect to,
communicate with and perceive the world around us changes in multiple
ways which often bring tangible benefits in terms of speed and convenience,
but at the same time mean that our distributed and externalized cognitive
system is opened up to new forces associated with loss or damage
to those geographically distributed technological memory supports.
Furthermore, the commodified nature of mobile computational systems
entails that these mnemotechnical transductions of ourselves involve a
dependence upon systems that the philosopher Bernard Stiegler describes
as the industrialization of cognition, as pivotal parts of the assemblages
identified as our selves are now owned by profit-driven corporations
who employ this data to predict and influence our future behaviours.
By exploring the ways that digital assemblages augment and alter our
abilities and modes of perception, communication and organization, this
book seeks to both foreground these concerns and consider how we might
take actions to re-organize the distributed and networked assemblages of
digital technoculture.
    Returning to the Flash Crash through the conceptual prism of
assemblages, we are confronted by a multi-scalar ecosystem that has been
historically individuated through the singular development of the Chicago
Mercantile Exchange, Globex, Standard and Poors and the E-Mini among
other agents. The assemblage of futures trading markets includes a vast
array of active components that includes algorithmic and human traders,
real-time databases, networked digital computers, specific buildings and
places, stocks, shares, hedge funds, investment banks, pit-traders and other
financial actors. Smaller assemblages, such as specific HFT programs are
composed of code, digital storage, processors and memory alongside the
fibre-optic and microwave networks that allow them to connect to other
entities. What we find is not a systemic totality, an autopoietic system and
environment, or active humans paired with structural constraints, but a
swarm of distributed agencies whose flows traverse the assemblages which
collectively comprise contemporary financial systems. While the capacities
of HFTs to operate at beyond-human speeds are often misunderstood for a
kind of dematerialization that resonates with certain rhetoric associated with
early cybernetics, media ecology emphasizes the multi-scalar materiality of
4


these assemblages and the ways that this materiality affects the capacities
and behaviours of the system.
   The transmission of electronic data through fibre-optic cables at close
to the speed of light has led to mistaken claims that barriers surrounding
time and space have effectively been removed from communicative
exchange, that contemporary digital communications are instantaneous
(Virilio . Although this may be the way that this appears from the
standpoint of embodied human perception, where reaction times to visual
stimuli are approximatelymilliseconds, this is absolutely not the
case for computerized financial markets and AT systems, which are able
to react in less than a single millisecond. While the time it takes data to
travel at the speed of light through optical fibres is incredibly brief, it still
takes time; at light speed, moving from the Merc in Chicago to the Nasdaq
matching engines in New Jersey would take milliseconds. Consequently,
for the owners of algorithmic trading systems, and particularly for HFTs,
being located as close as possible to the exchanges where they operate is
paramount to reduce these temporal delays, with co-location (placement
inside the same building) being the preferred solution. However, electronic
trading entities operate across multiple exchanges, and certain forms of
HFT exploit the time differences between these venues:

     Because Chicago is the traditional primary site of derivatives trading,
     and New York of share trading, the fibre-optic links between Illinois
     (originally Chicago, but now also Aurora) and New York/northern New
     Jersey are the US system’s ‘spinal cord’. Until August , the best
     one-way transmission time available on those links was around eight
     milliseconds. However, high-frequency trader Daniel Spivey persuaded
     venture capitalist James Barksdale to fund the creation of a new cable,
     which unlike the old routes (which largely follow railway lines) is as direct
     as possible. The project conducted largely in secret cost around $ 
     million. To speed construction,teams worked in parallel, in places
     even creating what are essentially little tunnels through the rock of the
     Allegheny Mountains. The resultant link runs from downtown Chicago
     (the South Loop) to New York and to the site of the southernmost of the
     New Jersey matching engines, Carteret. Leasing ‘dark fibre’ in the cable
     (i.e. fibre for one’s own private use) shaves around 1.3 milliseconds off
     the previously fastest one-way time, and this enables the link’s owner,
     Spread Networks, to charge fees reported to be as much as ten times
     higher than those of older routes. (Mackenzie et al.  87)

Contrary to claims of digital instantaneity, we see that place and time delays
are still hugely important within financial assemblages. Indeed, today HFTs
are typically transmitted across land through the air via microwave networks
that are able to exceed the speed of fibre-optic connections. For example,
  

private microwave networks operating between Chicago and New York
reduced latency by around milliseconds (Zook and Grote  28).
While speeds have increased to the point that humans cannot intervene
and regulate actions in real time, the micro-temporal delays experienced at
light speed are absolutely central to decisions surrounding the construction
of digital infrastructure. In Chapter 6, I explore the ecological and social
consequences that arise from this infrastructural activity, further rebuking
claims surrounding the immateriality or dematerialization of digital
ecologies while arguing that we must pay close attention to the social
and environmental harms associated with digital technologies if we are to
produce more equitable and resilient technocultural systems.
   The model of dynamic and distributed agencies that emerge across,
within and between assemblages that permeate the boundaries of the
human, the biological and the technological presents an account of agency
that significantly departs from the neo-Kantian active human subject and
the sociological constraints of structures. However, in order to think about
why the Flash Crash happened precisely when it did and why, I next move
from thinking about distributed agency and assemblages towards the
agential capacities of attractors and bifurcations, what Deleuze and Guattari
describe as abstract machines.


         Open systems and abstract machines
Ecology involves the study of open systems, entities which require a constant
flow of energy in order to maintain their structure at points far removed
from entropic equilibrium. Without energy from the sun – or in the case of
ecosystems that exist deep undersea, heat from the earth itself – ecosystems
cannot function. Unlike the thermodynamically closed systems explored by
classical mechanics, which have reached a stable state, displaying a minimum
amount of free potential energy or entropy in accordance with the second
law of thermodynamics, open systems require a constant intake of energy in
order to remain at a point of balanced disequilibrium. To close a dissipative
structure, to isolate it from the dynamic flows of matter and energy which
constitute the world, to reach a static point of equilibrium, equates to the
destruction or death of that system. Importantly, complexity theory and
ecology go beyond the scope of autopoiesis here, insofar as they study
negentropic systems which are not just biological but encompass physical
systems such as tornadoes and oceanic oscillations, as well as systems that
combine biological and non-biological components.
   Whereas mechanistic science was concerned with stability and order,
complexity theory highlights instability, fluctuation and nonlinearity. Rather
than scientific knowledge equating to predictive certainty based upon the
analytical model of physicalism, nonlinear dynamics and complexity theory
4


emphasize probability, multiple choices and uncertainty, qualities which
had previously been thought to be limited to human action and free will.
According to Ilya Prigogine, a Nobel laureate in chemistry for his work
on dissipative structures and self-organization, the shift from linearly
deterministic classical mechanics to the probabilistic determinism of
nonlinear dynamics, creates ‘the birth of a science that is no longer limited
to idealized and simplified situations, but reflects the complexity of the real
world, a science that views us and our creativity as part of a fundamental
trend in nature’ (Prigogine  . This shift is important, as it allows for
the construction of a non-dualistic onto-epistemology that does not posit a
determinate nature that is fundamentally separated from creative humans
who can choose between alternative courses of action or between biological
and physical systems, as with Varela’s model of autopoiesis.
   The stable subjects of classical physics were time reversible. As Albert
Einstein  famously commented, under the paradigm of classical
mechanics ‘the distinction between past, present, and future is only a
stubbornly persistent illusion’. Temporal reversibility allowed Newtonian
physicists to calculate not only the future states of a system based on
initial conditions but also past behaviours. Relegating time to the realm of
phenomenology, however, clearly contradicts our common-sense perception
of the everyday world, where we frequently observe phenomena which are
not time reversible: processes of biological growth, ageing and death. As
Henri Bergson   remarked, ‘Time prevents everything from
being given at once. … Is it not the vehicle of creativity and choice? Is not
the existence of time the proof of indeterminism in nature?’ Indeed, the
importance of the constructive and irreversible role of time in negentropic
systems has been a central tenet of recent works associated with new
materialism which have emphasized the importance of becoming and
fragility (Connolly .alongside the vibrancy of matter within a political
ecology of things.
   Self-organizing, negentropic systems can exhibit ‘chaotic’ behaviour. The
iterative processes central to the generation of novel complexity via positive
feedback entails that these systems display extreme sensitivity to initial
conditions, so they behave in an unpredictable, apparently random manner.
This sensitivity is commonly referred to as the butterfly effect (Lorenz  4),
with the accompanying example of climatic sensitivity, whereby the
miniscule atmospheric disturbance caused by a butterfly flapping its wings
in Fiji can be iterated upon until eventually it is responsible for causing a
tornado in Texas. Although chaotic systems behave in apparently random
ways – as suggested by the term ‘chaos’ whose vernacular application
indicates disorder – they are in fact governed by strictly deterministic, though
nonlinear, mathematical equations; they are both fully deterministic and
quantifiably unpredictable. Despite an inability to define a description of the
(singular) trajectory of a chaotic system, we can make qualitative statements
  

based on probabilities. This dictates that unlike the quantitative solutions
inherent in linear equations, ‘the laws of chaos have to be formulated at
the statistical level’ (Prigogine  7). Consequently, it is possible for
predictions about the future of the global climate to be made without
requiring a precise understanding of the chaotic short-term noise within the
system, what we generally refer to as weather. This is important insofar as
it demarcates a rupture across scales, whereby one does not need to be able
to predict what the temperature will be in two weeks’ or two months’ time
in a particular place in order to describe the longer-term tendencies of the
climatic system; that the global climate will be warmer if we continue to
emit billions of tonnes of greenhouse gases into the atmosphere.
    Within nonlinear dynamics, this probabilistic prediction typically involves
analysing the phase portrait of systems, an abstract space introduced by
mathematician Henri Poincare which allows systems’ trajectories to be
reduced to a two-axis graph where every point in space represents a different
state the system can occupy. Within this space, there are typically a series
of points or zones that exert an influence upon the system, compelling it to
behave in a particular way, and these entities are described as attractors as
they notionally ‘attract’ the system. Several distinct types of attractors exist;
point attractors compel systems to move towards a specific point, periodic
attractors (also known as limit cycles) create cyclical trajectories and strange
attractors form fractal patterns. Mapping attractors presents a geography
of the virtual, which allows the topographical examination of complex
systems, allowing detailed probabilistic predictions to be made about their
behaviour, thus clearly differentiating them from genuinely random actions.
In terms of agency, we can understand the nonlinear determinism invoked by
attractors as demonstrating the limits of the humanist model which divided
beings into those which did and did not possess agency. Attractors compel
systems to move in particular directions, but the outcome of this attraction
depends upon the specific distribution of other attractors within the system’s
basin of attraction. These relational arrangements comprise what Deleuze
and Guattari  62/   describe as abstract machines, the virtual
patterns of potentiality which form a nonlinear field of pressures and limits
for systems. Although abstract machines are not tangible structures, they
thereby exert a form of nonorganic agency over systems.
    While abstract machines may sound worryingly close to Platonic ideals,
they return to the cybernetic notion of an organizational pattern that exerts
an influence over heterogeneous structures, but which are always immanent
to them, rather than transcendentally existing as ideal types or essences.
Attractors thus become a way of describing a nonlinear determinism which
is immanent though abstract, presenting a very different model of agency
to that which has been traditionally invoked by humanist accounts. The
trope of the attractor has become increasingly popular within the domain
of cultural theory, albeit one which is often applied in a metaphorical way
4


to sociocultural systems (Thrift . As a concept, it is useful to media
ecologies insofar as it provides a way of mapping the topology of a system’s
potential evolutionary trajectories, exploring the various forces and factors
which affect this evolution.
   Alongside attractors, bifurcations provide a further source of
indeterminacy within complex dynamical systems. Nonlinear dynamical
systems display extreme sensitivity at certain points, as processes of positive
feedback repeatedly iterate upon the system, dramatically altering the system’s
trajectory and the composition of its phase portrait. In these circumstances,
there may be the appearance of new attractors, the disappearance of
existing ones and even the transformation of attractor types. These systems
are described as being ‘critically unstable’, and the exact points of critical
instability are termed bifurcation points. Bifurcations are junctures in
the evolution of a system whereby pathways abruptly diverge, presenting
sudden and often dramatic and irreversible alterations to the system’s phase
portrait. These points of critical instability are unique to nonlinear systems
which operate far from equilibrium, which range from cloud formation
to social organization, from social media trends to ecosystem population
dynamics.
   Across various forms of open system, there are languages which
recognize the systemic changes enacted by bifurcations. Within the fields
of evolutionary biology and palaeontology, there has been a move away
from the type of phyletic gradualism associated with Darwinism, whereby
evolution was thought to move in an incremental, slow and steady manner,
towards an understanding of the history of life as being characterized by
punctuated equilibria. This change dictates that ‘the history of evolution
is not one of a steady unfolding, but a story of homeostatic equilibria,
disturbed only rarely … by rapid and episodic events’ (Eldredge and Gould
 4). Within media studies, the homologous notion that technologies
are naturalized or stabilize themselves within defined parameters after
a period of initial experimentation and fluctuation is common (Winston
 ; Friedman . Within the language of complexity theory, we can
describe this as the process of settling towards one of the many metastable
attractor states possible with any given technology.
   Within the fields of climate change and Earth Systems science, tipping
points – thresholds which, once crossed, bring about irreversible, potentially
catastrophic systemic changes as we bifurcate from one basin of attraction
to another – are crucial elements of the scientific literature. Frighteningly,
according to Rockström et al. , the climatological tipping points
surrounding atmospheric greenhouse gas concentrations, the rate of
species extinction and alterations to the global nitrogen cycle have already
exceeded safe values. Finally, the hypothesis of disaster capitalism advanced
by Naomi Klein  demonstrates how neoliberal governments have
  

utilized moments of critical instability, notably environmental, social and
political crises, to implement rapid legislative changes that would have
been unthinkable during the longer periods of relative social stability,
highlighting how organized social forces can leverage bifurcations. While
the political outcomes of disaster capitalism – which broadly speaking,
have been privatization, precarity, environmental degradation and rising
social inequality – are far removed from the goals of an ecological politics,
these events demonstrate that organized movements can utilize moments of
critical instability to generate substantive social change.
   In the case of the Flash Crash, there were several key structural factors
in play that collectively afforded the type of extreme event that played out.
As Foresight remark, ‘It is something of a cliché to say that CBT can lead
to “Black Swan” events, i.e. events that are extremely rare but of very high
consequence when they do occur. … However, as far as financial stability is
concerned, the more interesting and significant aspects have to do with the
general nonlinear dynamics of the financial system’ (Beddington et al. :
11). It is important to grasp that no single entity caused or unilaterally
determined the Flash Crash; it was dependent upon the wider political–
economic context in addition to a range of actors whose behaviours
collectively contributed to the unfolding event.
   On May , there were significant concerns surrounding the European
debt crisis (which was itself a direct legacy of the /8 global financial
crisis allied with the sociopolitical and economic situation of the eurozone),
meaning that markets were both unstable and trending downwards before
any unusual activity from computer-based trading systems occurred. These
contextual determinants entailed that the large AT sell order was not able
to be easily absorbed as had happened with the two larger E-Mini sell
orders that had been placed in the previous twelve months, so these factors
coalesced to bifurcate the system from one basin of attraction to another far
less stable one.
   The assemblages that affect financial systems are not just the specific
actions of algorithmic trading programs, but include the downward causality
exerted by global economic systems, which in turn are affected by the
irrational behaviour of particular micro-assemblages. Consequently, we need
to reconceptualize agency in terms of systems dynamics and entanglement
rather than as a property that is possessed by individual entities. The activity
of HFTs which began rapidly selling the stocks they had accumulated at
2:41 p.m. forms a further bifurcation, which saw the value of the E-Mini
nosedive. The pause in trading then forms a third decisive change to the
trajectory of the system. Attractors and bifurcations thereby provide us with
a way of qualitatively describing the structural pressures and abrupt tipping
points associated with nonlinear systems, a way of sketching distributed,
nonhuman agencies.
4


                      Markets and cyborgs
Throughout this chapter, I have argued that addressing questions
surrounding technology and agency in digital media ecologies requires
moving from the notion of agency associated with the human subject
towards thinking about the agencies of assemblages and abstract machines.
Cybernetics has been a key genealogical touchstone for this type of systems
thinking, through the way that various strands of cybernetic practice
and thought have influenced: (1) the paradigm of digital computing; (2)
mechanisms of feedback, nonlinearity and circular causality; (3) the lineage
of second-order cybernetic thought that feeds into systems biology and
autopoiesis; and (4) the ecological models of complexity theory and open
systems, which are pivotal to scientific comprehensions of anthropogenic
climate change and served as the inspiration for the geo-philosophy of
Deleuze and Guattari. This section examines a fifth and final strand of
influence that cybernetics has had on contemporary approaches to the
agential couplings between and within biological, technological and human
assemblages, through the figure of the cyborg and the subsequent discourse
of posthumanism.
   In the mid- s, Donna Haraway adopted the figure of the cyborg –
a portmanteau of cybernetic organism – in order to illustrate how the
increasing volume of interactions between humans and technological
systems entailed that humans could no longer reasonably construct
themselves as somehow existing independently of their technological
environments; ‘By the late twentieth century, our time, we are all chimaeras,
theorized and fabricated hybrids; in short we are cyborgs’ (Haraway :
 ). At the time, the dominant cultural associations of the cyborg were
the quasi-fascistic figures of technological control and mastery portrayed by
Arnold Schwarzenegger in The Terminator and Peter Weller in RoboCop.
This connotation of technology being linked to social and military hyper-
masculine control produced an oppositional eco-feminist discourse which
equated scientific and technological knowledge with the domination of men
over women and humans over nature.
   Haraway’s cyberfeminism, on the other hand, argued that beneficial
social and environmental changes do not require the rejection of advanced
technologies, but their reorientation towards cooperative and communal
strategies designed to enhance relationships between humans and ecological
systems. This fusion of cybernetics and distributed agency alongside the
socialist feminism advocated by Haraway is a central feature of many more
recent accounts which identify with the label posthumanism, albeit with
many accounts of posthumanism being highly critical of particular elements
associated with cybernetic practice, especially those associated with the
militarized versions of technological mastery and control and the notion of
disembodied information (Hayles  Wolfe  Crogan .
  

   These connections between computation and systems of control have
obvious contemporary relevance when we consider both the ubiquitous
governmental and corporate surveillance of digital information (Lyon, Ball
and Haggerty .and the discourses of transhumanism and the quantified
self (Lupton  Nicholls .that seek to employ digital technologies
to augment, measure and extend various human capabilities. Here,
transhumanism and Haraway’s cyborg resemble the theory of technological
extensions found in Marshall McLuhan’s mid-twentieth-century medium
theory. For McLuhan, not only were changes in scale, pace and pattern
the primary message of any technology, but technologies allow humans to
extend themselves into their environment. While the wheel extends the foot,
and print extends the eye, according to McLuhan, electrical technologies
re-create society as a ‘global village’, with the decentralizing effects of
cool (participatory) electrical media, especially television, heralding the re-
tribalization of society, forming a decentralized, democratic and creative
society (McLuhan and Fiore . This return to sensory balance following
the dictatorship of the eye under typographic technics presents a redemption
narrative which echoes McLuhan’s strongly held Catholic beliefs and a
teleology in which electrical technoculture necessarily manifests a series of
positive social consequences.
   Extending a static notion of consciousness and returning to an original,
pure and idealized balance of senses – as suggested by McLuhan’s  5)
claims that ‘in this electric age we see ourselves being translated more and
more into the form of information, moving towards the technological
extension of consciousness’ – positions a pre-technical and essentialized
human. These claims are disputed by numerous contemporary accounts of
technocultural evolution, which instead maintain that humans have always
evolved alongside tools and technologies (Stiegler  Pugliese and Stryker
 ). Thus defined, humanity has never been a stable entity to which there
can be ante- or precedent states defined by engagements with technology.
Consequently, Stiegler  12) contends that posthumanism merely
restates the fundamental technical character of being human, while failing
to adequately address the issues relating to specific modes of becoming
associated with contemporary digital technics. From this perspective, humans
have always been cyborgs, with the transformation of the digestive system
through the technological process of cooking food presenting a pertinent
early example of how technical objects have defined human becomings.
   For Stiegler, the always-cyborgian condition of becoming human
involves the application of mnemotechnical technical supports that
engenders a process of proletarianization, whereby as we externalize our
memories we correspondingly lose the knowledges and ways of living that
are externalized, echoing McLuhan’s thesis that as we extend our bodies
via technologies the extension is accompanied by numbness within body
itself. However, Stiegler departs from McLuhan’s teleological determinism,
5


contending that the deployment of mnemotechnics constitutes a crucial
contemporary political question. Mnemotechnologies can be dissociated
from anamnesis – the embodied act of remembering – in which case they
displace memory into the hands of multinational corporations or the state,
acting as proletarianizing organs of the control society (Stiegler b: 68).
Alternatively, when associated with anamnesis, mnemotechnologies can be
used to create commonwealth by allowing new knowledges and ways of
living to emerge. Stiegler thus contends that in the contemporary context,
digital technics present pathways leading towards the commodification of
cognitive systems and the industrialization of memory, while concurrently
opening possibilities for alternative processes of transindividuation and the
consequent creation of a commons-based sociopolitical system of collective
care which he terms an economy of contribution (Stiegler a: 48).
    Focussing upon technological prosthesis and the process of epiphylogenesis
supports McLuhan’s claims that technologies alter the ways in which humans
(individually and collectively) perceive time and space; however, the decisive
departure from a McLuhanite model of technology is Stiegler’s emphasis on
technology as pharmakon, simultaneously both poison and cure. This opens
up a range of potential ways in which technologies can affect sociocultural
formations and crucially positions technology as a site of political
struggle whose outcomes are far from certain. Technologies have political
affordances; they are neither value-free entities that are solely inscribed with
human agencies as we typically find in social constructivist accounts, nor do
they present teleological determinisms whereby technology X has outcome
Y, as we find with McLuhan, for whom, ‘Electricity does not centralize but
decentralizes’  5). Such straightforward pronouncements regarding
technological effects homogenize the diverse sociocultural consequences of
technologies predicated upon electricity. Centralized electricity generation
based on coal-burning power plants clearly contradicts McLuhan’s
claim pertaining to electricity’s decentralizing nature, demonstrating that
electrical technologies can be centralizing or decentralizing. Equally, in the
case of using a centralized electrical grid to run a distributed network of
computers, technologies can simultaneously embody tendencies towards
both centralization and decentralization, convergence and fragmentation.
    Another highly evocative account that reconfigures conceptualizations
of cyborgian agencies and posthuman assemblages in a way that sharply
brings the politics of contemporary technoculture into focus appears in Sean
Cubitt’s  4) Finite Media, which rejects the dominant figure of the
cyborg as a technologically augmented humanoid:

     Fantasy cyborgs look like human beings with technological implants.
     Actually existing cyborgs are huge agglomerations of technologies with
     human implants. Corporations like Enron and FirstEnergy are such
     cyborgs, composed of nonhuman actors with human biochips embedded
  

  to carry out specialist tasks like those involving human resource
  management and public relations. Corporate cyborg agency is distributed
  but not communal, not least in electrical grids connecting aggressively
  active users (who can scarcely be caught in the term consumers), the
  unmanaged turbulence of deregulated and automated markets, and the
  inhuman drive for corporate profit. Such actor networks are realized
  socio-political agencies whose other-than-human standing is confirmed
  by their lack of shame. Frankenstein monsters created out of the logic
  of advanced capital, their sole motive is profit, regardless of all other
  consequences. Their environment is not the physical world but the
  financial, a world where human affairs appear only as inputs and
  price fluctuations. … The corporate cyborg not only risks the future of
  humanity and its environment but its own future in actions which, in a
  human being, would be deemed suicidal.

Cubitt’s intervention is to productively reconceptualize the cyborg as an
assemblage that exists at a different scale to that which is introduced by
Haraway and advanced through subsequent discourses around cyborgian
agency. Cubitt asks us to rethink the corporate assemblage as cyborg, an
individuated entity which melds technological and biological systems into
an inhuman logic based on short-term profitability at all costs, whose
ramifications currently reverberate throughout ecological systems as the
conjuncture known as the Anthropocene.
   A similar argument about algorithmic financial trading is advanced by
legal scholar Tom Lin who describes ‘cy-fi’, a cybernetic financial assemblage
in which AT systems, computational and networking hardware, software
and protocols collectively form a financial ecology where both systemic
risks and short-term profitability are increasingly prevalent. In both
Cubitt’s and Lin’s accounts, the key departure is from an anthropocentric
frame where individual biological/technical hybrids resemble humanoids,
towards re-imagining vast financial or corporate assemblages as historically
individuated cyborgian entities. Within these cyborgs, human values of
empathy, compassion and kindness have effectively been eliminated by a
light-speed drive for efficiency and profit.
   The suicidal logical that Cubitt explicates, that of cybernetic organisms
which relentlessly seek to externalize costs onto ecosystems precisely
because it does not recognize that this common world is required for its
own survival, is one that should be familiar from the Flash Crash. The
systemic instability that HFTs propagate potentially threatens the futures of
not just investment banks and hedge funds but the financial security of the
millions of workers whose pension funds are bound up in these automated
financial systems. Italian Autonomist-Marxist Christian Marazzi has
insightfully outlined the transformative process of financialization, in which
household savings and retirement funds have been funnelled into stock
5


markets from the s onwards. Whereas under the mid-twentieth-century
paradigm of Fordism industrial workers were able to define their identities
and class interests in opposition to capitalism, ‘with their savings invested
in securities, workers are no longer separate from capital, as they are, by
virtue of its legal definition, in the salary relationship’ (Marazzi  7).
Consequently, the risks caused by computer-based trading systems which
have been demonstrated to periodically behave in irrational and destructive
ways should be of significant concern for nurses, teachers, bus drivers and
other workers whose future economic security is now beholden to the
inhuman agencies of cyborgian financial systems.
   Nevertheless, the competitive advantage gained by employing faster-than-
human computational trading systems entails that not using them simply
isn’t an option in the current financial ecosystem. The question which this
situation logically proffers is, why are these inhuman agents which create
no use value and significantly increase the chances of catastrophic systemic
failure not regulated out of existence? The answer is that within the inhuman
logic of the market that Cubitt outlines, medium- to long-term systemic
instability is understood as a risk worth taking for the short-term profits
AT and HFT provide. Put simply, the values of the system are effectively
self-destructive if we look beyond immediate temporal horizons. Here we
begin to glimpse the temporal connections between the destructive short-
termism of digitally enabled financial ecologies and the broader ecological
conjuncture of the Anthropocene whereby the speed of change outpaces the
capacity of a multitude of species to adapt. In both cases, there are serious
issues with a self-destructive, myopic focus upon short-term advantages that
detrimentally impacts upon the resilience and long-term outlook of systems.
   In the wake of the Flash Crash, analyses tended to concur that the
5-second pause in trading was key to the subsequent recovery, and so
consequently the SEC approved a circuit breaker to securities which would
come into effect if that security experienced a change in price of over per
cent during any five-minute period. Since , however, there have been
a worrying number of events with similar dynamics to the Flash Crash.
In October , the Indian National Stock Exchange saw almost US$60
billion temporarily wiped off markets. In April , the Associated Press
Twitter account, which had been hacked, erroneously claimed that Barack
Obama had been injured in explosions at the White House. As HFTs cross
reference newsfeeds, they immediately began selling futures contracts, with
the result being a  -point fall in the Dow Jones industrial average followed
by a rapid recovery. In October , with concerns over a ‘Hard Brexit’
looming, the British pound saw a flash crash in which it lost around per
cent of its value against the US dollar in two minutes. Across these examples,
HFTs have been blamed for the severity and speed of these events, and the
implementation of circuit breakers has not prevented them from occurring,
only from spiralling completely out of control.
  

   There is a serious case to be made then for implementing regulations
designed to preclude forms of AT that increase the systemic risk of these
events occurring. These entities have no underlying use value, they merely
act as parasites that feed off other exchanges and events in ways that
are demonstrably detrimental to the overall system. Regulating markets
and banning particular trading technologies, however, is a strategy that
contradicts the ideologies of the free market and neoliberalism, which
despite being discredited by the global financial crisis of /8 are still
deeply ingrained within financial ecosystems and political elites. Surveying
this situation reminds us that ‘there is an ecology of bad ideas just as there
is an ecology of weeds, and it is characteristic of the system that the basic
error propagates itself’ (Bateson  89).
   Further addressing the logic of neoliberalism and the inhumanity
of contemporary corporate cyborgs requires us to move away from
conceptualizing agency – questions of who or what can act – and towards
questions of how we come to evaluate whether those actions are positive
or negative, and how assemblages collectively and intentionally mobilize
to enact particular outcomes. In order to address these questions, the next
chapter focuses upon ecological ethics and politics, and how ecological
praxis can be applied to digital media ecologies.
       Ecology, ethics and collectives



Whereas the previous chapter explored issues surrounding technology,
complexity and agency, using the Flash Crash as an example to assemble
various strands of cybernetic and ecological thought to outline a model
of distributed agency, this chapter shifts focus towards questions
surrounding media ecology, ethics and politics. Conventionally, ethics
are understood as inquiry into the prescription and rationalization of
good and bad values, which inform how individuals should act, whereas
politics is the venue within which ethics are negotiated and collectively
implemented. The traditional division between ethics and politics then
(assuming that politics is understood as collectively organizing around
shared values, rather than the formal political administration of an area)
relies upon a distinction between individuals and collectives which the
model of assemblages and entanglement outlined in the previous chapter
fundamentally undermines.
    Understanding ethics as the domain of the individual human subject helps
clarify why contemporary ethical discourses are both highly prominent and
highly problematic. Within the discourse of neoliberal capitalism – whereby
quantifiable competition between individuals through deregulated markets
is fetishized as a near-universal ideal form of social relation (Harvey 
Peck  Brown  Beer .– there is a concerted drive away
from collective interventions into public life that are explicitly designed
to benefit citizens. This does not mean that states no longer build roads,
run health services or schools, but that these activities are increasingly
subjected to a logic of competition and marketization (Gilbert .
Neoliberal dogma contends that markets are efficient and just arbitrators
of social disputes, that our frequent participation in financial exchanges
entails a greater level of democratic engagement than electing political
representatives once every few years. There is, undoubtedly, a very real
  


disconnection between many people and party politics;1 however, we
should remember that representative democracy rests upon each citizen
receiving one vote,2 whereas consumer participation in markets is directly
proportional to financial resources. A system that empowers the wealthy
to participate more than the poor unsurprisingly reflects an ideology that
presents competition between individuals as key to judging the success and
worth of those humans. Consequently, ‘mainstream’ neoliberal solutions
to contemporary ecological crises champion the oxymoron of market-
based solutions which will allegedly be realized through acts of ethical
consumption.
   This conception of ethics sees atomized individuals being able to positively
alter the trajectory of climate change through purchasing LED light bulbs,
the global plastics crisis through not buying bottled water and eliminating
e-waste through using certified recycling schemes. While these actions are
not inherently harmful, the rhetoric that postulates them as solutions to
Anthropocenic crises is ludicrous, and if taken seriously, removes attention
and energy from the collective mobilizations that are required to address
the global and geological scales at which these issues operate. The scalar
disconnection between individual acts of consumption and the problems
they supposedly address is further emphasized through a realization that
the majority of greenhouse gas emissions, plastic and electronics waste are
related not to individual, domestic consumers but to industrial, corporate,
military and governmental operations (Pachauri et al.  IEA . In
many cases, then, advocating ethical individual consumption as the solution
to Anthropocenic crises demonstrates a fundamental failure to grasp the
scale and scope of contemporary issues.
   Furthermore, ethical consumption can only address issues of production
through the mediation of the market. When we consider the complex,
global supply chains that are endemic to microelectronics, actions that
solely address consumption are far less likely to effectively address harmful
and exploitative processes of production than national and international
regulation. Similarly, while individual consumers purchase smartphones,


1
  Especially surrounding liberal and social democratic parties that in the wake of the global
financial crisis – which demonstrated the total failure of deregulated financial markets
to sustainably self-organize – have clung to the narrative that there is no alternative to
neoliberalism. Indeed, where left-wing parties have seen a resurgence, be it the Corbyn-led
Labour Party in the United Kingdom, Podemos in Spain or Syriza in Greece, there have been
explicit rejections of austerity and neoliberalism.
2
  Although the reality of political funding systems ensures that powerful economic interests are
frequently able to amplify their voices and influence. This is a fundamental issue that must be
addressed if democratic politics is to be worthy of the name.
5


wearable computing devices, smart televisions and so on, they do not
typically procure data servers, cellular towers, undersea fibre-optic cables,
GPS satellites or other back-end infrastructure that is necessary for
consumer devices to function. Consequently, ethical consumption can only
ever reach the tip of the technocultural iceberg. As a strategy for the type
of radical systemic transformation that is necessary to address the scale
and complexity of contemporary ecological crises, ethical consumption is a
woefully inadequate strategy.
   We should also note that the additional financial and temporal costs
associated with ethical consumption ensure that for those already struggling
within a generalized state of precarity and financial inequality, ethical
consumption is an unaffordable luxury; if you’re working sixty hours a
week and struggling to pay rent and bills, then buying organic food, avoiding
plastic packaging and conducting the research to find less exploitative and
environmentally destructive consumer goods are simply not viable options.
Often then, ethical consumption is predominantly accessible to the middle
classes; market-led solutions favour those who succeed financially, whereas
the poor who have failed in their ‘duty’ to succeed fiscally subsequently fail
as ethical consumers. That is not to say that there is no merit to enacting
small-scale changes such as using LED light bulbs, adopting a plant-based
diet or reducing the use of single-use plastic. As we shall see, an ecological
ethic asks us to practice the types of change we wish to see, but crucially this
requires us to enact forms of connectivity and collectivity that far exceed the
market-based individualist strategies of ethical consumption.
   Consequently, this chapter follows the ethico-onto-epistemology of Karen
Barad in ‘rejecting the metaphysics of individualism that serves as a foundation
for traditional approaches to ethics’  93). Within this formulation,
ethics is not separate from our ways of knowing (epistemology) or becoming
within the world. Rather than an abstract way of thinking about idealized
situations, ethics are enacted through performative processes: ‘Ethics is
not simply about responsible actions in relation to human experiences of
the world; rather, it is a question of material entanglements and how each
intra-action matters in the reconfiguring of these entanglements, that is, it
is a matter of the ethical call that is embodied in the very worlding of the
world’ (Barad  60). This emphasis upon ethics as an enacted material
process blurs the boundaries between a second category that has often been
used to divide ethics and politics – thought and action – with ethics being
associated with values and politics being the materialization and enactment
of those values through worldly activity. This distinction, in which ethical
purity can be contrasted with the Machiavellian world of political action,
rests upon a problematic dualism that seeks to divide the world into an
individualized interior world of thought and a collective world of action, in
contradistinction to the ecological model of distributed agency and multi-
scalar assemblages that was outlined in the previous chapter.
  


   This chapter builds upon this model of agential assemblages, elaborating
an ecological ethic that follows Barad, Bateson and Guattari in rejecting
key tenets of the main strands of Western ethics. I begin by outlining
virtue ethics, deontology and consequentialism as the three major strands
of Western ethical theory, foregrounding some of their inadequacies from
an ecological perspective, before outlining how concepts advanced by
Bateson, Guattari, Barad and Deleuze can form the basis for an ecological
ethic centred upon connectivity, collectivity, affect and speed. The chapter
subsequently addresses the politics of collective mobilizations and commons,
before concluding with a summary of how the insights surrounding ecology,
ethics, technology complexity and agency that have been explored in Part
1 of this book will be applied to the scales of content, code and hardware
in Part 2.


                     Ethics and anthropocentricism
Western ethical philosophy has three major branches: virtue ethics,
deontology and consequentialism. Virtue ethics is among the oldest forms of
ethical philosophy, with advocates including Plato and Aristotle. It contends
that abstract virtues exist and that they constitute positive character traits
such as wisdom, courage, temperance and justice.3 Virtue ethics stresses the
importance of acquiring these virtues; once courage has been acquired, an
individual will habitually act in a courageous way, embodying the virtuous
trait. Aristotle emphasized that the goal of ethics, the highest form of good
for humans, was Eudaimonia, a term which is often translated as happiness
but which transcends contemporary applications of the term, coming closer
to ‘human flourishing’ (Robinson . For Eudaimonia to be attained,
Aristotelian virtue ethics contends that an individual must acquire virtues
and exercise reason, the faculty which purportedly separates humans from
nonhumans. Consequently, virtue ethics is often critiqued for relying upon
teleology and essentialism; by positing virtues as abstract qualities which
exist outside of real-world interactions, virtue ethics present an ontological
essentialism which departs from the materialist approach to technoculture
presented in the previous chapter. Despite this, however, philosophers
such as Patrick Curry  contend that virtue ethics provides the most
promising tradition from which an ecologically sensitive ethic may emerge,
arguing that because virtue ethics emphasizes embodied habits rather
than exercising pure reason, it is closer to an ecological praxis in which
ethics arise through everyday activity rather than abstract rational–critical
philosophical constructions.


3
    According to Plato, these are the four cardinal virtues
5


    A second major branch of Western ethical thought is deontology, whose
etymology derives from the Greek term deon meaning duty. Deontology
maintains that ethics correlate to duties – what is due or right – which
must be enacted irrespective of material consequences if actions are to be
considered ethically just. Deontology, then, presents moral questions over
whether actions are right or wrong, with no ethical space for derelictions of
duty. Deontology was most famously championed by Immanuel Kant, who
responded to David Hume’s claims that whereas science and mathematics
were the realms of reason, ethics were the domain of passions, sympathies
and emotions, with the formulation of the categorical imperative, which
mandates: ‘Act only according to that maxim whereby you can at the same
time will that it should become a universal law without contradiction’ (Kant
 0). By aiming to create a rationalist ethics predicated upon universals,
Kant sought to ground ethics upon reason and the notion of rights, which
has proven to be important in developing modern political discourses,
especially those surrounding human rights. Consequently, the Kantian
tradition of deontological ethics is still popular today, largely through the
strand of rights-based ethics derived from the works of American political
philosopher John Rawls.
    Regarding the kinds of agents this justice applies to, Rawls contends
that the ‘status of the natural world and our proper relation to it is not
a constitutional essential or a basic question of justice’  26).
Consequently, we can understand deontological ethics to be firmly
anthropocentric. Indeed, for Kant the only reason to treat animals with
consideration was as practice for treating humans well. By contending that
humans are the sole source of value, deontological ethics thus departs from
an ecological ethics. Furthermore, we should ask serious questions regarding
the viability of applying universalist concepts such as the categorical
imperative to the contemporary ecological conjuncture; following Timothy
Morton , we can note that climate change makes hypocrites of us
all insofar as simply by sustaining ourselves as living entities by eating we
contribute to the problem.4
    For our individual actions to be ecologically sustainable if extended
to over seven billion humans, we would have no air travel, no cars, no
computers, no smartphones and so on. Aside from the practical implication
that few humans would choose to give up many of the comforts of modern
life, we should note that those few individuals who are both inclined and
economically privileged enough to buy land and live directly off it in an


4
 The issue with such a universal declaration of hypocrisy is the failure to acknowledge the
heterogeneous contributions to climate change, with per cent of the human population
contributing half of greenhouse gas emissions and the poorest half of the population
contributing just per cent of emissions. Nonetheless, Morton’s work does significantly
problematize a deontological ethic of purity in relation to climate change.
  


ecologically non-harmful way are sufficiently disconnected from the rest
of society to practically ensure that they will not contribute to collective,
political solutions to ecological crises. An ethic of moral purity which
ignores collective crisis, instead focusing on a misplaced sense of individual
importance is of little use precisely because of its scalar misidentification.
Consequently, an ecological ethic must seek to enact change to collectives,
rather than exclusively focusing on a puritanical individualism.
   Consequentialism presents a third ethical position, which is often cast
as the antithesis of deontology. Whereas deontologists assert that ethical
behaviour requires the observance of morals regardless of consequences,
consequentialists contend that the ethical character of actions ‘derive entirely
from the value of consequences’ (Blackburn  7). For consequentialists,
what matters are outcomes rather than intentions. The dominant form of
consequentialism, derived from Jeremy Bentham and J. S. Mill is known
as utilitarianism and contends that ethical actions result in advancing the
greatest amount of happiness for the greatest number of people. Although
this can be understood to address some of the problematics surrounding
inflexibility and individualism within the deontological tradition – by
addressing notions of the common good – there are serious issues to be
raised with the notion of to whom and what the greatest good applies.
   Whereas for Bentham and Mill this meant living humans, there are
powerful arguments for extending the common good to future generations
of humans and to nonhumans, at the very least to sentient animals clearly
capable of experiencing pleasure or pain (Singer . This requires that
serious questions are asked pertaining to how calculations surrounding
collective happiness can be derived. How does the happiness of current
humans compare to that of future humans or even to that of nonhuman
biological entities? Such questions are especially pertinent when we consider
that Anthropocenic crises are fundamentally orientated towards the future
and are likely to result in extinction for innumerable nonhuman species.
   Another weakness inherent to consequentialist ethics is the assumption
that there are knowable and calculable outcomes that can be derived
from specific actions. While in certain scenarios we can reasonably expect
to predict the immediate ramifications of specific actions, extending the
nonlinear consequences of these actions across time and space soon becomes
impossible, largely as a result of the impacts of feedbacks and bifurcations,
whereby seemingly insignificant actions may have grave consequences,
and apparently important actions may have minimal impacts. Resultantly,
the moral cost–benefit analysis proposed by consequentialism appears
entirely unworkable within an ecological agential framework. Perfectly
understanding the consequences of actions – even with hindsight – is often
impossible, so it makes little sense to base an ecological ethics upon such
problematic foundations, even if we disregard its anthropocentricism and
presentism.
6


    The ecological ethics outlined in this chapter reject aspects of all three
of the dominant ethical traditions of Western philosophy. Unlike virtue
ethics, ecological ethics reject essentialism and teleology. Unlike deontology,
ecological ethics do not contend that universalization is necessary for
an act to be ethical or that consequences do not matter. Departing from
consequentialism, ecological ethics contend that happiness/good cannot be
objectively calculated and that ethics must include future generations, despite
the future being defined by a lack of certainty. Additionally, ecological ethics
contend that the anthropocentric focus implicit in each of these traditions
is problematic and that, instead, an ethic of the multiple, or a logic of the
AND – which considers varying overlapping and interconnected points of
view (human and nonhuman, current and future) – is required.
    Furthermore, ecological ethics require a focus upon action rather than
reflection and pure reason, aligning it with Barad’s ethico-onto-epistemology,
which contends that embodied processes of becoming are inseparable from
our ways of knowing and acting upon and with the world, and Francisco
Varela’s approach to ethical know-how, which combines insights from
autopoiesis and enactivist theories of cognition with elements of Buddhism.
For Varela, Western ethics have grossly neglected the role of skilled behaviour
(habit), in favour of deliberative and intentional analysis: ‘Praxis is what
ethical learning is all about. If we don’t practice transformation we will
never attain the highest degree of ethical expertise’ (Varela  3). For
Barad and Varela, ethical behaviour is learned through enacting beneficial
transformation, rather than calculating ethical cost–benefit analyses
or adherence to the categorical imperative. Situating ethics in this way
additionally means unmasking the universalism present in Western ethical
philosophies, whereby a white, male, bourgeois position problematically
presents itself as an abstract, disembodied arbiter of truth. Ethical praxis
therefore has to recognize the colonial, sexist and classist dimensions of
Western thought.
    Experimentation and creativity, acting and learning from the complex
consequences of actions are therefore key to this ethic. This marks a
departure from green ethics that are predicated upon the precautionary
principle (Curry  Riordan and Cameron . which propounds that
if actions present risks to the public or environment and potentially adverse
effects are not entirely understood, then action should not proceed. The
precautionary principle has been deployed in varying ways by conservation
and preservation groups to undertake ethically and politically important
tasks such as protecting rainforests from clear-cutting; however, the
principle – or at least any strong reading of it – is somewhat dubious given
the impossibility of understanding the potential implications of actions. As
uncertainty and risk are pervasive in complex systems, prohibiting actions
predicated upon uncertainty makes little sense. As we have seen, points
of critical instability may be breached with little forewarning, leading to
  


rapid change which may be systemically catastrophic or lead to increased
complexity. While there may be probabilistic ways of considering the
likelihood of particular bifurcations occurring, the type of control over risk,
or ecological cost–benefit analysis, implied by the precautionary principle is
effectively unworkable.
   Humanity has never been capable of observing the world without
disturbing it. Indeed, at a time where technocultural activities are having
devastating consequences for ecological systems, with the prospect of
considerably worse to come, we cannot extricate ourselves from the ethical
imperative to act: ‘“Do I dare disturb the universe?” is not a meaningful
question, let alone a starting point for ethical considerations. Disturbance is
not the issue. … There is no such exterior position where the contemplation
of this possibility makes any sense’ (Barad  96). Adhering to the
precautionary principle entails that few if any actions could be considered
ethical, and many promising avenues for affecting positive change would
be rejected on the basis of uncertainty. Far from an ethic of praxis and
change, the precautionary principle presents an ethic of inaction and
paralytic stasis.
   Such a position is, of course, entirely in keeping with the tradition
of environmentalism as conservation, whereby the central premise of
environmental action is to conserve and preserve presently existing elements
of the ‘natural’ world. Conservationism, while generally well-intentioned,
appears seriously flawed when we adopt an ecological approach which
emphasizes the world as a dynamic system which is constantly becoming,
flowing and evolving. From this vantage point, conservationism aims to enact
cultural practices which are fundamentally ‘unnatural’ in their attempts to
sever ecosystems from flows of change. Rather than preserving nonhuman
nature, conservation in fact seeks to create an artifice of ecological stasis
which is primarily predicated upon the valorization of specific aesthetics
associated with anthropocentric projections of the nonhuman world that
frequently align with a colonial ideology whereby lands existed in a natural
state of harmony prior to European discovery.
   Consequently, conservationism frequently focuses upon the plight
of charismatic megafauna, species such as polar bears, giant pandas and
elephants, which have affective resonances with humans, but which,
objectively speaking, are no more important than microbial life forms
or beings that are negatively categorized as pests, vermin and invasive
species. That is not to suggest that pests and invasive species cannot be
immensely problematic, insofar as they can rapidly reduce biodiversity
and leave ecologically fragile monocultures in place, but there are also
instances whereby nonhumans are considered to be objectionable,
perverse or deviant life forms for questionable reasons predominantly
based upon aesthetic judgements, such as animals successfully adapting
to localized anthropocentric ecological alterations which conservationists
6


routinely characterize as degradation caused by human interference with a
romanticized (nonhuman) nature (Holm .
   Focusing upon the conservation of ‘nature’ underpins rhetorics of
sustainability which advocate that environmentally beneficial practices are
those which somehow segregate presently existing systems from any sense
of dynamism and change in order to preserve contemporaneous relations
or revert them to a lionized pre-industrial past. As political ecologists Bram
Büscher and Rob Fletcher  illustrate, such rhetoric has become deeply
ingrained in neoliberal conservation practices that have commodified natural
resources and ‘wilderness reserves’ while displacing the human inhabitants
of these areas to make way for international ecotourism. Effectively,
neoliberal economic policy is unironically applied to address the ecological
devastation caused by those same policies. Consequently, an ecological
ethic, does not seek the conservation or preservation of a static nature which
is ontologically removed from humanity but considers how we can bring
forth the kind of world that we wish to inhabit and bequeath to future
generations of humans and nonhumans. In order to explore this ecological
ethic in more detail, I begin by returning to the work of Gregory Bateson,
the cyberneticist, biologist, anthropologist and ecologist whose pioneering
work examined how feedback and systems thinking impacts upon ethics
and politics.


                         The three ecologies
In Steps to an Ecology of Mind, Bateson  advances the notion that
environmental and social crises are created by pathological epistemologies.
The dissemination of erroneous premises leads to the introduction of bad
habits, actions rooted in epistemological principles so deeply ingrained
into the fabric of our lives that we do not question their validity. These
bad habits take root and proliferate as an ecology of conceptual weeds.
Bateson delineates that these erroneous premises stem from a hegemonic
epistemology created by the mode of production and organization present
in industrialized societies which he contends are primarily dominated
by competitive individualism aided and abetted by a misplaced belief in
humanity’s ability to wield unilateral control over the environment, and
the ability of scientific and technological progress to afford this apparent
domination:

     On the one hand we have the systemic nature of the individual human
     being, the systemic nature of the culture in which he lives, and the
     systemic nature of the biological ecological system around him; and on
     the other hand, the curious twist in the systemic nature of the individual
     man whereby consciousness is, almost of necessity, blinded to the
  


  systemic nature of the man himself. Purposive consciousness pulls out,
  from the total mind, sequences which do not have the loop structure
  which is characteristic of the whole systemic structure. If you follow the
  common-sense dictates of consciousness you become, effectively, greedy
  and unwise. (Bateson  40)

In place of the agonistic ethos of industrialized culture, Bateson’s cybernetic
epistemology constructs the world around mutualism, the communication
of difference, emergent formations and processes of feedback.
   In developing this approach, Bateson introduces the concept of the three
ecologies – mind, society and environment – a triadic schemata, which
using the language of the previous chapter we can describe as entangled
assemblages. Each ecology exists as a complex system consisting of a
multitude of heterogeneous components, transversally interacting within
and between the three ecologies, with balance across these entangled
ecologies portrayed as essential to the continuation of human societies. For
Bateson, the key is to enlarge the unit of ecological survival from thinking
about individuals – whether conceived as organisms, species, nation-states
or corporations – to a systemically orientated conception of the entity-in-
environment: ‘The last hundred years have demonstrated empirically, that
if an organism or aggregate of organisms sets to work with a focus on its
own survival and thinks that is the way to select its adaptive moves, its
‘progress’ ends up with a destroyed environment. If the organism ends up
destroying its environment, it has in fact destroyed itself (Bateson :
 ). Whereas the anthropocentric model of competitive individualism
encourages the externalization of harms onto the environment precisely
because the environment is viewed as an external entity, Bateson contends
that this perspective is premised upon an epistemological blindness that fails
to recognize our dependence upon that environment. Consequently, instead
of competitive individualism, Bateson advocates adopting an ecological ethic
that thinks in terms of communities and ecosystems instead of atomized,
competing individuals.
   It is worth foregrounding the resonances between Bateson’s model of
the three ecologies and the multi-scale approach that is applied within
the contemporary science of ecology. In Ecology: From Individuals to
Ecosystems, Begon, Townsend and Harper  i) explain that ecology is
concerned with relationships at three distinct scales of organization:

  The individual organism, the population (consisting of individuals of the
  same species) and the community (consisting of a greater or lesser number
  of species populations). At the level of the organism ecology deals with
  how individuals are affected by (and how they affect) their environment.
  At the level of the population, ecology is concerned with the presence
  or absence of particular species, their abundance and rarity, and with
6


     the trends and fluctuations in their numbers. Community ecology then
     deals with the composition and organization of ecological communities.
     Ecologists also focus on the pathways followed by energy and matter as
     these move between living and non-living elements of a further category
     of organization, the ecosystem, comprising the community together with
     its physical environment.

Consequently, the textbook is separated into three sections, with each
section addressing a particular scale: from organisms, through populations,
and finally to communities and ecosystems. While the differing scales
are not identical, the triadic structure allied with similar categorization
demonstrates the way in which the ontology of ecology has been transposed
by thinkers such as Bateson into social and political theory. Given the
catastrophic predictions associated with the Anthropocene, the merits of
adopting approaches based on ecological principles are that they provide
an alternative ethic to those of industrial culture, which may point towards
more ecologically resilient and socially equitable futures.
   There are, however, dangers in seeking a basis or justification for ethics
in the nonhuman world. Various conceptions of nature have been mobilized
in support of ideologies that have vehemently promoted racism, sexism,
colonialism and anthropocentrism. Eugenics and social Darwinism are
particularly acute examples that reinforce why drawing upon particular
elements of the nonhuman world to mobilize support for political projects
may be ill-advised. Similarly, the language of ecology is often mobilized
today by venture capital and Silicon Valley techno-utopians to naturalize
socially and ecologically destructive elements of neoliberalism and platform
capitalism. Alongside these politically regressive naturalisms, there are,
however, longstanding radical alternatives, notably Peter Kropotkin’s
anarchism which drew upon ecological mutualism to propose a human
society based upon mutual aid, which has some striking parallels with the
contemporary works of Haraway and Lynne Margulis.
   While these accounts do stress collaboration and mutualism, they should
not be read as totalizing models that suggest mutualisms or latent commons
are universally good things. Multispecies collaborations create spaces that
suit some entities and are inhospitable for others. Species that are left out
of mutualistic relations may find themselves outside of an ecological niche.
As Tsing  55) insightfully remarks, ‘The best we can do is to aim
for “good enough” worlds, where “good enough” is always imperfect and
under revision.’ In contrast to the utopian biocentrism of deep ecology, this
kind of compromise does not mean a world where everything can succeed.
We need to evaluate how and why we take actions that foster certain forms
of becoming while closing others off, while acknowledging that although
humans will never completely control ecological processes, we do already
play important roles in their ongoing assembly.
  


   While Bateson first advanced the concept of the three ecologies, it is perhaps
more commonly encountered today through the work of Felix Guattari,
the French philosopher, activist and psychoanalyst who is best known for
his collaborations with Gilles Deleuze. Despite often being considered a
fervent anti-statist, towards the end of his life, Guattari unsuccessfully stood
as a green party candidate. Guattari’s ecosophical position, synthesizes his
earlier collaborative endeavours that fuse Marxism, poststructuralism and
complexity theory with elements of Bateson’s ecological epistemology,
contending:

  Environmental ecology, as it exists today, has barely begun to prefigure
  the generalized ecology that I advocate here, the aim of which will be to
  radically decentre social struggles and ways of coming into one’s own
  psyche. … Ecology must stop being associated with the image of a small
  nature-loving minority. Ecology in my sense questions the whole of
  subjectivity and capitalistic power formations. (Guattari  2)

Guattari rejects the value system valorized under neoliberalism (Guattari’s
own term is ‘integrated world capitalism’) in which economic growth is
viewed as the sole determinant of a societies’ worth, aligning his perspective
with Bateson’s rejection of economic determinism. Guattari and Bateson’s
ecological models diverge, however, in that whereas Bateson’s framework is
predicated upon cybernetic models of homoeostasis, Guattari additionally
incorporates insights derived from complexity theory where both positive
and negative feedbacks are pivotal. Whereas for Bateson ecological balance
was a homeostatic point of equilibrium, the focus upon iterative processes
of self-making within Guattari’s ecosophy denote that any sense of balance
is temporary, unstable and open to new forms of becoming.
    We should note that Guattari departs from Maturana and Varela’s
definition of autopoiesis, advocating for technical systems to be considered
autopoietic:

  Varela reserves the qualification ‘autopoietic’ for the biological domain.
  Social systems, technical machines, crystalline systems and so forth are
  excluded from the category. That is the sense of his distinction between
  allopoiesis and autopoiesis. But autopoiesis, which thus encompasses
  only autonomous, individuated and unitary entities that escape relations
  of input and output, lacks characteristics essential to living organisms,
  such as being born, dying and surviving through genetic phyla. It seems
  to me, however, that autopoiesis deserves to be rethought in relation
  to entities that are evolutive and collective, and that sustain diverse
  kinds of relations of alterity, rather than being implacably closed in
  upon themselves. Thus institutions, like technical machines, which, in
  appearance, depend on allopoiesis, become ipso facto autopoietic when
6


     they are seen in the framework of machinic orderings that they constitute
     along with human beings. (Guattari  7)

By extending the realm of autopoiesis to include technological and
institutional systems, Guattari’s ecosophy rejects the individualism that, as
we have seen in the previous chapter, pervades Varela’s version of autopoiesis.
Rather than delimiting autopoiesis to biological systems, Guattari’s interest
lies in the ways that biological, technical and social systems demonstrate a
shared set of characteristics surrounding circular causality, emergence and
dynamism.
    Illustrating this position, Guattari draws upon the work of Samuel Butler,
who deconstructs the notion of bounded species by providing the example
of pollinators and co-evolved reproductive organs:

     It is said that machines do not reproduce themselves, or that they only
     reproduce through the intermediary of man, but does anyone say that
     the red clover has no reproductive system because the bumble bee (and
     the bumble bee only) must aid and abet it before it can reproduce? The
     bumble bee is part of the reproductive system of the clover. … We are
     misled by considering any complicated machine as a single thing. In truth
     it is a city or a society. (Butler  59)

This move, as we saw in the previous chapter, takes us from understanding
individuals as wholes towards grasping them as sympoietic assemblages in
which agencies are diffuse, distributed and relational. Consequently, the
onto-epistemological shift required by the paradigm of mental ecosophy
leads from the Cartesian ‘I think’, to a collectivist ‘we think’, whereby
humans comprise knots within entangled meshworks which encompass
flows of machinic and biological life, information and affects, bound
together in complex feedback-based patterns.
   The reconceptualization of the self as an assemblage is one area where
Deleuze and Guattari depart from conservationist forms of environmentalism
which attempt to preserve the environment without enacting wider changes
to the self or society. For example, discourses surrounding environmental
activism frequently maintain a rigid nature/culture dualism, such as  .og
founder Bill McKibben’s  4) forceful claim: ‘We have ended the thing
that has defined nature for us – its separation from human society. That
separation is quite real.’ While McKibben accepts that scientists and artists
have argued against this dualistic ontology, he maintains that ‘on the inside
none of us quite believe it’ (McKibben  4). According to Deleuze,
Guattari and Bateson, this epistemological pathology – the maintenance of
the nature/culture dualism – is precisely what must be addressed.
   In contrast to Manichaean perspectives, Deleuze and Guattari 
contend: ‘Everything is a machine. …There is no such thing as man or nature
  


now, only a process that produces the one within the other and couples
the machines together.’ Rather than human culture and nonhuman nature,
Deleuze and Guattari present both humans and nonhumans as open systems
governed by the logics of self-organization and complexity. Contradicting
common-sense constructions of the self, this approach asks us to accept
that the boundaries between the self and the environment are not as clearly
defined as we typically imagine. ‘The individual mind is immanent but not
only in the body. It is immanent also in pathways and messages outside
the body; and there is a larger mind of which the individual mind is only a
subsystem’ (Bateson  67). Just as humans are dynamic assemblages
of life which are entirely dependent on the ecology of microbiota within our
digestive tracts to extract sufficient energy from food to maintain ourselves,
what we perceive to be a static and external nature is in fact a dynamic
multi-scalar assemblage which humans – collectively and individually –
inhabit as active subsystems.
   An ecosophy predicated upon negentropy and openness contradicts the
‘balance of nature’ argument presented by environmentalists such as Al Gore,
which erroneously advocates that nature was harmoniously balanced prior
to exposure to industrial–cultural activities which threaten the continued
existence of both nature and culture (Herzogenrath  . Nature is not
a closed and static system, but an open and turbulent one; anthropogenic
climate change does not present the destruction of stasis but involves
altering the speeds and viscosities of flows of change beyond the resilience of
many species which evolved at slower speeds of ecological change. This also
provides a key distinction between Deleuze/Guattari’s naturalism and that
presented by deep ecologists, such as Arne Naess  and Derrick Jensen
 ), whose appeals towards a pre-industrial ‘golden age’ (Naess :
   and natural mysticism present essentialist and idealized approaches
to a romanticized nature which allegedly requires protection from human
culture (Hayden  9).
   One of the key insights here regards the importance of speed; rates of
change are key to knowing whether systems can adapt to environmental
alterations. Deleuze  c:  articulates this as ethology, which he
defines as ‘the study of the relations of speed and slowness, of the capacities
for affecting and being affected that characterize each thing’. Ethology
presents considerable overlap with ecology, insofar as both are concerned
with flows or, more precisely, with the rates and viscosities of flows through
complex systems. Ethology also considers phenomena as existent within
a multiplicity of entangled scalar assemblages rather than as discrete and
isolatable individuals: ‘A thing is never separable from its relations with the
world. The interior is only a selected exterior, and the exterior a projected
interior’ (Deleuze c: . An ecological ethic then is concerned with
affective capacities and the differential speeds that alter flows of energy
and matter. In the current conjuncture, the speeds of 24/7 digital capitalism
6


are simply incompatible with those of earth’s ecological systems. As digital
capitalism is ultimately reliant upon these ecological systems, this temporal
imbalance places social and environmental systems on a pathway towards
ecological catastrophe.


                      Scale and network politics
Addressing this impending devastation, whereas deep ecologists promote an
anti-technological primitivism which seeks to enact the impossible task of
reverting cultural damage to pristine nature, Guattari elaborates,

     Wherever we turn, there is the same nagging paradox: on the one
     hand, the continuous development of new techno-scientific means to
     potentially resolve the dominant ecological issues and restate socially
     useful activities on the surface of the planet, and, on the other hand the
     inability of organized social forces and constituted subjective formations
     to take hold of these resources in order to make them work. (Guattari
      1)

Media ecology does not advocate abandoning technology; instead, it
promotes a fundamental reorientation of techno-scientific practices. In many
cases this means moving away from competition, marketization, enclosure
and commodification, towards creating new commons and publics designed
to benefit ecosystems rather than individuals.
   In addition to a critique of contemporary technics, this passage suggests the
failure of ecology as a science to present sufficiently moving and humanized
accounts of looming ecological crises. As Eric Heroux  83) observes,
‘While science continually delimits its statements for value-neutrality in the
traditional project to preserve its objectivity from subjective distortions, this
approach proves to be too simplistic and inadequate for a human ecology.’
Consequently, Guattari argues for the formation of an ecosophy predicated
upon an ethico-aesthetic paradigm which goes beyond the affectively
detached observational practices of an ecological science which provides
ecological insights, but not ecological technocultural practices.5 This model
of ecosophy is always inherently contested and politicized, rather than
gesturing towards a politically neutral objectivity, thereby situating media
ecology alongside the approach of political ecology which defines itself


5
  We should note that there are now several high-profile examples of climate scientists explicitly
taking political stances as a consequence of decades of scientific data doing little to produce
political change. This is exemplified by the scientists blogging at RealClimate, which is discussed
in chapter three. The critique of the Anthropocene as a technocratic and depoliticized concept,
however, demonstrates that this kind of engagement is far from ubiquitous.
  


through an explicit rejection of the apolitical approach to ecology often
encountered within the sciences (Robbins . Political ecology works
to denaturalize social and environmental phenomena, demonstrating
that they are not inevitable or simply how the world functions, but are
the consequences of particular power relations, and therefore can often be
altered to provide more equitable, sustainable and resilient outcomes.
   While there are currents within A Thousand Plateaus that advocate for
forms of micropolitics, this does not denote the dissolution of large-scale
collective action in favour of limited and local actions, a claim frequently
advanced by critics which cast poststructuralist or postmodernist positions
as being inherently conservative due to an alleged inability to confront
globalized power structures and political/ecological issues (e.g. Herman
 ; Myerson  Bookchin . Indeed, Guattari quite explicitly
proposes that successful responses to certain ecological crises require actions
that can only be realized on a global scale: ‘The ecosophical perspective does
not totally exclude unifying objectives such as the struggle against world
hunger, an end to deforestation or to the blind proliferation of the nuclear
industries; but it will no longer be a question of depending on reductionist,
stereotypical orderworlds which only expropriate other more singular
problematics’ (Guattari  4). Addressing issues such as world hunger
and deforestation – which we can productively extend to contemporary
crises such as climate change, the sixth mass-extinction and the global
plastics crisis – as global objectives clearly contradicts critiques which allege
that new materialist, poststructuralist and Deleuzo-Guattarian positions
exclusively focus upon localized forms of micropolitics.
   This focus on operating transversally, across multiple entangled scales
does, however, mark a distinction between the position of Deleuze and
Guattari and those associated with Bruno Latour and Actor Network Theory
(ANT). Media ecology and ANT share notable features insofar as they both
approach agency as a distributed and relational phenomenon and extend
agential capacities to nonhumans. However, media ecology’s engagement
with political conflict and scales that range from the imperceptibly fast
speeds of digital computation through to the global and geological scales
of the Anthropocene departs from the disinterest with power structures,
domination, oppression, inequalities and forms of resistance that are
associated with ANT. As Nick Couldry  .surmises, ‘What limits
the usefulness of ANT as a research tradition for media analysis and social
analysis generally is its relative lack of interest in the long-term power
consequences of networks’ establishment for social space as a whole and its
equality or inequality. For all its intellectual radicalism, ANT comes charged
with a heavy load of political conservatism.’ Latour would likely retort that
the notions of ‘long-term power consequences’ and ‘social space as a whole’
are both highly problematic, idealized concepts which are abstracted from
the specificities present in the particular networks of relations between actors,
7


therefore obscuring rather than producing insightful analyses. However, this
does not confront the fact that the micro-ethnographic approach of ANT
largely ignores hierarchies and inequalities, particularly when these traits
manifest themselves over prolonged spatio-temporal durations.
    Consequently, Latour  76) has argued that ‘it is useless to
denounce capitalism – on the contrary, denunciation only reinforces
it’. This formulation contends that discussing capitalism reifies it as a
totalizing system from which society cannot escape, so a range of diverse
economic practices and policies become an immutable and homogeneous
system. Latour contends that discussing capitalism in this way during the
Anthropocene inverts premodern approaches to nature and culture; nature
becomes radically modified by anthropogenic activity, while culture is
governed by fundamental and indisputable economic laws. Consequently,
Latour  .argues that discussing capitalism merely invokes affects of
helplessness and despair: ‘I get no other feeling than an increase [sic] sense
of helplessness. The mere invocation of capitalism renders me speechless.
… It might be best to abandon the concept entirely.’ Totally abandoning
the concept of capitalism, however, has the effect of making ideologies
predicated upon competitive individualism invisible and synonymous with
common sense. While we should distinguish between various capitalisms,
such as mercantile, social democratic and neoliberal capitalism, abandoning
the term entirely obscures historical continuities between earlier practices
of colonialism and exploitation that have shaped and continue to shape the
systems of trade, governance and media that exist today.
    Latour has argued that after an initial indifference to the term ANT, he
warmed to the metaphor of a researcher taking on the role of an ant, blindly
following pheromone trails in order to painstakingly compose a picture of
the subject of interest, a process which resembles the methodology employed
in studies such as Laboratory Life . While this form of analysis reveals
the inner workings of a particular scale of practice, the metaphorical figure
of the ant also suggests the limitations of ANT when working with larger-
scale systems. The ant is a member of a colony, a super-organism whose
collective intelligence far exceeds that of its members. The individual ant,
however, is destined to remain ignorant of the larger-scale phenomena which
are addressed by the scale of the colony. Consequently, when contemporary
media scholars such as Jose Van Dijck  have utilized ANT as a method
to approach social media, it has been accompanied by political economy in
order to additionally address macro-scale phenomena. However, this move
simply results in reinforcing the macro/micro and local/global dualisms
which ANT seeks to resist through its focus upon networks as a form which
allegedly ‘allow us to pass with continuity from the local to the global, the
human to the nonhuman’ (Latour  21).
    In contrast to ANT then, media ecology employs the concept of scale to
engage with political conflicts that exist within digital phenomena ranging
  


from voltages and pixels to global satellite networks and the geological
temporalities of plastic waste. While aspects of ANT, particularly nonhuman
agency, relationality, and the re-connection of nature and culture, resonate
with the conceptual and agential foundations of media ecologies, there are
significant differences surrounding their political and ethical imperatives.


       Rhizomatic and arborescent tendencies
Perhaps the ecological trope within A Thousand Plateaus which has
received the greatest volume of attention within accounts of digital media
has been the rhizome. The form of the rhizome closely approximates a
dynamic distributed network, whereby there is no central point from which
information flows. Indeed, a rhizome cannot even be said to be polycentric,
with multiple centres of competing power; rhizomes are decentred structures.
Information flows through horizontal channels of communication, rather
than being sent down a vertical causal chain. Any point of a rhizome can
connect to others, much like the peer-to-peer system of the internet. Without
a centre, or even rigid and determinate connections, rhizomes present
extremely resilient structures. Whereas an individual tree can be killed by
severing its centre, its trunk – the vertical foundation on which the leaves
and branches sprout forth from – you can plunge a stake into the centre of
a rhizome, only for it to grow around the ruptured ground, again echoing
the genealogy of the internet as a decentralized telecommunications system
that was designed to function after nuclear warfare had annihilated urban
centres.
   The homologies between the organizational structure of the internet –
as a dynamic, distributed network of peers who can freely connect to one
another – and Deleuze and Guattari’s evocation of rhizomatics has led a
strand of media studies to proclaim that the internet presents an example of
a rhizome (e.g. Froehling  Poster  7). Claims that a technological
assemblage is rhizomatic, rather than being dominated by rhizomatic
tendencies, immediately contradicts Deleuze and Guattari’s claims that the
rhizome is an organizational model which is always balanced to some extent
by arborescent tendencies: ‘There are knots of arborescence in rhizomes,
and rhizomatic offshoots in roots’ (Deleuze and Guattari  2).
   A rhizome is the subterranean mass of roots which comprise the stem of
plants including ginger, banana plants, tree ferns and cabbage trees. While
the horizontal growth of rhizomes can be understood as a counterpoint
to the vertical stem of most arborescent, or tree-like, structures, the key to
this metaphor is the fact that certain trees are also rhizomes, and specific
rhizomes are also trees. There is not, therefore, an arborescent/rhizomatic
binary opposition at work here but an interplay of verticalist and horizontal
tendencies which exist alongside one another. The configurations explored
7


by Deleuze and Guattari, then, are not idealized or essentialized forms
but denote attempts to produce a language geared towards mapping the
dynamic movements and becomings of complex assemblages across multiple
dimensions relating to control, hierarchy and freedom. Using the language of
the previous chapter, we can grasp these tendencies as the abstract machines
that exert pressures upon assemblages.
   While the peer-to-peer structure of the internet seemingly presents a far
more inclusive, horizontal and participatory structure than media such
as television and radio, it still contains lines of arborescence, such as the
tendency of search engine algorithms to direct traffic towards certain areas
(Hess .or to produce results that denigrate minorities (Noble .
the relatively few pathways that exist for transcontinental communication
through undersea fibre-optic cables (Starosielski . and the Domain
Name System that requires websites to be registered to an identifiable
individual owner (Galloway . Indeed, a cursory examination of the
political economy of contemporary internet reveals its domination by a
handful of corporations including Facebook, Google, Yahoo, Tencent,
Comcast, Verizon, Apple and Samsung. In other words, this is the antithesis of
an egalitarian and rhizomatic structure. The globalized telecommunications
industry of platform capitalism in the twenty-first century is in many ways
typified by a greater degree of arborescent centralization than its nationalist
twentieth-century predecessors, despite the fact that these networks and
platforms frequently offer users the chance to become (inter)active creators
of user-generated content. What we see then, are metamorphic combinations
of rhizomatic and arborescent tendencies within existing assemblages of
networked digital telecommunications.
   While media theorists have frequently misread Deleuzian tropes, it is
equally true that Deleuzian theorists have misunderstood digital media.
Ian Buchanan  articulates the issues with conflation of rhizomes and
the internet; however, he conflates the internet with the World Wide Web
and consequently erases the structural differences between the peer-to-peer
model of the internet and the web’s dependency upon the demarcation
between clients (periphery) and servers (centre). Indeed, this difference is
expounded by cybercommunists such as Dimitri Kleiner  .as the
difference between peer-to-peer communism and the client–server capitalist
state; whereas the internet is a system of equal peers, the web segregates
(most) users from the servers where information is stored, creating a
hierarchical division between data seekers and data storage.
   More recently, however, there have been commercially successful
movements towards harnessing the architecture of the internet to deliver
application-based services limited to specific mobile platforms such as
Apple’s iOS and Google’s Android. This usage of the internet presents a
significant departure from the open protocols of the web, which, as we shall
see, were designed to be hardware neutral. App stores instead form platform-
  


specific walled gardens which effectively lock users into using hardware
from particular vendors. While such moves are obviously advantageous to
these corporations, they effectively stifle competition, as prospective new
platforms are locked out of the millions of apps found in the iOS and Google
Play stores, and thus become a less attractive option. The prominence
of these closed, proprietary systems, which shall be explored further in
Chapter denotes the existence of additional arborescent tendencies within
the contemporary internet.
   Equally, we should note that the documents revealed by Edward Snowden
expose numerous ways that governmental and intergovernmental signals
intelligence agencies leverage the internet and other digital technologies such
as mobile phone SIM card identifiers to conduct surveillance at a scale which
would have been unimaginable prior to the widespread diffusion of mobile
networked digital technologies (Pugliese  Taffel . As David Lyon
   demonstrates, corporate and governmental surveillance are mutually
beneficial regimes, with the data used by social media platforms to provide
targeted advertising requiring these platforms to harvest a range of data
associated with real-name user profiles, providing a rich vein of material for
state agencies such as the NSA, who can access these data streams through
platforms such as Xkeyscore and Prism.
   Across this broad range of examples drawn from across political
economy, surveillance studies, software studies and critical infrastructure
studies, what we see are various new modes of hierarchy and corresponding
concentrations of knowledge and power that are made possible by the
digital telecommunications environment. Consequently, we must address
the internet not as a rhizome, but as an assemblage which contains both
specific rhizomatic tendencies and arborescent reterritorializations. Mapping
the rhizomatic and arborescent tendencies within media assemblages,
considering the potentialities for radical change and the ways that this
potential is negated and foreclosed through reterritorializations, therefore
becomes an important task for media ecology.


              Process philosophy and politics
Ecology focuses upon flows of energy and matter through entangled
assemblages. From the perspective of ecological process philosophy, a
central epistemological pathology is a misplaced focus upon individual
objects, such as an iPhone, rather than the diverse flows of energy and matter
that traverse multiple spatial, temporal and relational registers. An iPhone
requires the assembly of geographically distributed materials that have
taken millennia to coalesce. The energy required to mine, manufacture and
power the device requires the combustion of fossil fuels whose formation
involves a similarly slow temporality. Once discarded, the ‘waste’ materials
7


will continue to transform ecological systems for durations that humans
struggle to comprehend. iPhones additionally require terrestrial networks of
cellular towers, transoceanic fibre-optic cables and orbital GPS satellites to
function. While it is typical to think of the iPhone as a discrete possession,
an individual object, it is part of multiple flows of matter and energy whose
ecological affects are rarely explored or understood precisely because
we mistakenly conceive devices as existing in isolation from systems and
infrastructures.
   Consequently, an ecological ethics seeks to map these differing types
of movement and flow across ecological registers. Following Deleuze and
Guattari, we should grasp that not all change is desirable, lines of flight
(bifurcations) and modes of cancerous growth can lead to destructive as
well as emancipatory change:

     If you blow apart the strata without taking precautions, then … you will
     be killed, plunged into a black hole, or even dragged towards catastrophe.
     Staying stratified – organized, signified, subjected – is not the worst that
     can happen; the worst that can happen is if you throw the strata into
     demented or suicidal collapse, which brings them back down upon us
     heavier than ever. This is how it should be done: lodge yourself on a
     stratum, experiment with the opportunities it offers, find an advantageous
     place on it, find potential movements of reterritorialization, possible lines
     of flight, experience them, produce flow conjunctions here and there, try
     out continuums of intensities segment by segment, have a small plot of
     new land at all times. (Deleuze and Guattari  78)

Examples of catastrophic change where structures are blown apart into a
suicidal collapse include the sociopolitical upheavals surrounding regime
change in Iraq following the second Gulf War and Libya during the Arab
Spring. In both cases there were lines of flight that saw radical change
enacted upon populations and organizational structures; however, those
changes – which in both cases saw the removal of oppressive, hierarchical
dictators – preceded precisely the kind of collapse into even more repressive
systems that Deleuze and Guattari elucidate. While A Thousand Plateaus
presents an ethic that accords importance to speeds, affect and becoming,
this does not mean that rapid change is always valorized. Indeed, returning
to climate change, it is precisely the velocity of eco-systemic alteration and
the potential for crossing points of critical instability that pose such grave
threats to the continuation of life as we know it.
   This ethic of experimentation with flows and intensities is ontologically
grounded in the abstract machines of attractors and bifurcations which
were explored in the previous chapter. As these abstract machines are
equally constitutive of both organic and nonorganic dynamical systems
balanced far from equilibrium, Manuel DeLanda  53) argues,
  


‘We are all inhabited by processes of nonorganic life. We carry in our
bodies a multiplicity of self-organising processes of a definite physical
and mathematical nature – a set of bifurcations and attractors.’ In place
of the binary oppositions of (human) culture and (organic) nature, and
the living and inert substrata of nature, there stands what Deleuze and
Guattari describe as the machinic phylum. Not only are humans no
longer ontologically distinct from other life forms, but open systems of
living and nonliving matter are also connected and created by the same
nonlinear processes and are thus governed by the same abstract machines,
an arrangement which is equally applicable to tornadoes and tsunamis on
the one hand and tomatoes and turtles on the other.
    DeLanda consequently argues that human societies, as complex
dynamical systems, evolve via bifurcations between numerous temporarily
stable configurations and thus that ‘an ethics of everyday life, in these terms
would involve finding the relative viscosities of our flows, and giving some
fluidity to hardened habits and making some fleeting ideas more viscous –
in short, finding, through experimentation, the “right” consistency for our
flows … the exact consistency that would allow humanity to self-organize
without the need for coercion and war’ (DeLanda  53/ ). This
position suggests that autopoietic systems operate within a finely tuned
range described as the ‘edge of chaos’ (Kauffman . if a system is too
structured then its inability to flow limits its adaptive capacity to evolve and
adapt to changes within its environment. On the other hand, if a system is
too susceptible to small changes, that is, if it is too chaotic, then it will not
be sufficiently stable to maintain its existence. For dissipative systems to
proliferate requires this fine balance between order and chaos, change and
stability.
    There are, however, acute differences between the positions of Guattari/
Deleuze and DeLanda when we move from ethics to political methods of
collectively enacting ethical frameworks. This is visible with regard to their
respective stances concerning the enduring value of previous paradigms of
social change, particularly Marxism, which DeLanda  3) contends
was Deleuze and Guattari’s own ‘Oedipus, the little territory they did not
dare to challenge’. While DeLanda has a point, insofar as Marx’s labour
theory of value is predicated on a humanism which is inconsistent with the
approach favoured by Deleuze and Guattari,6 his critique neglects the ways
that Deleuze and Guattari depart from Marxist analyses, contending that
‘although Marx’s writings still have great value, Marxist discourse has lost
its value’ (Guattari  3/44).


6
 We should again note Marx’s  37/   argument surrounding the metabolic rift
of capitalism which robs both the worker and the soil. This posits that value derives from
interactions between humans and environment.
7


   This departure from orthodox Marxist perspectives have seen Marxists
such as Jodi Dean  .and Slavoj Zizek contend that Deleuze is
‘the ideologist of late capitalism’ (Zizek  93), that the focus upon
fluidity, dynamism, horizontal organization and minor politics are all
strategies which have become central to neoliberalism, and thus the
politics of A Thousand Plateaus has itself been reterritorialized. There is
undoubtedly an element of truth to claims that elements of the radical
politics of Paris in – especially notions that difference and cultural
diversity are in and of themselves somehow opposed to capitalism – have
subsequently been reterritorialized by neoliberalism. We should, however,
bear in mind that as theorists of movement and dynamics, of how radical
deterritorializing possibilities are nullified and integrated into capitalist
systems through processes of reterritorialization, Deleuze and Guattari’s
positions surrounding the dynamism of capitalism pre-empts precisely the
type of criticisms levelled by Dean and Zizek. Additionally, it is important
not to lose sight of the distance between the rhetoric of corporate flexibility,
horizontality, social and environmental responsibility and the underlying
practices which accompany them; a marked rise in mental health issues,
economic inequalities and ecological exploitation; a range of undesirable
impacts across the registers of the three ecologies. Consequently, claims that
the ethics and politics espoused by Deleuze and Guattari are compatible
with, let alone exemplified by, neoliberalism appear to be well wide
of the mark.
   Furthermore, we should emphasize the central thematics of the later
writings of Deleuze and Guattari when considering their relation to
contemporary politics. This chapter has explored Guattari’s ecosophical
position and his critique of integrated world capitalism, but of parallel
importance is Deleuze’s work surrounding societies of control. For Deleuze,
by the s – when neoliberalism was widely considered to have succeeded
Keynesianism – societies were decisively moving away from the disciplinary
model outlined by Michel Foucault , towards one dominated by
cybernetic prescriptions of control. Deleuze argues that this transition is
marked by the supersession of entropic mechanical machines by computers,
of Fordist factory production by flows of financial capitalism and marketing,
and of subjects who were predominantly formed in the distinct spaces of the
school, factory and prison to the datafied ‘dividual’, which is constantly
being modulated, analysed and reformulated.
   Deleuze resists the urge to rank regimes of disciplinarity and control as
being better or worse than one another, arguing, ‘There is no need to ask
which is the toughest or most tolerable regime, for it’s within each of them
that liberating and enslaving forces confront one another. … There is no
need to fear or hope, but only to look for new weapons’  b: 4). This
new societal logic requires novel strategies for resistance and constructing
ways of living that leverage the material affordances of computation, while
  


accepting that particular tools and tactics that had proven effective under
the logic of disciplinarily – such as trade unions – are likely to be less potent
within societies of control.
   The focus upon cybernetic-derived control as forming the diagrammatic
logic of contemporary societies, on the one hand, and radical ecosophy, on
the other, illustrates precisely why Deleuze’s and Guattari’s analyses are
central to my theoretical framework for media ecologies. Bearing this in
mind, it is difficult to reconcile their positions with claims that they are
ideologically aligned with neoliberal capitalism. Writing from a position
parallel to the tradition of Marxism, they seek to update and advance the
movement which animated the majority of anti-capitalist struggles for social
justice throughout the twentieth century. This contrasts with the approach
adopted by DeLanda, who identifies broadly with the left, and convincingly
argues for the need for left-wing politics to be grounded in a materialist
ontology – something he holds in common with Marxists – but which
largely fails to address the potential rejuvenation of Marxist or socialist
traditions, as he finds little value within them.
   My own writing is in this sense politically closer to that of Deleuze
and Guattari, and contemporary authors who have similarly sought to
situate their work in relation to contemporary social movements such as
Bernard Stiegler, Antonio Negri and Franco Berardi. These theorists stress
the necessity of connecting political philosophy to existing social structures
and activist movements, rather than existing within a theoretical vacuum.
In the following section, I explore how these contemporary theorists have
advanced arguments surrounding an ecology of the common, and how
specific calls around openness and commons can influence media ecology.


            Commodities and commonwealth
Since the s, society has undergone a paradigm shift which has seen
older forms of left-wing politics rendered redundant by transformations
to institutional structures. These changes are traced by Hardt and Negri
to the countercultural movements of the s and s, the hippy,
feminist, civil rights, LBGT, punk and other subcultural movements that
expressed dissatisfaction with the homogenized manifestations of mass
culture. Consequently, capitalism had to reinvent itself, reterritorializing
the field of identity politics and transforming politics into biopolitics:
‘Capital had to confront and respond to the new production of subjectivity
of the proletariat. This new production of subjectivity reached (beyond
the struggle over welfare) what might be called an ecological struggle. A
struggle over the mode of life’ (Hardt and Negri  69). Resultantly,
the contemporary mode of capitalism that Hardt and Negri term ‘Empire’
reaches beyond the Foucauldian disciplinary society that was based upon
7


vectors of subjectification that were tied to specific locations such as the
school, the prison and the factory.
   Instead, in a manner heavily influenced by the control society depicted
by Deleuze  b), Hardt and Negri contend that today power circulates
within a cybernetic system that leverages digital computers to provide
automated and continually modulating forms of feedback-based control over
subjects. Following Foucault, they describe this as a move from the power
exercised by a sovereign state to that of biopolitics, of control exercised of
and over life itself. From this perspective, difference is no longer a threat
to the singular composition of ‘the people’, but has become an opportunity
to increase profitability by selling countless alternative consumer lifestyles
to differentiated subjects. Hardt and Negri  38) therefore argue
that ‘postmodernist and postcolonial theorists who advocate a politics of
difference, fluidity and hybridity in order to challenge the binaries and
essentialism of modern sovereignty have been outflanked by the strategies
of power’.
   The exploration and critique of contemporaneous moves towards the
commodification of difference, identity and desire resonates strongly with
the central thematic of Franco Berardi’s writings. Like Negri, Berardi was
both a member of the Italian autonomist movement and a close friend of
Guattari. Berardi contends that since the s, societal changes wrought
by the introduction of digital computing technologies have seen the human
faculties, which in previous eras were considered to be constitutive of
the soul – our capacities for language, creativity, emotion and empathy –
become increasingly central to the economy through the commodification
of desire:

     Putting the soul to work: this is the new form of alienation. Our desiring
     energy is trapped in the trick of self-enterprise, our libidinal investments
     are regulated according to economic rules, our attention is captured in
     the precariousness of virtual networks: every fragment of mental activity
     must be transformed into capital. (Berardi a: 24)

Whereas for autonomist Marxists such as Berardi, this commodification of
human attention is characterized negatively as the new form of alienation
endemic to neoliberalism, it should be noted that for liberal capitalists such
as Clay Shirky , this development is lauded as the generation of a
‘cognitive surplus’. Here the emphasis is upon the integration of what had
previously been ‘unproductive’ leisure time into the production of monetary
value. While the novelty of this situation may be contested by analyses of
the political economy of the media which have long emphasized the role
of the audience commodity (Smythe . it is worth noting how voices
which both valorize and decry the marketization of attention highlight
the commodification of attention as a crucial feature of the contemporary
  


technocultural milieu. Consequently, exploring flows of attention will be
one of the central themes of the following chapter which explores ecologies
of digital content.
   Hardt, Negri and Berardi all agree upon certain strategies likely to effect
beneficial changes to this system, surrounding re-evaluating value and wealth.
Whereas neoliberal definitions approach wealth in a strictly quantitative
manner dependent on gross domestic product, entailing that wealth
equals money, a perspective Berardi  a:  derides as ‘economistic
fanaticism’, these authors instead propose that wealth is approached from
an ecological perspective, which surpasses a humanist theory of value.

  Whereas the traditional notion poses the common as a natural world
  outside of society, the biopolitical conception of the common permeates
  equally all spheres of life referring not only to the earth, the air, the
  elements, or even plane and animal life but also to the constitutive
  elements of human society, such as common languages, habits, gestures,
  affects, codes, and so forth. Whereas for traditional thinkers such as
  Locke and Rousseau, the formation of society and progress of history
  inevitably destroy the common, fencing it off as private property, the
  biopolitical conception emphasizes not only preserving the common but
  also struggling over the conditions of producing it, as well as selecting
  among its qualities, promoting its beneficial forms, and fleeing its
  detrimental corrupt forms. We might call this an ecology of the common –
  an ecology focused equally on nature and society, on humans and the
  nonhuman world in a dynamic of interdependence, care and mutual
  transformation. (Hardt and Negri  71)

The emphasis on understanding commons as distributed wealth, which
flows through not only human but broader ecological systems, is a crucial
concept in an ecological theory of value, which enlarges understandings of
wealth, contending that economic wealth is merely one form among many,
including personal psychosocial and physiological well-being, social justice
and equitability, environmental diversity and resilience.
   This ecologically informed approach to wealth and value, and the
importance of the commons to it, resonates with debates over Free/Open
Source and proprietary forms of software, whereby software is produced
either as a commodity or as commons. Indeed, software is seen as an
exemplary case study of how the infrastructure of the network society
affords the capacity for the distributed, collaborative construction of an
informational commons, a point that is emphasized by theorists such as
Michel Bauwens and Stiegler, who like Hardt and Negri claim that the free
software movement demonstrates that open systems which peer-produce
commonwealth provide positive externalities which exceed economic
impacts, benefiting subjective, social and ecological systems.
8


   If this activity merely takes place at the scale of software, however, there
remains a danger of creating an opposition between the ‘creative’, ‘valuable’
work of coding and the relatively undervalued task of producing the
hardware required for this creative activity. Consequently, Stiegler, Bauwens,
Hardt and Negri all contend that models of peer production must go
beyond the realms of software and Creative Commons-licensed content, and
extended to incorporate hardware: ‘Such an infrastructure must include an
open physical layer (including access to wires and wireless communications
networks), an open logical layer (for instance, code and protocols) and an
open content layer (such as cultural, intellectual and scientific works) (Hardt
and Negri  08).
   The remainder of this book takes up Hardt and Negri’s call by examining
models of openness and commonwealth across the differing scales of
content, software and hardware, exploring innovations and considering
the obstacles these systems face in terms of constructing media ecosystems
which enact alternatives to neoliberal models predicated on privatization
and consumerism.
   We should, however, sound a note of caution surrounding an
unconstrained valorization of openness. As we shall see in Chapter 4,
while the cooperativism of open source software may be antithetical to
a rigid ideology that competition in deregulated markets is the ideal and
most efficient economic form, in practice capitalism has always been more
dynamic than this and has repeatedly proved capable of reterritorializations
precisely because of this flexibility. As a result, we have seen corporations
such as Google, IBM and Microsoft successfully utilize open source software
to enhance profitability. While the early rhetorics of free and open source
software (much like those of the early internet) were of a radical alternative
to corporate capitalism, with Microsoft’s Steve Ballmer describing it as
‘communism’ and a ‘cancer’, much of that radical potential has now been
harnessed as a means of increasing corporate profitability while leveraging
the benefits of free or crowdsourced labour.


       A conceptual toolbox for media ecology
The ecological ethics and politics outlined in this chapter contend that
media ecology should present pragmatic alternatives to contemporary
practices that currently orientate societies towards ecological catastrophe.
Rather than casting ethics as the means to adjudicate actions as right or
wrong (using either deontological or consequentialist logics), an ecological
ethic is orientated towards an experimental praxis. It approaches conflicts
surrounding commodification and commons in digital assemblages to enact
bifurcations that form new mutualisms and which internalize externalities
in order to bring forth more equitable and resilient technocultures. This
  


ethic therefore departs from normative understandings of how schemata of
right and wrong behaviours for individual subjects are rationalized (Singer
 ), instead positing that ethics relate to collective assemblages rather
than atomized individuals. Building on the previous chapter’s exploration
of technology, complexity and agency, I have considered how ecology and
entanglement suggest a series of onto-epistemological and ethical shifts.
Based upon the framework of the three ecologies espoused by Bateson and
Guattari in conjunction with Hardt and Negri’s call for open physical, logical
and content layers, I am interested in how a parallel approach to media
ecology explores content, code and hardware in order to consider questions
of commonwealth, commodification, openness and ecological resilience
when exploring the political, ethical, affective and agential dimensions of
contemporary digital media systems.
    This chapter also explored how an ecological model of value departs
from the normative model provided by neoliberalism, whereby quantitative
economic value is presented as the primary determinant of wealth. An
ecological approach posits value at a multiplicity of levels, including subjective
well-being (where notions of the subject or mind always exceed the notion
of an atomized individual), social justice and ecological resilience. Whereas
neoliberalism is associated with the externalization of extra-economic costs
onto social and environmental systems, an ecological system of value seeks
to internalize these costs into systems of valuation. Gaining short-term
profitability at the expense of long-term ecological resilience repeats the
epistemological error described by Bateson, which will ultimately see the
collapse of the purposive system seeking to maximize individual (personal,
corporate, national or other individual forms) benefits while damaging its
ecology. Broadly speaking, ecology values difference and biodiversity over
monocultural uniformity and building commonwealth and community
over commodities. In each case, these pairs should not be reduced to binary
oppositions, but thought through as poles on a continuum which offer a
multiplicity of potential futures, following the Deleuzian logic of the AND.
The choice is not either/or, but between the precise balance of combinations
which are actualized at any moment.
    In Part of the book, I seek to apply an approach informed by the
theoretical content that has been outlined during Part to technocultural
systems viewed through the scales of content, code and hardware. While
in practice these components are entangled and therefore inseparable, by
approaching them individually we can see how various conflicts and tensions
sit within and across these three areas. Each of these chapters eschews the
familiar approach of the singular case study which frames the phenomena
being examined, instead assembling a diagram from numerous different
cases that foreground the multiple scales that are operative within the major
focus of the chapter. Rather than each scale being a singular and bounded
whole, they are approached as assemblages that are themselves composed
8


of assemblages. Following Barad, each section of the chapters forms a cut
that allows us to perceive these assemblages from a different vantage point,
and by making a series of these cuts we build a partial portrait of key sites
of conflict and interest within digital media ecologies. Rather than a smooth
space of uniformity, this method explores scalar ecologies through difference
and bricolage.
   The next chapter explores ecologies of content across various scales.
It begins by exploring discourses, practices and architectures surrounding
the economy of attention and thinking about how Big Data alters how
we comprehend content, before moving on to examine the specific case of
Climategate, an episode where an international news scandal developed
following the release of emails sent by climate scientists which were
originally published on climate change sceptic blogs. Methodologically, this
gleans different types of insight about information and attention from very
different methods of analysis. Following the theoretical approach which
has been outlined in Part of this book, media ecology seeks to synthesize
what emerges from these different scales of content about the relationships
between contemporaneous relationships between communication and
capitalism, activism and agency.
   The subsequent two chapters contained in Part operate on a homologous
basis. Chapter examines forms of software that range from firmware
to networking protocols, as well as exploring open source and closed/
proprietary modes of software production, whereas Chapter explores the
linear life cycle of digital devices where planned and perceived obsolescence
are key design parameters, in addition to specific issues that arise at numerous
key junctures within that cycle, from extracting raw materials from the crust
of the earth through to the disposal of toxic electronic waste. Each chapter
demonstrates how multiple scales are operative within and across each of
these entangled digital ecologies. Through a process of assembling, a range
of sites and spaces of conflict are introduced in order to consider the political
ecologies of digital media.
  

 Ecologies of
content, code
and hardware
         Flows of attention and data



Any attempt to address the political ecology of digital content is immediately
confronted with the enormity of the domain of contemporary digital
mediation. Whereas in the late-twentieth-century information and media
were (relatively) scarce due to the expense associated with production and
distribution, today digital information is inexpensively produced by billions
of humans and nonhumans. The enhanced scope of digital mediation sees
everything from bodily functions, such as the number of steps taken and
minutes slept each day, through to urban traffic and financial transactions
now producing digital data which is networked, databased and used to
predict and affect future behaviours (Amoore .
   In this new media ecology, exploring the affordances associated with
the speeds and volumes of digital mediation alongside the viscosities
and affects of flows of data and attention is key to grasping the spatial,
temporal and political transformations being enacted. Consequently, this
chapter begins by examining some of the claims surrounding informational
abundance and how this produces an economy of attention, whereby
attention displaces information as the systemically scarce property within
mediated ecologies. This is followed by a critical exploration of Big Data,
explicating numerous limitations associated with data-driven positivism,
a discourse which contends that newfound access to connectable,
real-time digital datasets renders critical, theoretical and qualitative
investigation redundant in the face of the allegedly self-evident empirical
truth. Furthermore, the assumed objectivity of Big Data frequently builds
existing inequalities into opaque technological systems that are difficult
to challenge precisely because they are mistaken for ideologically neutral
mathematical technics.
   After examining the attention economy and Big Data, I then shift scale
to consider how the public sphere has been altered by the move towards a
networked information ecology. Whereas the earlier parts of the chapter are
8


relatively technical, this section is more closely concerned with how human
actors within digital assemblages produce meaning and knowledge from
online encounters. To provide a useful overview of the ecology of content,
we must consider the technical, political–economic and human discursive
elements of the assemblage together, rather than solely focusing upon
human/discursive or nonhuman/material components. Using the example of
‘Climategate’, this section of the chapter provides a very different aperture
through which to view digital content, albeit one that again raises questions
surrounding attention and speed.
   The chapter concludes by bringing key insights into conversation with
the model of communicative capitalism, which argues that communication
has become a key driver of growth for contemporary capitalism.
Whereas early rhetorics of the internet as a postmodern public sphere
celebrated globalized, participatory communicative spaces as politically
emancipatory and democratizing, communicative capitalism argues that
digital communications effectively forecloses critical political debate. The
conclusion asks how it may be possible within this technocultural context
for the left to meaningfully engage in digital discourse that escapes the filter
bubbles constructed by social media algorithms and goes beyond tokenistic
clicktivism.


                        Economies of attention
A commonly heard mantra within vernacular discourses of digital culture is
that the internet is fundamentally divorced from the logic of informational
scarcity. Famously, Google CEO Eric Schmidt stated in that we
now create as much information every two days, as the human race had
created from the dawn of existence until ,1 somewhere in the region
of exabytes2 of data. By , the amount of information being globally
generated had risen to over zettabytes3 a year (Press . times
more information than was produced in . This rate of data growth is
still increasing, in no small part due to the massive increase of networked
digital devices commonly known as the internet of things which is forecast
to grow from two billion devices in to overbillion devices by
(Intel . As Jonathan Crary  notes, whereas the rhetoric
of the digital revolution suggests a singular moment of disruption followed
by stabilization, the reality is that the changes associated with digital


1
  There are doubts over the accuracy of Schmidt’s original claim, with a Berkeley University
study from finding that there had been exabytes of information produced in alone.
2
  An exabyte is bytes; one million terabytes.
3
  A zettabyte is bytes; one thousand exabytes/one billion terabytes.
  


technologies are increasing in pace. This begs the question as to whether
humans can or will ever ‘catch up’.
   It must, however, be emphasized that the expansion of data storage and
transmission is a thoroughly material process in which the microscopic
spatial scale of microelectronics affords the storage of vast volumes of data
in comparison to older media forms such as books. To highlight this, we can
consider the different amounts of space taken up by books and magnetic
hard disk drives by comparing the space occupied by the seventeen million
books held in the US Library of Congress with Google’s data centres. The
Library of Congress holds a ‘mere’terabytes of information, less than the
volume of data contained within a single rack within a contemporary data
centre.4 While information has been spatially condensed from the letters on
each page to the nanometre-sized magnetized grains found upon hard disk
drive platters or the similarly microscopically sized transistors within solid
state disk drives, it is in no way dematerialized or magically transformed
into free-floating information. The key point here is that computational
hardware operates at miniscule spatial scales and that the technical processes
which play a pivotal role in the expansion of digital culture are often poorly
understood by the general public and cultural theorists.
   While the microscopic materialities of media technologies are often
mistaken for dematerialization, the global networks of fibre-optic
cables, internet exchange points, cellular towers, server farms and other
infrastructure of the internet are equally obfuscated by the metaphors of
cloud computing and virtualization. Media ecology seeks to foreground the
material processes of mediation and the mattering of technology as a way
of drawing attention towards architectures of digital culture and the social
and environmental impacts associated with digital infrastructures, while
foregrounding the fallacy within the common-sense perception that the
internet is a static or homogeneous thing. As work in critical infrastructure
studies has demonstrated, technological infrastructures require an enormous
expenditure of energy, labour and attention to continue to function smoothly
as maintenance and repair are vital to their functioning, but this activity is
typically invisible as infrastructures only become visible when they cease to
function (Graham and Thrift  Mattern . Consequently, media
ecology approaches the internet as a dynamic assemblage, a constantly
evolving network of networks within which bandwidth – the carrying
capacity of the network infrastructure – is regularly being both maintained
and increased in order to cope with the growing volumes of devices and
traffic. Indeed, the changing bandwidth afforded by this expanding
infrastructure affects the form in addition to the volume of information
which is transmitted.


4
    In , a single 60-disk 4U server contains up to  TB of storage.
8


   When the internet predominantly utilized copper telephone wires and
dial-up modems with speeds of around kilobits per second, text and
heavily compressed low-resolution images were the main media forms
utilized. Following the advent of broadband internet, with speeds of over
1 megabit per second (1,  kilobits per second), video became viable as
a medium. YouTube, which was founded in , initially allowed videos
to be streamed at a resolution of  x  pixels. As the bandwidth and
connection speeds available to both YouTube’s servers and its users’ PC
connections increased, so did the maximum resolution of the videos, with
resolutions of  x  and then x  becoming available in ,
followed by xin and x(4K) in .5
   Alongside the spatial resolution of images, their bitrate is key to the file
size, and thus the requisite bandwidth required to view videos as a real-
time stream. Video streaming services such as YouTube and Vimeo use low
bitrate, long group of pictures (GoP) codecs (compression/decompression
algorithms – see Mackenzie .in order to allow high spatial resolutions
to be achieved while requiring relatively low bandwidths. A long GoP
codec compresses the information in such a way that most of the individual
frames in the video stream6 do not contain all of the pixel-level information.
Instead, these frames use macroblocks which scale in size (depending upon
the specific codec utilized) between 2x2 and 8x8 pixels based upon the
amount of movement within the particular region of the frame.
   Rather than requiring per-pixel data, the codec deals with macroblock
movement by applying a vector to them, thus requiring far less data to
accomplish the same movement as a pixel-level transformation. This does
not result in an apparent loss of quality to the viewer, as typically there
are large regions of a video image which remain unchanged between
most frames. For example, within a close-up shot there is only likely to be
significant movement around the mouth and eyes. The rest of the frame,
which has minimal movement, can be divided into larger macroblocks
without perceptibly altering the image. The exception, where it is important
to have full-frame pixel-level data is when there is a cut to another scene
or shot, where typically every pixel in the image changes, so each cut is
followed by a keyframe that ensures that this new arrangement is accurately
rendered.
   Long GoP codecs (such as the popular h  (MPEG-4) codec) often
utilize P (predicted) and B (bi-directionally predicted) frames, which depart


5
  As of , average connection speeds were 18.7 megabits per second in the United States
and 16.9 megabits per second in the United Kingdom. South Korea has the fastest average
connection speeds of 28.8 Mb/s. The global average is 7.2 Mb/s.
6
  Typically, frames per second for digital cinema, frames per second for regions which
historically adopted the PAL system for television broadcast and frames per second for
regions who traditionally used the NTSC system.
  


from I (intra-coded) frames which contain macroblocks which are coded
with reference only to the pixel data of the frame itself. P and B frames
both require prior decoding of other elements of the image stream to be
calculated; P frames use macroblock data from previous images to determine
the vectors of movement within the frame, and B frames use predicted data
from both preceding and following frames to determine the motion vectors
to apply to the macroblocks. The end result is the production of files whose
size is often miniscule in comparison to codecs which use pixel-level data
for each frame. For instance, the popular ProResand DNxHD codecs
used for editing and mastering both support xfootage at bitrates
up tomegabits per second, whereas material encoded at the same
spatial resolution using the h  codec for commercial Blu-ray disks have
a maximum bitrate of megabits per second and YouTube’s h  p
streaming uses a bitrate of megabits per second.
   A crucial point which arises from this is that because of the technicalities
associated with compression, the volume of information or size of a file do
not necessarily relate directly to changes in content which are perceptible
to end-viewers. Although high-definition YouTube videos are of noticeably
lower quality than Blu-ray disks because of the bandwidth constraints of
streaming, commercial Blu-ray disks are not usually of a noticeably lower
image quality for playback than ProRes files which are over four times
the size. While there are significant differences between the two files, these
are primarily in terms of the amount of computational power required to
decode the image stream in real time and the extent to which data which
can be manipulated and altered within the image. When keying7 or colour
grading video footage, the extra detail from the per-pixel information and
extra colour depth is useful in being able to accurately extract regions of
the image for specific postproduction processes, explaining why ProRes and
DNxHD are primarily employed within postproduction workflows, as these
advantages are largely irrelevant to the end users of media content.
   The point of engaging in this level of technical detail with regard to digital
data is that this case demonstrates that when discussing volumes of digital
information, the number of magnetized regions on a hard disk drive does
not necessarily reflect human perception of that data, especially in terms
of how much attention is required to engage with that information. The
  terabytes of information in the US Library of Congress would require
many lifetimes for a single human to read every page of each book, but that
same volume of information only comprises approximatelyhours of
uncompressed RAW 4K video footage.


7
 Chroma keying is the technical name for the digital postproduction process whereby a selective
colour is removed from a scene, typically a bright green and which is commonly referred to as
greenscreen.
9


    While the contemporary telecommunications ecology creates a
situation whereby the relative scarcity of information before the twenty-
first century has been replaced by concerns of information overload (Carr
 ; Andrejevic . it would be wrong to assume that this new milieu
removes the constraints of scarcity. While those humans who have internet
access do not suffer from an inability to acquire diverse forms of content,
the issue of scarcity moves from accessing materials to having the time to
meaningfully engage with this content. ‘A wealth of information means
a dearth of something else: a scarcity of whatever it is that information
consumes. What information consumes is rather obvious: it consumes the
attention of its recipients. Hence a wealth of information creates a poverty
of attention’ (Simon  0). Scarcity is thus understood as a systemic
property which exists within a technocultural assemblage composed of
human and nonhuman elements. Where the transformation of particular
lines within the meshwork increase flows, another area now becomes the site
of blockages or scarcity. Consequently, attracting and maintaining human
attention becomes a driving force within the digital economy (Crogan and
Kinsley .
    

data. Contrary to Anderson’s claims that Big Data means that we can forget
causality in favour of engaging with the straightforward empirical truths
of data, we see that in order to make any sense of information, we have to
contextualize data with additional research and analysis. The data alone
may provide answers to certain types of questions which ask ‘what’, but to
begin approaching more complex questions surrounding ‘why’ and ‘how’,
we need to go beyond simply regurgitating statistics and engage in the type
of analytical and synthetic inquiry which has long been the hallmark of
humanities and social science scholarship.
   As well as questions surrounding how we interpret data, there are crucial
questions pertaining to choices that are made when selecting data, especially
given the flexible, extensible and scalable properties of Big Data, which entail
that there will always be a multiplicity of options available to a researcher.
Were we to alter the dates examined to only focus upon those following
Dota 2’s announcement, it would be over four times as popular a search
term as ‘Climate Change’. If instead we chose to only look at the most recent
months’ worth of data, we would find ‘Dota 2’ was six times as popular
a search term as ‘Climate Change’, and twice as popular as ‘X Factor’.
Were we to simply examine the allegedly self-evident truth from that single
months’ worth of data, we would fail to see the annual cycle surrounding ‘X
Factor’, which unsurprisingly correlates with the show airing on television.
It is crucial, then, that the process which decides which connectable and
comparable datasets are selected for analysis reflects an informed decision-
making process, rather than arbitrarily suggesting that the ‘truths’ contained
in the data must simply speak for themselves.
   There are also questions which have to be raised about data access and
availability. Whereas Google Trends provides a normalized 0–  graph
denoting relative search popularity, if a researcher wanted the actual
numbers of searches, they would have to contact Google in order to obtain
this data, and the same is true were the data to be held by other social media
platforms such as Facebook and Twitter. While there are various forms of
freely provided or scrapable data available from these platforms, there is
additional data which is not made publicly available. Indeed, some of this
data is highly guarded, as it is used by the social media platforms themselves
for commercial purposes. Such data can be made available to researchers,
but usually come at a price, entailing that researchers working at well-
financed institutions are far more likely to obtain access to these additional
datasets. Of course, if the researcher worked directly for the social media
platform, accessing the raw numerical data would be significantly easier,
further denoting that within the sphere of Big Data there is a hierarchy
of who has access to varying levels of information, with the social media
platforms themselves positioned at the apex of this hierarchy.
   Intriguingly, this suggests that the more data a researcher has access to, the
less likely they are to adopt a highly critical approach to its origins, due to
10


the relationship and proximity to the corporate entity which has privileged
the researcher with access. Although there are a burgeoning number of
free and open source data analytic tools, which allow for various forms of
novel and interesting modes of research to be pursued, these tend to pale in
comparison to the capabilities of social media platforms, whose commercial
model is predicated upon the collation of Big Data and analysing this data
to provide complex models of an individual’s behaviour in order to target
advertising material accordingly, or to sell this data and associated patterns
and trends to other companies.
    In addition to hierarchies among researchers, the rise of Big Data and
computational data analytics creates novel forms of striation between the
users of social media, researchers and the owners of the platforms. The
first group comprises the billions of individuals who use the internet who
create data via their web search and browsing histories, tweets, likes, status
updates, online purchases, spatial movements while carrying a GPS-enabled
mobile computing device and so on. The other two groups are relatively
miniscule, and this novel form of hierarchy is described as the comprising
the ‘new “data-classes” of our “big data society”’ (Manovich  1).
Far from creating a cyber-utopian rhizomatic communication space marked
by increased levels of equity, we find that Big Data primarily benefits elite
groups who have access to raw datasets, the computational power to analyse
them, the education to utilize complex statistical tools and the time in which
to conduct such work. Put simply, those who are best placed to benefit from
Big Data are highly privileged elites. That is not to say that others cannot
benefit at all, but when mapping the affordances of Big Data, it is crucial
to highlight the structural context and hierarchies which are generated,
perpetuated or otherwise sustained.
    It would also be misguided to universalize the data presented in tools
such as Google Trends, naively conflating data from Google’s web searches
with an objective and accurate reflection of global public interest. While
Google is the most popular search engine globally, its employment is not
homogenous among all regions and states. For example, in China the most
popular search engine is Baidu (Arthur . in South Korea it is Naver
(Kocken . in Russia it is Yandex, and in Japan it is Yahoo (Graham and
De Sabbata . It may be that regional variation does not impact certain
projects dependent upon Big Data; however, given the rhetoric surrounding
Big Data as providing a series of neutral and objective facts, it is important
to highlight that these facts are far from universal. The partial nature of
the user base entails that while Big Data certainly captures sociocultural
material at scales and speeds which are unparalleled, this does not equate to
a straightforward demarcation of universal and global truths.
    Indeed, we must remind ourselves that internet penetration itself is
far from universal and that access (especially to broadband internet)
still broadly mirrors global economic divisions. Nick Couldry and Ulises
  Mejias have argued that the contemporaneous extraction of data from
the developing world that is then leveraged by the United States (and
to a lesser extent Chinese) digital platforms can best be understood as a
new form of colonialism that ‘combines the predatory extractive practices
of historical colonialism with the abstract quantification methods of
computing’ (Couldry and Mejias  . There is a real risk that Big Data
presents a neocolonial perspective on the world beneath the guide of a non-
ideological empiricism. Contrary to Kitchin’s attributes, Big Data are rarely
exhaustive. While they may capture a scale and scope which is far greater
than traditional methods, they do not typically examine entire systems, and
paying attention to who and what gets left out is vital to comprehending
both the insights produced and how pre-existing forms of exploitation can
be reified by big data.
   To summarize, we have seen that information gleaned from Big Data is far
from the presentation of straightforward and objective truths. When applied
to the sociocultural sphere, the results of data always require interpretation,
the selection of which data to present affects the interpretations which
emerge, data depends upon access to information which is often controlled
and selectively released by corporate platforms, and the various datasets
themselves are far from exhaustive, so understanding what gets left out is
important to contextualizing the claims being advanced. There is no such
thing as raw data (Gitelman .
   It is also important to emphasize that Big Data are themselves active
participants in the assemblages of contemporary technoculture which bring
forth particular modes of becoming. They have their own agencies rather
than simply being passive, transparent and neutral mediators which provide
insight into the world that produces them. As David Beer  0) quite
aptly states, Big Data ‘are not something that exist outside of the social
world. They circulate through it, reshaping it, altering and disrupting the
configurations of power and decision-making. These new types of data are
an implicit and integrated part of how the social world is performed and
enacted.’ Information, then, must be understood as a performative actant,
which actively shapes sociocultural ecologies, rather than simply presenting
a neutral reflection of a pre-existing culture. As Barad emphasizes in relation
to quantum physics, an apparatus of measurement is never entirely separate
from the phenomena being measured. Big Data are entangled with the
world they map, and in doing so, can be more aptly understood as creating
particular types of diffraction patterning within ecosystems in which they
are constituent parts, rather than as neutral objects that reflect reality from
an external vantage point.
   Evgeny Morozov  argues that an apt metaphor to employ here is
that data is more like an engine, an active driver of social change, than a
camera which passively and accurately records events. While I agree with
Morozov’s overall sentiment surrounding the active agency of data, the
10


metaphor he employs is somewhat problematic. A camera does not simply
record life; it is always in the process of bringing a particular viewpoint
on the world into being. The photographer’s choice of aperture, shutter
speed, sensor sensitivity (ISO), focal length, depth of field, focal point,
composition, exposure, picture style and so on all inform the way in which
light is recorded and consequently rendered into a photographic image, to
the point that the result of these decisions can be so dramatically different
that a viewer would be unable to discern that two photographs captured
with very different settings had been taken in the same place or of the same
subject. As Ansel Adams famously stated, ‘You don’t take a photograph, you
make it.’
   Furthermore, it is worth noting that many authorial choices, such
as aperture, shutter speed, sensor sensitivity and focal point are now
predominantly automated processes on many cameras (especially those
found in smartphones), which provide the human photographer with little
agency to affect parameters of the image beyond framing and composition.
These decisions are instead taken by technological elements of the human–
camera–environment assemblage, and through this process of automation,
the transparency of the decision-making process evaporates, leaving the
impression that the camera is simply recording the scene ‘as it is’. Images
and media are also, of course, performative actants that are capable of
driving social change rather than merely documenting it, so there is a
useful comparison to be drawn between Big Data and digital photographic
images. Both are selective processes which performatively enact social and
cultural change, but largely due to their automation by poorly understood
digital technologies are commonly mistaken as presenting straightforward
empirical truths.
   Big Data performatively act as ‘productive measures’ (Beer  . By
according additional importance to what is quantifiable and measurable,
Big Data create feedback loops whereby in order to be deemed valuable,
phenomena must be translatable to a measurable form, thereby further
enhancing the value of what can be quantitatively captured, which further
extends the scope and worth of Big Data, and so on. This process of
fetishizing the quantifiable, however, also sees the adaptation of behaviours,
as ‘people start to game the system in rational, self-interested but often
unpredictable ways to subvert metrics, algorithms, and automated decision-
making processes’ (Kitchin  27). Ascribing value to specific types of
knowledge alters social behaviours rather than simply measuring them.
   A prime example of this can be seen by exploring the field of viral
marketing; marketing that uses social media to create forms of advertising
and promotion that spread by utilizing the free labour of social media
users (Terranova  Fuchs . Utilizing existing communication
networks in this way provides a relatively low-cost form of advertising
  in comparison to traditional models such as billboards, television or
newspaper advertisements. Viral marketing additionally benefits from being
transmitted by individuals through trusted semi-personal social networks,
rather than appearing as commercial advertising. The notion of ‘going viral’
draws a homology with the form of replication seen in viruses, whose model
of self-division results in an exponential growth curve. Within a relatively
small period of time, the virus can become an epidemic because of this
nonlinear growth, and viral marketing seeks to achieve a similar pattern
of reach through utilizing the positive feedback loops involved in online
trending patterns, combined with a process of transmission through multiple
overlapping social networks. Once a piece of content begins trending, it
will be identified by various algorithms which promote apparently popular
material, be it for Google’s search engine, Facebook’s newsfeed or Twitter’s
trending category, thus affording it broader exposure and increasing the
likelihood that it continues to spread. Alongside this, as a topic begins to
trend a large number of news and niche topic blogs will begin to run stories
on the trending topic, again reinforcing positive feedback effects. This is the
type of network effect that causes the logarithmic power-law distribution
associated with preferential attachment.
   Whereas early internet scholarship tended towards rhetoric of a
postmodern public sphere (Barlow  Poster  Kellner . where
everyone becomes a pamphleteer and an equally important voice within
debates surrounding normative sociopolitical behaviours, subsequent
research revealed that the web does not function in the egalitarian manner
envisioned by early theorists of cyber-democracy. In , Albert-Laszlo
Barabasi and Reka Albert published a breakthrough paper entitled ‘The
Scaling of Random Networks’, which investigated the topography of various
complex networks, ranging from genetic networks, to citations within
science, to linking practices on the web, which concluded that there is a very
low probability that any given node within a network will be connected
to a high number of other nodes and a very high probability that the vast
majority of nodes will be very loosely connected or not connected to each
other at all.
   Whereas a normal distribution forms a bell curve around the mean, a
power law involves an exponential decay with a long tail and follows an
80/20 rule, entailing that the vast majority – per cent – of the links,
citations and other connections relate to only per cent of the nodes
within the network. Barabasi and Albert investigated the mechanisms by
which complex networks grow and formulated a hypothesis of preferential
attachment whereby new nodes connect themselves to nodes that are already
disproportionately well-connected, further entrenching the preferentially
attached position of these nodes. Reflecting upon the ramifications of his
research for internet democracy, Barabasi  6/57) argues that ‘the most
10


intriguing result of our Web mapping project was the complete absence of
democracy, fairness and egalitarian values on the Web. We learned that the
topology of the Web prevents us from seeing anything but a mere handful of
the billion documents out there.’ Rather than creating a postmodern public
sphere where everyone functioned as communicative equals, the attractor of
preferential attachment produces distributions where a handful of voices are
disproportionately visible while the vast majority of content residing in the
long tail of the web has few inbound links or visitors.
   Viral marketing productively highlights how economically privileged
actors exploit the nonlinear dynamics of digital media ecologies in order to
leverage them to the advantage of their clients. In the torrent of information
constantly being uploaded and shared online, the key is creating the initial
sense of interest in order to reap the benefits of the iterative network effects
associated with trending. In Trust Me I’m Lying: Confessions of a Media
Manipulator, public relations director and strategist Ryan Holiday outlines
how he would engineer interest and/or outrage in order to create initial
waves of interest which would later translate into a trending tsunami.
He stresses the importance of maintaining the appearance of authentic,
grassroots public sphere communication, which he would initially seed by
sending hot tips, scoops and scandals to small niche bloggers, a process
Holiday  8) describes as ‘trading up the chain’.
   These bloggers promote ‘their’ breaking story, which is then picked up
by larger blogs, and at this point the public relations agency intervenes,
using multiple accounts across numerous social media sites to upvote,
share, tweet and comment upon the story, ensuring that it generates a level
of interest which pushes it over trending thresholds, so it appears on the
front page of sites such as Reddit. As journalists from national newspapers
and internationally prominent blogs such as the Huffington Post monitor
the front pages of these preferentially connected sites, the content may now
become national or international news. Importantly, once a story, video or
other piece of content has passed the initial threshold for trending, it will
continue to be prominently visible in comparison to the vast majority of
content which resides in the internet’s long tail. Consequently, a PR firm
does not have to continuously promote material, it merely has to convince
trending algorithms of its initial value, and then there is the appearance
that the content will continue autonomously spreading itself (Morozov
 57).
   Indeed, Holiday  1) concludes that commercial blogs are merely
‘beachheads for manufacturing news. I don’t think someone could have
designed a system easier to manipulate if they wanted to.’ Whereas viewers
of these stories believe they are receiving authentic content from the public
sphere, they are in fact being fed cunningly disguised marketing material
by a public relations agency which leverages the functioning of algorithms
  which make trending content disproportionately visible and the desire of
bloggers to achieve reach, visibility and prominence within the cacophony
of online discourse. Consequently, we see a huge amount of ‘clickbait’
online, professionally produced content with unbelievable headlines
which are designed to attract user attention in order to produce clicks
and associated advertising revenue. Far from being a space of rational–
critical debate among informed citizens as the cyber-utopian visions of
the s internet as postmodern public sphere suggested, we instead find
that significant volumes of trending online content are manufactured viral
marketing or fake news stories that effectively game the logic of trending
algorithms.
    Questions surrounding fake news and post-truth politics burst into
mainstream political discussion in during the US election campaign and
to a lesser extent during the Brexit referendum. Exploring viral marketing
demonstrates how many of the tactics employed during these political
campaigns did not arise in , they had long been utilized online and their
efficacy in gaming trending algorithms and garnering attention had been
thoroughly demonstrated. Given their results in producing trending online
content, it is entirely unsurprising that these methods were subsequently
deployed in political campaigns by domestic lobbying groups and foreign
governments who sought to influence the outcomes of these crucial votes.
Additionally, the ability to gain significant financial reward from the
advertising revenues linked to fake political news saw large volumes of
material published from Veles, Macedonia, where posters claimed not to
care about the election outcome, their motivation was simply making (easy)
advertising money (Kirby . This further demonstrates how Big Data
and trending metrics do not merely measure a pre-existing social world;
they are performative actants which are key to understanding contemporary
economies of attention and the genealogies of digital transformations to
mediated political discourse.
    Understanding Big Data, then, does not simply mean being able to glean
information from the growing array of tools that scrape information from
social media, search engines, mobile networks and so on. It requires an
engagement with the limitations and gaps of the tools, technologies and
datasets, interpretation and contextualization of the information itself,
and a nuanced understanding of how the assemblages of Big Data do not
objectively measure and reflect contemporary technoculture but create a
value system in which quantifiability and measurability are highly prized
qualities. Put simply, the aperture afforded by Big Data introduces a way
of understanding the world which privileges certain measures over others,
and, consequently, an array of sociotechnical actors have altered their
behaviours in order to manipulate metrics by gaming the algorithms which
underpin them.
10


          Climategate or the scandal that wasn’t
Whereas the attention economy and Big Data raise numerous issues over
ecologies of content and information at relatively large scales that address
the volumes of content being generated within contemporary computational
ecologies, I now turn to a more localized example which examines forms
of human meaning making and activist intervention that occur within
the dynamics of digital ecologies. Widely known as ‘Climategate’, this
manufactured scandal surrounds the theft and publication of emails and
associated documents from the Climatic Research Unit (CRU) at the
University of East Anglia (UEA). This presents a case where climate blogs,
which usually reside within the long tail of the internet, led to a major
international news story, which had a marked effect on public perceptions
surrounding climate change and correlates with a noticeable increase in
web searches for the term ‘Climate Change’. Focusing upon Climategate
and climate change blogging, a heavily contested field where scientists,
activists, NGOs, industry think tank and other social actors converge upon
a technocultural space, presents a very different type of insight from that
drawn from exploring Big Data and the attention economy, and media
ecology is interested in what emerges across and between these different
ways of examining flows of digital content.
   On November , a UEA server was accessed by an unauthorized
party and data was copied and uploaded to the RealClimate blog9 and
inserted into a spoof posting after the RealClimate server was hacked
(Schmidt . Gavin Schmidt deleted the posting and informed the CRU
of the security breach later that day (RealClimate a). On November,
prominent climate change sceptic blogs such as Climate Audit and Watts
Up with That began reposting a small number of the emails that had been
anonymously uploaded to the Air Vent, another sceptic blog, advancing
claims that these materials proved that climatologists had been involved
in manipulating data, perverting IPCC processes and preventing sceptical
papers from being published in peer-reviewed journals (McIntyre 
Watts a).



9
  RealClimate was launched in , and describes itself as ‘a commentary site on climate
science by working climate scientists for the interested public and journalists. We aim to
provide a quick response to developing stories and provide the context sometimes missing in
mainstream commentary’ (RealClimate . Among the scientists who contribute to the blog
are Gavin Schmidt, director of the NASA Goddard Institute for Space Studies (GISS), Eric Stieg,
an isotope geochemist at the University of Washington and Michael E. Mann, a member of the
Penn State University faculty, and lead author of the controversial Mann, Bradley, Hughes 
paper which recreated global temperatures for the last thousand years using palaeoclimatic
proxy data, resulting in the ‘hockey stick’ graph, which provides evidence that current warming
is unparalleled in speed and scale over the last thousand years.
     The email which received the most prominent attention was from
CRU director Phil Jones and stated, ‘I’ve just completed Mike’s Nature
trick of adding in the real temps to each series for the last years (i.e.
from onwards) and from for Keith’s to hide the decline.’ This
was highlighted for appearing to suggest that Jones was using a form of
deception, to ‘hide the decline’, which sceptics claimed ‘appears to discuss
a method of overlaying data of temperature declines with repetitive, false
data of higher temperatures’ (Morrissey . These blog posts attracted
thousands of comments, the majority of which argued that the emails
revealed that scientists had deliberately misled the public over the threat of
climate change.
   One of the changes to the media ecosystem afforded by the rise of user-
generated content platforms such as blogging is the enhanced opportunities
for experts to provide rapid feedback to stories without the intermediaries
of professional journalists who traditionally functioned as gatekeepers to
entering mediated debate. It is worth noting, as Oreskes and Conway  :
 –11) demonstrate, that as recently as the s, scientists who sought
to respond to erroneous allegations published in the mass media often
had no outlet for rebuttal other than scientific journals or organizational
newsletters, media with comparatively miniscule audiences. The changing
media ecology afforded the scientists who author RealClimate the capacity
to directly participate in this debate, communicating with a large and
geographically dispersed public.
   As early as November, one day after sceptic blogs began publicizing the
emails, blogs began publishing material that contextualized the information
being used to suggest scientific misconduct and fraud. This clearly denotes
a way that blogging has altered the pace and scale of the dissemination of
scientific information. The blogosphere enables information from scientist–
activists to flow far more rapidly than within previous technocultural
assemblages. The contemporary media assemblages – the computational
and networking hardware; blogging software, networking protocols,
web browsers and operating systems; and the users and social practices
of blogging, commenting and online discourse – allow knowledgeable
individuals to contribute to public debates at a speed and scale which
significantly departs from the situation that existed before the mid- s.
   With specific regard to the widely circulated claims surrounding the ‘hide
the decline’ email, the RealClimate group responded:

  The paper in question is the Mann, Bradley and Hughes  Nature
  paper on the original multiproxy temperature reconstruction, and the
  ‘trick’ is just to plot the instrumental records along with reconstruction
  so that the context of the recent warming is clear. Scientists often use the
  term ‘trick’ to refer to ’a good way to deal with a problem’, rather than
  something that is ‘secret’, and so there is nothing problematic in this at all.
10


  As for the ‘decline’, it is well known that Keith Briffa’s maximum
  latewood tree ring density proxy diverges from the temperature records
  after (this is more commonly known as the ‘divergence problem’
  – see e.g. the recent discussion in this paper) and has been discussed in
  the literature since Briffa et al. in Nature in (Nature,  ,  –82).
  Those authors have always recommend not using the post part of
  their reconstruction, and so while ‘hiding’ is probably a poor choice of
  words (since it is ‘hidden’ in plain sight), not using the data in the plot is
  completely appropriate. (RealClimate a)

Both the original RealClimate thread from November and a follow-up
published on November (RealClimate b) exploring other accusations
stemming from the emails, received over a thousand comments, where users
queried the context and meaning of the emails, and the scientists authoring
the blog were able to respond and advance the discussions.
   By the time the press had picked up the story, thousands of comments
providing various opinions and insights from observers including scientists,
sceptics, other academics and interested members of the public had been
published online and the merits of their opinions discussed. The speed and
scale of these informational and attentional flows departs significantly from
previous media. The increased pace of communicative exchange within
networked online media affords rapid responses and detailed dialogic
encounters which in this instance can be understood to resemble particular
elements of the vision of a postmodern public sphere outlined by early
theorists of internet discourse and democracy.
   The model of the public sphere expounded a communicative realm which
was conceived as affording universal access and being separate from both
the state and private (market) interests, thus affording a space of democratic
debate and dialogue between equal participants who were able to logically
and rationally debate sociopolitical issues (Habermas . Habermas
presents the eighteenth- and nineteenth-century coffee shop culture of
bourgeois Europeans as a normative model of the public sphere, contending
that earlier, premodern times did not sufficiently differentiate between
public and private realms for a public sphere to exist, and that the genesis
of the mass media saw the public sphere of face-to-face debate replaced
by one predicated upon the consumption of industrially produced cultural
commodities. Habermas consequently argues that ‘the world fashioned by
the mass media is a public sphere in appearance only’  71), that
debates over cultural norms are primarily framed by elite corporate interests
which are reflected in a commerce-driven culture of competition.
   There have been numerous notable criticisms of the Habermasian model
of the public sphere, which claimed that universal access was a necessary
requirement, but presented a normative model which excluded women
(Fraser .and the working classes (Negt and Kluge . that is, the
  vast majority of the population. Despite this, the concept is still widely
applied and was heavily adopted by early internet scholars (e.g. Poster 
Bohman  Kellner .who argued that the peer-to-peer architecture
of the internet provided a technological means for remedying the issues
Habermas associated with the introduction of the mass media surrounding
the centralization, commercialization and distortion of communications
and the public sphere:

  Democracy involves democratic participation and debate as well as voting.
  In the Big Media Age, most people were kept out of democratic discussion
  and were rendered by broadcast technologies passive consumers of
  infotainment. Access to media was controlled by big corporations and a
  limited range of voices and views were allowed to circulate In the Internet
  Age, everyone with access to a computer, modem, and Internet service
  can participate in discussion and debate, empowering large numbers of
  individuals and groups kept out of the democratic dialogue during the
  Big Media Age. Consequently, a technopolitics can unfold in the new
  public spheres of cyberspace and provide a supplement, though not a
  replacement, for intervening in face-to-face public debate and discussion.
  (Kellner 

Although such claims must be tempered by the corporate domination of the
contemporary internet at the levels of platforms, code and infrastructure,
it is still important to note that certain groups are empowered to engage
in public discourse through networked digital media. Indeed, the tension
between the multiple ways that the internet both empowers and encloses
individuals and groups is a key concern regarding activism and agency in
contemporary technoculture.
    Having initially been published on niche climate denial blogs, with
rapid responses from blogging climate scientists, the Climategate story was
picked up by major newspapers within a few days, demonstrating the flow
of information from blogs into print and broadcast media. Far from existing
in opposition to, or standing outside of, previous media ensembles, what
we observe here is the way that digital debates flow across and through
a media assemblage which includes ‘old’ and ‘new’ media forms. Indeed,
one of the primary ways that content which begins life on niche blogs can
effect widespread change is through the process of diffusing through forms
such as newspapers, television and websites maintained by corporate media
networks which typically have a far greater reach than forums such as
climate blogs.
    The CRU officially responded to the escalating scandal on November,
stating that it would conduct an internal investigation into allegations
(CRU a), alongside a comment from Phil Jones explaining the ‘trick’
to ‘hide the decline’, repeating information published several days earlier
11


on RealClimate. The relative slowness of UEA/CRU in addressing the story
was criticized as ‘a textbook example of how not to respond’ (Monbiot
 ): the CRU knew that the emails were circulating among climate
sceptics from November, and during the week that they remained
silent, the story became an international scandal. This highlights how the
blogosphere, and the contemporary media ecology in general, corresponds
to a huge increase in the pace at which information travels, circulates and
is reproduced.
    Whereas in previous media ecologies waiting six days to respond to a
story may have been considered adequate, by it was not. The CRU
unveiled another press release on November (CRU b), and when
this failed to stem the tide of criticism, they announced that Phil Jones
would step down as CRU’s director pending the results of an independent
review of the scandal (CRU c). Whereas the CRU initially announced
that UEA would conduct the review with external support, the growing
sense of crisis entailed that this position was no longer tenable, and in
early December they announced a wholly independent investigation, to
be chaired by Alastair Muir Russell (CRU d). There were additional
investigations into the affair announced by the UK House of Commons
Science and Technology Committee, an external reappraisal of the science
in the CRU’s key publications via the Scientific Assessment Panel, and Penn
State University conducted an internal inquiry into allegations resulting
from the email publications which pertained to research misconduct
allegedly perpetrated by Michael Mann while tenured at PSU (Penn State
University .
    The first investigation to publish its findings was the UK House of
Commons Science and Technology Committee , which effectively
exonerated Jones and the CRU, finding no evidence that data had been
falsified, the public had been misled, or that collusion to keep opposing
perspectives out of peer-reviewed journals had occurred. While the report
did criticize the handling of freedom of information requests, criticisms
were directed at UEA rather than Jones or the CRU. The report’s publication
generated further heated debate on climate blogs, with RealClimate 
and similar anti-sceptic blogs quoting key findings approvingly while
highlighting that the conclusions validated statements which these blogs had
made throughout the course of the scandal (Hoggan  Romm .
Sceptic blogs covered the report by either focusing on sections which called
for greater transparency in climate science (Watts . the fact that one
MP voted against parts of the report (McIntyre a) or claiming the
report was a whitewash (McIntyre b).
    The other inquiries: the Scientific Assessment Panel, the Penn State
University investigation into Michael Mann’s conduct and the Muir Russell-
led independent review, all corroborated the UK House of Commons Science
and Technology Committee report. Although they made minor criticisms
  of CRU’s work,10 the reports unanimously concurred the CRU had not
falsified data, subverted academic peer-review protocols or undermined
IPCC processes. The allegations which made worldwide headlines were
unsupported by analysing the emails, the CRU’s publication record and
evidence from interested parties, which included numerous sceptical
climate bloggers who made depositions to the inquiries. Indeed, the reports
emphasized that CRU’s ‘rigour and honesty as scientists are not in doubt’
(Muir Russell et al. .
   Responses from climate blogs to these inquiries followed a similar pattern
to events following the UK House of Commons Science and Technology
Committee report. Blogs which broadly support the IPCC position, who
had consistently rejected the alleged significance of the emails, saw each
successive report as vindication of their judgement, whereas sceptic blogs
criticized each successive report as part of an ongoing whitewash. Criticism
was aimed at the choice of the panels’ personnel, the terms of reference
adopted, the analyses and conclusions that each panel provided. It is
important to emphasize that sceptic blogs refused to accept conclusions
drawn by the independent reports which contradicted their preconceived
positions. Having published material claiming that the emails were clear
evidence of data manipulation, fraud and subversion of peer-review/IPCC
processes, having received tens of thousands of (largely) approving comments
echoing these sentiments, and having seen these allegations blossom into a
highly publicized international scandal, this process of becoming a major
news story sufficiently reinforced ideological preconceptions for sceptics to
simply reject contrary conclusions drawn by the independent investigations.
   This strongly suggests that far from being a hotbed of rational analytical
discourse, the blogosphere reinforces and entrenches existing views.
Consequently, sceptic responses to independent investigations support
Castells’s  69) claim that ‘information per se does not alter attitudes
unless there is an extraordinary level of cognitive dissonance. This is because
people select information according to their cognitive frames.’ As the
sceptical bloggers, commentators and readers had established an ideological
framework connecting the emails to fraudulent behaviour and conspiracy
on behalf of the scientists, their response to the repeated exonerations of the
scientists was to re-frame the inquiries themselves within an establishment-
led conspiracy, rather than abandoning the aperture through which they
had previously perceived events. Even when exploring online contestation
of a scientific discourse allegedly steeped in objective rationality, what



10
  The Scientific Assessment Panel report criticized minor aspects of the statistical methodologies
employed alongside the informal nature of CRU’s internal procedures and the Muir Russell
review criticized the CRU for lacking openness regarding how they dealt with freedom of
information requests.
11


we find instead of cold analytical reason is a highly affectively charged
situation, where subjective preconceptions and ideology effectively structure
proceedings.
   One particularly noteworthy blog post which appeared after the
conclusion of the first two investigations was published by Scott Mandia
 ), who employed web analytics to examine media responses to both the
original scandal and subsequent exonerations of Jones and Mann. Mandia
used the search terms ‘Phil Jones’ ‘Climatic Research Unit’ and ‘Michael
Mann’ ‘Climatic Research Unit’ compiling results published during the two
weeks after the story originally broke and subsequently for the two weeks
after the scientists’ initial exonerations. This was done as both a web-wide
search and a Google news search, in order to compare and contrast web-
wide coverage with that published by newspapers. This is notable insofar
as it demonstrates bloggers using computational tools to publish metalevel
analyses of media coverage, comparing publications across media, a task
which until recently would have required a full-time researcher to compile
the data. In this instance, we see that networked digital technologies not
only afford novel ways of communicating, or alter the pace and scale of
communications, but also enable unique methods for obtaining insights.
While we have considered some of the caveats and limitations associated
with Big Data, this case demonstrates one way they can be utilized by
interested citizens to provide a scale of analysis which previously would
have required a significant mobilization of time and resources, and thus
been impractical for non-professionals.
   Mandia’s analysis foregrounds a huge discrepancy between the volume
of reporting which accompanied the initial scandal and subsequent
exonerations of the scientists. Interestingly, the scale of the discrepancy is
far greater in news media than web hits; whereas the ratio for web pages
was roughly 3:1 for accusations compared to exonerations, the ratio for
news headlines was closer to 10:1. As we have seen, many online pieces
responding to the inquiries accused them of participating in a whitewash;
however, they still provided responses to the inquiries, whereas many news
outlets which covered the initial accusations simply ignored the subsequent
rebuttal of these claims produced by the independent investigations. This
tendency to focus on the politics of scandal, whereby sensationalist headlines
trump subsequent rigorous inquiries which find more mundane conclusions,
correlates with Castells’s  observations regarding the centrality of
scandal politics to commercial media networks in the network society.
   Climategate, however, provides compelling evidence against Castells’s
 60) claim that ‘while skilful manipulation of information and the
shrewd weaving of facts and fabricated evidence, increase the impact of the
scandal, it is the raw material provided by the extent and significance of
the wrongdoing that ultimately determines the effects of the scandal’. The
extent and significance of the wrongdoing here proved to be very low. Had
  the initial story presented honest, rigorous scientists who did not properly
deal with a torrent of disingenuous freedom of information requests from
sceptics, allied with mild criticism that certain publications could be slightly
improved through closer collaboration with statisticians, the story would
have generated few headlines. Indeed, as Mandia demonstrates, when
these conclusions emerged, the press lost interest, as the lack of sensational
headlines made the exonerations of the scientists a far less marketable story
within the dynamics of the attention economy.
   The Climategate scandal therefore presents a useful precursor to the
subsequent torrent of fake news and post-truth politics. This includes
Gamergate, where the harassment of female game designers was ‘justified’
by a nonsensical critique surrounding standards in game journalism;
Pizzagate, where during the presidential election it was ludicrously
claimed that Hillary Clinton’s emails contained coded messages that
connected senior officials within the Democratic Party to a child sex
ring; and aberrant claims during the Brexit referendum campaign that
the European Union was poised to ban tea kettles. It is wrong, however,
to present such non-events as a decisive break from journalistic history.
There are plenty of cases where high-profile scandals were manufactured
to provide a rationale for actions that may otherwise have been unpopular.
For example, the Second Gulf of Tonkin incident that Lyndon Johnson
used to pass the Gulf of Tonkin resolution through the Congress in ,
providing a legal basis for commencing military action against North
Vietnam, is generally regarded to be a complete fabrication. No actual
attack on the US Navy occurred. Similarly, the Nayirah testimony, where
a fifteen-year-old Kuwaiti girl heart-wrenchingly recalled invading Iraqi
soldiers removing babies from incubators and leaving them to die was used
to justify US intervention in first Gulf War. This was later found to have
been false testimony delivered by the daughter of the Kuwaiti ambassador
to the United States.
   What we see with the recent fake news scandals, then, is not a decisive
break whereby ‘real’ news has been replaced by fake news, but that in the
context of the digital attention economy it is possible for well-organized
groups with political or economic motivations to garner significant
quantities of attention through producing sensationalist communications
that are designed to produce doubt in received wisdom. While this occurred
in the past, it was largely limited to powerful political and economic elites,
however, today the ability to manufacture fake scandals has seemingly
been democratized by digital media; small but dedicated groups are now
able to use strategies such as trading up the chain to help game trending
algorithms and garner significant attention for events which did not occur.
In the cacophony of online communications, rational–critical debate is often
less prominent than affectively charged sensationalism that successfully
leverages the productive measures of Big Data.
11


   A final outcome of the CRU email scandal worth discussing was the
formation of a page on the RealClimate blog on November which
presents a co-creatively sourced catalogue of links to data sources and
code relating to climate science. Unlike most blog posts, which appear in
reverse chronological order, this page was placed in the top navigation
bar of the site alongside links such as ‘home’ and ‘about us’ so that it
received extra prominence. The page sought to demonstrate that far from
being a secretive clique, climate scientists have made vast amounts of
data publicly accessible via the internet for interested parties to explore
and examine, with links to overdifferent datasets covering global
and regional temperature records, tidal gauges, oceanic heat content,
aerosols and cloud formation data, snow cover, palaeoclimatic data and
reconstructions, source code used in global and regional climate models,
and data visualization tools.
   Prior to the existence of the internet, the majority of this data would have
been inaccessible outside the institutions where they are maintained. The
dynamic nature of many of the datasets, such as those for global and regional
climate, further problematizes the notion of printing and distributing this
information, as it rapidly becomes outdated. Now, however, this enormous
volume of data is available for perusal by any member of the public with
internet access, and the RealClimate catalogue presents an interface to
this distributed database. The practice of creating the database, initially
undertaken by the scientists running RealClimate and subsequently enlarged
through the participation of the site’s users, additionally demonstrates
how professional expertise and crowdsourcing can be combined to create
comprehensive catalogues of openly available online data. Changes to
modes of data storage, access and communication combine here, affording
far wider access to a massively enlarged database of available information
pertaining to the global climate, with the interface residing on a prominent
climate blog.
   Alongside the enhanced pace and scale of the online debates and feedback
loops surrounding the scandal, the emergence of blog postings utilizing web
analytics to provide meta-coverage of the events and the crowdsourced
catalogue of online climate data present examples of how contemporary
sociotechnical assemblages afford new modes of interrogating and
scrutinizing information. The contemporary technical ensemble affords
different types of analyses and peer-based open data exchanges that simply
did not exist within previous media ecosystems. While the existence of these
resources invites public review of these datasets, the caveats surrounding the
politics of data still apply. In order to meaningfully review these materials,
the preconditions are not just access to the data but an assemblage of
statistical and coding knowledges and practices allied with computational
power, relevant software tools and the time to conduct this work. Meaningful
access, then, is still dependent upon meeting preconditions which demarcate
  that only a tiny fraction of internet users can interrogate such datasets.
While open access to content does enhance public participation in science,
we are still only talking about a very specific, elite group as the public here.
   This does not mean that such moves are not beneficial or do not assist in
democratizing scientific knowledge, but this raises questions surrounding the
precise manner in which the term ‘democratization’ is defined in discourses
surrounding digital technologies. Claims that technology, the internet,
open access and user-generated content platforms are democratizing are
frequently encountered, but what is often left to reader’s imagination is
what this process of democratization entails. While the term democracy is
etymologically derived from the Greek demos (people) and kratos (rule or
power) and thus denotes the rule of the people, the capacity for collective
governance is quite different from indicating a broadening of access to
particular resources. Although participation in public debate and dialogue
can assist with democratic deliberation and decision-making, frequently
we find that with reference to digital discourse, democratization has been
employed to demarcate providing access to information, spaces for debate
and/or users producing mediated content; but corporate ownership of
platforms and infrastructures persist, so all too often little actual power for
meaningful collective governance is ceded to users.
   The purpose of analysing Climategate has been to explore how climate
change blogging reflects key elements of the contemporary media ecosystem
surrounding pace, scale and democratic engagement (or its lack thereof).
While this resembles existing modes of discursive media analysis, the focus
has been upon exploring how these developments affect flows of attention
and information, implementing the ethological methodology proposed by
Deleuze that examines the relative speeds, viscosities and affective capacities
which was outlined in the previous chapter. The example of Climategate
demonstrates that blogs are capable of generating international news stories
through ‘viral’ diffusion, initially between blogs, then between blogs and
blogger/journalists before appearing in print and broadcast media and
reaching hundreds of millions of people spread across the globe. From the day
the scandal broke, detailed, accurate information was available via climate
blogs published by experts who previously would have been excluded or
constrained from mediated conversations. However, that information failed
to significantly affect discursive structures surrounding the scandal in the
mainstream media and public perception.
   During the twentieth century, the primary problematic for media activists
was access, which fostered a perspective that obtaining access would
necessarily entail a transformation of public opinion through rational debate
in the public sphere. Within digital media ecologies, while many more actors
have access to mediated communications, the scarcity of attention denotes
that crucial challenges involve getting noticed within the cacophony of
information and mis-information.
11


         Content in communicative capitalism
The disparate strands of this chapter, which have examined the attention
economy, Big Data and Climategate, collectively suggest rejecting claims
that digital culture empowers citizens by forming the type of postmodern
public sphere which was passionately advocated during the early days
of the internet. While there is enhanced access, debate, discussion and
participation, the type of rational–critical debate that was envisioned is
often notably absent, and in its place, we find a combination of fake news,
clickbait headlines and hyperbolic rhetoric proliferating from commentators
whose ideological framework appears unmovable by evidence-based debate.
The prevalence of democratic and progressive values such as participation,
access and dialogue within networked telecommunications is part of their
integration into the circuits of contemporary capitalist formations, which
require massive volumes of communication and the data it produces to
commodify and capitalize affective relationships, a situation that Jodi Dean
refers to as communicative capitalism.
   Within this new mode of social relations, Dean  1/22) argues:

  Multiple opinions and divergent points of view express themselves in
  myriad intense exchanges, but this circulation of content in dense,
  intensive global communications networks actually relieves top level
  actors (corporate, institutional, and governmental) from the obligation
  to answer embedded in the notion of a message. Reactions and rejoinders
  to any claim are always already present, presupposed. In this setting,
  content critical of a specific policy is just another story or feature in a 24/
  news cycle, just another topic to be chewed to bits by rabid bloggers.
  Criticism doesn’t require an answer because it doesn’t stick as criticism.
  It functions as just another opinion offered into the media-stream. …
  The proliferation, distribution, acceleration, and intensification of
  communicative access and opportunity result in a deadlocked democracy
  incapable of serving as a form for political change.

For Dean, networked digital telecommunications superficially appear to
provide solutions to contemporary crises of political participation through
affording individuals the ability to create, remix, connect and share
content. However, the circulation of digital content is not a precursor to the
formation of infrastructures capable of creating meaningful social change;
it simply affords ever faster circulations of content, underpinned by an
economic model based upon the commodification of communication and
communities via surveillance in order to serve targeted advertisements.
   From this perspective, the emphasis within a substantive fraction of
contemporary activism upon communicative action and raising awareness,
rather than building the infrastructures necessary to revitalize or form
  alternatives to the socialist, communist and trade union movements which
animated the majority of twentieth-century political struggles appears
somewhat misguided. That is not to suggest that political struggle does
not require communication, as raising awareness is often a necessary
precondition for politically motivated action, but within the current
context, all too often activist endeavours are limited to online awareness
raising, which is understood as an end in itself, rather than being a step
towards affecting social or ecological justice. According to Dean, within the
technocultural context of the commodification of communicative practices,
such actions, on the one hand, become sites where huge amounts of energy
and passion are invested, as commentators pour untold hours of time into
debates which are seen by only a handful of readers and fail to change
anyone’s mind, and, on the other form, a convenient avenue for attention
poor citizens to feel like they have undertaken meaningful political action
by liking a campaign or retweeting a message. In both cases, the underlying
issue usually remains entirely unaffected by these communicative actions,
returning to Bateson’s cybernetic epistemology, they exemplify differences
that do not make a difference.
   Climate change blogs present a useful example here. Scientists have
spent countless hours producing content that debunks myths surrounding
climate change. Similarly, sceptic bloggers have spent vast quantities of
time advancing claims that anthropogenic climate change is not happening,
is a fraudulent proposition concocted by devious climate scientists,
or has been dramatically overstated and is unlikely to have significant
adverse ecological effects. The question which Dean’s framework of
communicative capitalism posits in the face of all of this endeavour is,
what has this actually achieved beyond participating in flows of digital
content? One could quite convincingly argue that climate blogging has
failed to achieve anything substantial. From RealClimate’s launch in 
until , global atmospheric carbon dioxide concentrations rose from
  ppm toppm, while global leaders repeatedly failed to agree on
the kind of binding international emission reductions that would increase
the probability of averting catastrophic climate change.11 While enormous
quantities of time have gone into providing detailed knowledge of the latest
scientific publications and explaining why commonly asserted counter-
narratives are scientifically improbable, this has clearly not translated into


11
   While saw agreement at the UN Climate Change Conference in Paris, featuring an
ambitious commitment to reduce climate change to a 1.5-degree centigrade increase over pre-
industrial temperatures, the agreement requires each nation to produce a voluntary emission
cut to achieve this. Based upon the cuts pledged, estimates suggest that by we are on
course for between three and four degrees of warming. Voluntary solutions to environmental
crises have rarely achieved positive outcomes, whereas legally binding mandatory frameworks
have typically been far more successful.
11


effective action by governments compelled by their citizens to prevent
Anthropocenic ecocide.
   Conversely, one might argue that climate sceptics have had tremendous
results; however, such conclusions dramatically overstate the influence
of niche blogs. More realistically, we would have to consider the impact
of climate blogs within the broader context Dean outlines, whereby
communicative contestation and the appearance of ongoing debate simply
fuels further communicative action without really impacting the issue. One
important conclusion to be drawn here is that the notion of the blogosphere
as a single and whole space, as suggested by the metaphor of a sphere, is
undermined by the manner by which oppositional discourse traverse climate
blogs. There is little in the way of reasoned debate between scientists and
sceptics, and over time, the tone of the exchanges has become increasingly
hostile, in part due to the frequent personal harassment and character
assassination perpetrated by climate sceptics (see Oreskes and Conway
 ).
   Rather than a single smooth space of conversational unity premised
upon the utopian fantasy of wholeness associated with the internet as
the privileged place of global politics, blogs are better characterized as
occupying numerous distinct ‘blogipelagos’ (Dean  8), segregated and
disconnected islands. While there is at least some (albeit highly antagonistic)
debate across climate blogs, where there is a shared topic of conversation,
the same cannot be said for online content emanating from disparate groups
such as American teenage fan-fiction sites, Pakistani religious blogs and
Spanish social movements. Although networks of hardware and software
physically connect users to a common technological infrastructure with
shared platforms and protocols, there is no conversational unity within an
overarching public sphere that connects these users or discourses.
   While blogs have undoubtedly increased access to and participation in
mediated debates over climate science, it is worth considering the range
of actors who benefit from this access. There are scientists and concerned
and informed activist/citizens, but, equally, there are actors such as Marc
Morano, who runs the Climate Depot blog, as a paid employee of the
conservative Committee for a Constructive Tomorrow lobby group, and Joe
Romm who is similarly employed to manage the Climate Progress blog for
the Centre for American Progress, a progressive advocacy organization. The
contributions of actors such as Morano and Romm, in terms of volume
of posting, typically far outweigh those of professional scientists such as
the RealClimate team, as their time is not predominantly occupied by
another occupation, demarcating an important way that online discourse is
stratified by temporal engagement and attentive capacity. There are always
hierarchies implicit within internet usage and users, those involved in climate
blogging are not restricted to the politics of access; those encumbered by
having to sell their productive labour capacity to attain financial stability
  are disadvantaged in comparison to either actors who are employed to
contribute or those actors whose financial position is sufficiently secure to
permit them to focus on blogging full-time. Within the blogosphere, then,
actors do not function as equal participants once they have overcome the
material barriers to entry, the material conditions of everyday life continue
to dictate the levels to which individuals can participate.
   In many cases the practice of paying bloggers to propagate a specific
viewpoint represents the extension of pre-existing media campaigns by
organizations who now adopt astroturfing (Lee .– fake grassroots social
media and UGC campaigns – to complement more conventional lobbying
and advertising strategies. While the creation and adoption of networked
digital media technologies opens up new possibilities for democratic debate
and participatory culture through a process of deterritorialization, the
subsequent deployment of these media technologies and practices as tools of
propaganda by hierarchical organizations indicate their reterritorialization.
Indeed, the links between blogging practices and economies of attention
highlights that this terrain provides a far from egalitarian space, as certain
actors are able to mobilize attention via leveraging economic power to
provide vast amounts of content propagating particular positions.
   Astroturf blogs financed by private interests clearly problematize notions
of a public sphere delineated through its separation from both the apparatus
of the state and the realm of private interest. In this case, private interests
act via astroturfing within the (apparently) public spaces of the internet
to propagate ideological positions, utilizing economic leverage to attain
attention and influence. Consequently, climate blogs present contested spaces
which combine personal and private interests, which can be understood
as part of a wider cultural trend towards the amalgamation of public and
private realms of life, resonating with McLuhan’s notion of ‘retribalization’
in which the pre-industrial situation where public and private were not
recognized as discrete realms returns in a mutated form.
   Where McLuhan’s analysis departs from the contemporary situation,
however, is that apparently public persons within grassroots campaigns are
frequently revealed to be private interests. While the boundaries between
public and private are blurred by networked digital media, there remain
differences in how discourse is perceived if it is believed to emanate from
concerned citizens acting upon ethical impulses, or corporate actors
purchasing time and space to propagate commercial perspectives. Here
we are reminded of Holiday’s claims that blogs are an ideal medium for
manufacturing stories which masquerade as independent journalism but
are in fact duplicitous corporate promotional materials which bypass any
notional ideals of fourth-estate gatekeeping and quality control.
   In considering the hierarchies found within the technocultural assemblages
of climate blogging, it is also important to consider the backgrounds of the
human bloggers. The shared characteristics of the authors of science and
12


sceptic blogs include their geographical situation in affluent countries, the
receipt of a high standard of education, with the overwhelming majority
of prominent climate bloggers also being Caucasian men. In other words,
prominent bloggers are socially, economically and educationally privileged
individuals. Indeed, the bloggers’ ability to effectively communicate is
undoubtedly a factor leading to the standing of their blogs, which can be
(at least) partially attributed to their privileged social background; well-
educated individuals are likely to have superior communicative and technical
skills, so their content is more likely to become preferentially attached. This
provides further evidence discrediting contentions that the internet presents
a smooth space whereby offline hierarchies are rendered irrelevant. Indeed,
conspicuously absent from these debates are voices from the developing
world or indigenous communities, that is to say, from the groups likely to
be most heavily impacted by climate change.
    That hierarchies based on pre-existing social inequalities arise within
networked digital ecosystems suggests claims that the technological
foundations of the internet are inherently democratic, because they allow
users to contribute content, are woefully naive insofar as they typically ignore
how these technologies propagate existing and novel forms of inequality.
Social media involves billions of users generating monetary value for the
venture capitalists and shareholders who own multibillion-dollar platforms.
Equally, following Dean, we should note that the social context which has
emerged alongside blogs and social media is one of increasing economic
inequality, with the richest twenty-six individuals on the planet owning more
wealth than the poorest per cent of the global population (Oxfam :
6). The rise of participatory, user-generated media has not brought about
a more equitable and democratic global culture, in fact, quite the opposite
appears to be occurring. This power-law distribution of wealth eerily echoes
that which is evident within online popularity; far from making everyone
an equal participant in a global public sphere, the dynamics of the internet
ensure that a handful of voices have a greater reach than ever before, while
the vast majority of content and contributors are almost invisible.
    The burning question for activists, then, is how to amplify particular
messages so that they become visible beyond the immediate niche communities
in which they arise. In many cases, this involves the process of ‘trading up
the chain’, being noticed by prominent blogs and commercial media outlets,
whose exposure will make a story trend. The pitfall here, as we have seen,
is that this strategy has been more successfully pursued thus far by public
relations firms and right-wing lobby groups who utilize their economic and
attentive advantages to regularly attain a level of exposure which is rarely
achieved by grassroots activists. Climategate provides a useful example of
how an assemblage of climate denialist bloggers, conservative think tanks,
lobby groups and politicians created a scandal which successfully impacted
global attention towards climate change. While it is undoubtedly important
  for left activists to pursue similar media strategies, we must note that such
endeavours can be understood as a continuation of what Michel de Certeau
   describes as the tactics of the weak, which stand in contrast to the
strategies of the economically, attentively and politically powerful.
    In addition, it is important to emphasize that Dean’s framework
of communicative capitalism emphasizes the significance of digital
telecommunications to contemporary neoliberal ‘creative’ economies and
that within this technocultural context the specificity of messages often
is lost within massive data flows. ‘In communicative capitalism … the use
value of a message is less important than its exchange value, its contribution
to a larger pool, flow, or circulation of content. A contribution need not be
understood; it need only be repeated, reproduced, forwarded. Circulation is
the setting for the acceptance or rejection of a contribution’ (Dean  7).
If activists are to meaningfully effect and alter this situation, then
communications must not be focused purely upon awareness raising, which
effectively falls into the circulatory system Dean describes, but instead must
be geared towards organization and mobilization.
    Rather than simply attracting attention, such moves must instead
focus upon taking actions that go beyond liking and retweeting content
or signing online petitions. Indeed, this is a key conclusion which both
Dean and Jonathan Crary  draw from exploring the always-on 24/7
system of communicative capitalism; effective action in the twenty-first
century does not require withdrawal from the digital domain (which would
entail a functional invisibility), but a tactical reorientation of the activist
employment of digital telecommunications towards projects which bridge
the perceived, although illusory, division between a digital world of debate
and an embodied one of actions, and which resists the commodification of
communication.
                 Ecologies of software



Whereas the previous chapter focused upon flows of attention and
information to interrogate key issues surrounding digital content, this
chapter examines software, exploring a range of examples which elucidate
how differing forms of software are connected to political, economic, ethical
and agential concerns. These examples foreground key areas of contestation
surrounding the mode of production, agencies and capacities of software.
This process begins by examining prominent debates relating to software,
notably those arising within the academic field of software studies and the
free and open source software movements, before shifting scale to assemble a
range of specific cases that traverse different elements of software ecologies,
ranging from firmware and device drivers through to networking protocols,
digital rights management and algorithmic filter bubbles. In each of these
cases, conflicts foreground issues primarily pertaining to either the agencies
of code or the contested and multiple affordances of free and open source
software.


                      Software and society
Although studying software is not new, most approaches originate within
computer science and human–computer interaction. These perspectives
are predicated upon utilitarian understandings of software which entail
mastering the relevant programming languages in order to use existing
software platforms or produce new software applications. In both cases,
the framework departs from how software’s connectivity with other
aspects of digital culture interfaces with material politics, affects and
nonhuman agencies. These questions are, however, addressed by the field
of software studies, which emerged from digital/new media studies in the
late noughties. Software studies argues that despite the growing breadth
  and scope of technocultural, sociopolitical and economic systems which are
at least partially governed by software, that ‘software is often a blind spot
in the wider, broadly cultural theorization and study of computational and
networked digital media’ (Fuller  .
    Software studies theorists contend that approaches from new media and
cultural studies primarily focus upon content, typically overlooking how
software functions at registers outside of user interfaces and experiences:
‘It isn’t just the external appearance of digital media that matters. It is also
essential to understand the computational processes that make digital media
function’ (Wardrip-Fruin  i). As a result, software studies enacts a
shift away from the study of content and user experience, towards examining
agencies and power relations within the data processes, algorithms,
languages and codes which create user experiences. Importantly, this shift
challenges assumptions that software systems are ideologically neutral,
utilitarian tools, instead proposing that a plethora of values are built into
the ways software functions and malfunctions, and that this under-theorized
area requires reassessment if we are to create more realistic understandings
of how software shapes societies. Lev Manovich summarizes this approach,
with an oft-repeated mantra from one of the key texts of early digital media
studies: ‘From media studies, we move to something which can be called
software studies; from media theory – to software theory’  8).
    The erroneous assumption that software is immaterial is contested
by software studies, which argues that such claims present a theoretical
blockage which diverts attention from the materialities and agencies of
code. In contrast to the dematerialization of computational technologies,
software studies articulate

  an understanding of the materiality of software being operative at many
  scales: the particular characteristics of a language or other forms of
  interface – how it describes or enables certain kinds of programmability
  or use; how its compositional terms inflect and produce certain kinds of
  glitches, cross platform-compatibility, or ease of sharing and distribution,
  how through both artefact and intent, events can occur at the level of
  models of user subjectivity or forms of computational power, that
  exceed those of pre-existing social formatting or demand new figures of
  knowledge. (Fuller 

This foregrounds the heterogeneous agencies that are associated with the
human and nonhuman elements of digital assemblages: those of programmers,
hackers and users; the unintended consequences of code associated with
glitches, bugs and exploits; and those pertaining to the automated decision-
making and machine-learning capabilities of software. The adoption of a
materialist approach to software resonates with ecological methods, refuting
12


the alleged dematerialization of software present in approaches predicated
upon virtuality or immateriality.
    Software is comprised of binary code, which is materially encoded at
scales imperceptible to humans. The binary data stored on a mechanical
hard disk drive (HDD) exists as discrete magnetized areas on a disc-shaped
platter, which rotates at speeds between 5,  and 15,  revolutions per
minute. The direction of magnetization indicates whether each microscopic
region represents a or 1, and binary code is composed of these samples,
whose magnetization is detected and altered by the HDDs mechanical read/
write head.1 Each magnetic sample is a single bit of information, with bits
comprising a byte, and contemporary HDDs commonly contain up to 8
terabytes of storage (8, , , ,  bytes). While binary data can be
perfectly copied, shared or operated upon by a diverse range of algorithmic
processes, these affordances stem from the specific material capacities of the
networks of hardware, software and culture employed.
    In the case of HDDs, the ordering of magnetization can be copied exactly,
providing a congruent copy of the binary code found in the original locations.
We should, however, be aware that process of bitrot entail that copies will
degrade over time, so perfect copies do not retain their congruence over
protracted durations (Dharini . Indeed, digital media often have
lifespans far shorter than their analogue predecessors. When Pixar sought to
produce the DVD version of Toy Story, they discovered that up to a fifth of
the material had been corrupted due to the short lifespan of the mechanical
HDDs used for digital storage. Despite the film’s digital heritage, the DVD
version had to be constructed by scanning a celluloid print (Bollmer :
66–7).
    Fuller’s description of software’s materiality is particularly attuned to ways
that software is always enmeshed within dynamic assemblages, focusing
not only upon properties of the software but also upon its affordances and
capacities which are actualized through interconnections with other entities,
providing a discursive framework which parallels an ecological methodology.
Ecology is the study of interactions and transferences of energy between
entities rather than entities in themselves, so software studies’ stated aim
‘to map a rich seam of conjunctions in which the speed and rationality, or
slowness an irrationality, of computation meets with its ostensible outsiders
(users, cultures, aesthetics)’ (Fuller  .closely corresponds to the
relational and ethological emphases of media ecology.
    While these aspects of software studies inform the analyses contained
in this chapter, where software studies and media ecology differ, at least
on some accounts, regards the status accorded to software with regard to
contemporary culture. Manovich argues that not only does software add a


1
    For a detailed explication of the functioning of HDDs see Kirschenbaum 
  new dimension to culture but ‘our contemporary society can be characterized
as a software society and our culture can be justifiably called a software
culture – because today software plays a central role in shaping both the
material elements and many of the immaterial structures which together
make up culture’ (Manovich a: emphasis in original). The danger
of ascribing centrality to any feature of the massively complex globalized
networks of hardware, software, content, humans, natural resources,
techniques, knowledges, legal, political and social structures that collectively
comprise ‘culture’ or ‘society’ is that this immediately relegates other areas
to the margins. While software is important to contemporary spaces –
particularly the urban and domestic spaces which Rob Kitchin and Martin
Dodge  describe as Code/Spaces, whereby spaces are functionally
dependent upon automated software systems – there are questions to be
asked about the abstraction of software and code from the technocultural
ecology in which it is always situated.
   A useful counterpoint here is Castells’s  9) genealogy of
contemporary technoculture, which is predicated around the convergence
of microelectronics, optoelectronics, telecommunications, biogenetics and
cultural factors. This meshwork of technological evolution thus forms a
digital ecology which is not structured around a single central point, be
it software, microelectronics, information or another entity, but which is
instead approached as an entangled assemblage. Although software plays a
crucial role in contemporary societies, this role is a relational one which is
entirely dependent on a series of other, equally critical areas, such as reliable
sources of electrical energy, silicon, rare earths and other essential materials
for digital infrastructures.
   Declaring the centrality of software in computational assemblages
risks positioning the ‘creative’, ‘cognitive’ or ‘immaterial’ mode of labour
associated with programming (and by extension certain modes of content
creation) as the central site for producing value within digital assemblages.
This move relegates the ‘physical’, ‘material’, and increasingly automated
practices of hardware production, which as we shall see in the following
chapter are often predicated upon highly exploitative labour practices and
environmentally harmful activities, to a peripheral or marginal position. This
type of separation between valuable cognitive labour and devalued physical
tasks seemingly replicates a division of labour in which the creative class
(Florida .of the urban developed world are lionized as intellectual, agile
and digitally articulate, in glaring contrast to those whose physical labour
is increasingly devalued by the growing presence of an industrial reserve
force of the unemployed, and whose opportunities for employment are
increasingly squeezed through the proliferation of automated technologies.
Media ecology’s commitment to social and environmental justice requires
that such value systems are made visible and challenged, rather than
relegated to a marginal position within a so-called software society.
12


    Media ecology therefore stresses entanglement within the context of
the networks of energy and matter, software and hardware, humans and
technology, which comprise contemporary technocultural structures, rather
than presenting software as the central element of these structures: the
epistemological consequences of entanglement are that we cannot separate
particular aspects of a technocultural milieu, either declaring code to be
of central importance or studying its capacities in isolation from dynamic
entanglements across multiple scales. This means not solely studying
software but mapping assemblages of software, hardware, content and
culture, and foregrounding the material politics involved.
    Contesting the claim that contemporary society can be described as
software culture or a software society does not, however, denote that software
is not integral to contemporary technocultural systems. Most audio, visual,
literary and other forms of mediated content are today predicated upon the
use of digital publishing tools which afford content creation, distribution and
modification in ways that were unthinkable with previous technocultural
assemblages. Elsewhere, our economies, commerce, transport, legal and
welfare infrastructures rely heavily upon software to provide automated or
semi-automated processes for their continued ability to function. Indeed,
the automated trading systems analysed in Chapter delivers one exemplar.
Equally, as Kitchin and Dodge demonstrate, contemporary urban spaces
from airports to supermarkets would effectively fail to function without
software (although of course they would also fail were they to be without
electricity or wired/wireless network connectivity, indeed with no electricity
at all they would be less functional than with a software fault). Given the
proliferation of digital technologies within contemporary society, and that
all such technologies require various forms of software in order to operate,
it seems reasonable to agree with the contention of software studies scholars
that software forms an area whose importance currently exceeds the amount
of attention commonly bestowed upon its study.


                     Free, open and closed
Although the materiality of software has largely been under-explored, the
mode of production associated with free and open source software has
received substantial attention. Both free and open source software refer to
ways in which software is produced and licensed, and this licensing affects
how the software can subsequently be modified and distributed. While
the terms ‘free software’ and ‘open source software’ are frequently used
interchangeably, and there is significant overlap between them, particularly
in their practical applications, the two terms reflect diverging philosophical
standpoints on software production and distribution. Although the blanket
terms ‘FOSS’ (Free and Open Source Software) and ‘FLOSS’ (Free, Libre and
  Open Source Software) are commonly used, the philosophical incongruences
between the free and open source software movements are worth exploring
in order to comprehend the political dimensions of these debates within
coding communities.
   With roots in the s computer hacker community, the free software
movement was launched by Richard Stallman in with the development
of the GNU (Gnu’s not Unix) operating system (Stallman . Once
development was underway, Stallman founded the non-profit Free Software
Foundation (FSF) in to maintain the GNU General Public License
(GPL), a legally robust way of licensing free software which remains one of
the most popular ways of licensing FOSS projects. The FSF also publishes
the Free Software Definition (FSD), which provides a periodically updated
definition of free software. The FSD outlines four freedoms which are
necessary for software to be free software. The definition of ‘free’ being used
is not gratis (free of financial cost), but free as in freedom, or as the FSD
describes it, ‘“Free software” is a matter of liberty, not price. To understand
the concept, you should think of “free” as in “free speech”, not as in “free
beer”’ (FSF .
   The four freedoms required by the FSD are (1) the freedom to run
the program for any purpose, (2) the freedom to study and modify the
program (with access to the source code being a necessary precondition
for this freedom), (3) the freedom to redistribute copies of the program,
and (4) the freedom to distribute modified copies of the software. These
freedoms do not prohibit corporate usage of free software or preclude
parties from charging money for free software; in fact, both practices are
explicitly encouraged as valid uses of free software. The freedom of any
user to redistribute copies of the software does, however, place practical
limitations on the potential for charging significant fees for free software,
impeding the type of artificial scarcity engineered by monopolistic control
over the distribution of products whose actual economic cost to copy and
share (among those with the requisite hardware) is minimal.
   The free software movement describes itself as adopting a moralistic
approach to software that is designed to address the ‘social problem’
(Stallman .of proprietary software. The term ‘moralistic’ is applied to
denote a stable series of cultural judgements about what is right or wrong:
the freedoms designated by the FSD are understood as positive values,
whereas the encroachment upon these freedoms found within proprietary
software is prescribed as problematic behaviour.
   The open source movement emerged from the free software movement,
with a group departing from the FSF and forming the Open Source Initiative
(OSI) in . Eric Raymond, who along with Bruce Perens founded
the OSI, argued that moving from free to open source software was less
attributable to divergent conceptual thinking than to terminological and
public-relations-related problems pertaining to fears and suspicions that the
12


term ‘free software’ engendered among corporate users (Raymond .
Raymond contends that these concerns partially arose through ambiguities
pertaining to different usages of the term ‘free’ and cites the fact that
much of the FSF’s literature begins by emphasizing their utilization of the
term to demarcate freedom rather than gratis as evidence to support this.
Consequently, Raymond employed the term ‘open source’ as an (allegedly)
ideologically neutral alternative to free software. The OSI maintain the
Open Source Definition (OSD)2, which contains ten clauses, which extend
beyond providing access to source code. Like the FSD, for a licence to
comply with the OSD it must allow free redistribution of the software and
its code, and permit modifications to the original program. Other clauses
include statements that the licence must not discriminate against persons,
groups or fields of endeavour (explicitly denoting that the licence cannot
prohibit the use of open source software in businesses), that the licence must
be technology neutral and must not restrict other software.
    Comparing the details of the respective definitions of open source and
free software, what becomes striking are the similarities between them; their
differences lie not in what the terms refer to but in how they approach the
same production process. According to Stallman :

      Nearly all open source software is free software. The two terms describe
      almost the same category of software, but they stand for views based
      on fundamentally different values. Open source is a development
      methodology; free software is a social movement. For the free software
      movement, free software is an ethical imperative, because only free
      software respects the users’ freedom. By contrast, the philosophy of open
      source considers issues in terms of how to make software ‘better’ – in a
      practical sense only. … For the free software movement, however, nonfree
      software is a social problem, and the solution is to stop using it and move
      to free software.

The philosophical schism between the approaches to free and open source
software – with one side approaching the field from a moralistic position
pertaining to freedom and the other pursuing the practical benefits of a
commons-based production model – is somewhat reflected in wider
discourses pertaining to commons-based peer-to-peer production.
   The terms ‘commons’ and ‘commonwealth’ refer to assets which are
communally managed and which create value which nourishes the community
at large, rather than private interests. Unlike privately owned commodities,
or centrally administered public services, commons are typically managed
by the community that uses and produces them. Commons-based peer


2
    Available online at http://opensource.org/docs/osd
  production refers to the creation of commonwealth by a distributed network
of peers, voluntarily self-aggregating participants who are accorded equal
status. That these peers are able to be geographically distributed is often
highlighted as a key departure from earlier forms of commons that were
place based.
   The open source approach to commons is exemplified by authors
such as Yochai Benkler  ; .and Lawrence Lessig , who
both self-identify as liberal capitalists, but present strong cases regarding
the advantageous nature of commons-based peer production, both in the
specific case of software and within other particular areas of contemporary
technoculture. Benkler  75/   contends that certain affordances
specific to twenty-first-century digital culture makes peer production a
successful strategy:

   Peer production is emerging as an important mode of information
   production because of four attributes of the pervasively networked
   information economy. First, the object of production – information – is
   quirky as an object of economic analysis, in that it is purely nonrival
   and its primary nonhuman input is the same public good as its output –
   information. Second, the physical capital costs of information production
   have declined dramatically with the introduction of cheap processor-
   based computer networks. Third, the primary human input – creative
   talent – is highly variable, more so than traditional labour, and certainly
   more so than many material resources usually central to production.
   Moreover, the individuals who are the ‘input’ possess better information
   than anyone else about the variability and suitability of their talents and
   level of motivation and focus at a given moment to given production
   tasks. Fourth and finally, communication and information exchange
   across space and time are much cheaper and more efficient than ever
   before, which permits the coordination of widely distributed potential
   sources of creative effort and the aggregation of actual distributed effort
   into usable end products.

There exists a range of scholarship that explores the use of commons-based
economic systems in tribal societies (Strathern  Seeger .and
premodern Europe (Boyle  Runge and Defresco . highlighting
pre-capitalist commonwealth-based alternatives to market production and
delineating that commons-based production is not a historically novel
method of production. Equally, the work of feminist geographers J. K.
Gibson-Graham  productively illustrates the manner through which
commons-based economic activities are dispersed throughout advanced
capitalist economies, with examples ranging from domestic work and gift-
giving to gleaning, poaching and theft. Benkler’s thesis, however, is that several
specific affordances of contemporary technoculture creates opportunities
13


for commons-based production to occur through geographically dispersed,
distributed networks of self-aggregating peers, which, he argues, in certain
conditions, provides a more economically efficient model than market-based
production, and that this is particularly true in cases involving information.
    This approach is based not on an overarching critique of capitalist social
or ecological relations but on a utilitarian position surrounding economic
efficiency, although we should note that Benkler does note several positive
cultural externalities associated with commons-based peer production. As
with the open source development model, Benkler’s position encourages
corporate capitalism to adopt commons-based peer production within
certain realms, under the rationale of competitive cost advantage, a position
which has become increasingly popular within the techno-capitalist business
literature (e.g. Davenport and Beck  Tapscott and Williams 
Leadbeater . This position is not post- or anti-capitalist; primarily
limiting itself to economic efficiency, it does not engage with contemporary
crises of capitalism; instead, it looks to reform the neoliberal fetishization
of competition and free markets by including commons under certain
conditions where it proves economically competitive.
    This reformist liberal–capitalist position can be juxtaposed with the more
revolutionary claims associated with common-based production advanced
by theorists such as Bernard Stiegler and Michel Bauwens. Stiegler contends
that by the twenty-first century, capitalism has become intrinsically and
systemically focused upon short-term financial flows, and consequently
is thoroughly incapable of presenting useful solutions to problems such
as climate change or creating economies free of dangerous economic
speculation. Addressing this toxic short-termism, Stiegler contends that
digital technoculture forms a new pharmacological context, in which
there exists tendencies towards both an ecologically catastrophic short-
termism and an alternative possible mobilization towards an economy of
contribution, which Stiegler posits as a non-market mode of organization
predicated on a model analogous to that of commons-based peer-to-peer
production.
    Free software is the exemplar that Stiegler provides as presenting an
immanent alternative to consumer capitalism and Anthropocenic ecocide:

  The software industry and its digital networks will eventually cause
  associated techno-geographical milieus of a new kind to appear, enabling
  human geography to interface with the technical system, to make it
  function and, especially, make it evolve, thanks to this interfacing:
  collaborative technologies and free license software rest precisely on the
  valorization of such associated human milieus, which also constitute
  techno-geographical spaces for the formation of positive externalities.
  (Stiegler a:  /9)
  Commons-based peer production is celebrated not only for its economic
efficiency here but for creating technocultural structures which embrace
cooperation. Stiegler terms these ‘associated milieus’, which he contrasts
with ‘dissociated milieus’, technological systems within which producers and
consumers are treated as separate entities. Stiegler argues that dissociated
milieus lead to a process of proletarianization, whereby consumers lose
the knowledge of how to produce culture, which becomes grammatized or
embodied within technical apparatuses. Consequently, Stiegler contends that
commons-based peer production creates positive externalities: beneficial
extra-economic values which derive from the process of communal care.
    In place of the perspective which contends that growth is understood
solely along quantifiable monetary terms, Stiegler  a:  proposes
a broader understanding whereby ‘a pathway to genuine growth must be
refound, a growth running counter to the misgrowth that consumerism has
become, and a growth which would consist in a renaissance of desire. Such
a rebirth would be achieved by implementing an economy of contribution,
an economy for which “to economize” means “to take care.”’ This
position aligns with ecophilosophy and ecological economics, insofar as it
emphasizes that socioeconomic systems of valuation require a reorientation
away from monetary value and towards qualities such as environmental
sustainability and personal well-being. What is pertinent to this chapter is
that, in Stiegler’s analysis, it is the affordances of sociotechnical assemblages
to produce distributed commons-based peer production which enables this
type of economy of contribution. While this mode of production has been
primarily visible in areas such as free software, Stiegler asserts that it can
become the basis for a post-capitalist socioeconomic model.
    The potential benefits of the widespread adoption of commons-based
peer production are central to the work of Michel Bauwens, the founder
of the P2P Foundation. Bauwens examines distributed peer-to-peer (P2P)
networks as an alternative mode of production to both the market and the
state, predicated upon the voluntary self-aggregation of individuals who
control their own means of production, as well as having a commons-
orientated output (Bauwens . FOSS is cited within Bauwens’s writings
as a prime example of P2P production, with FOSS projects such as Apache,
Linux and Mozilla Firefox presenting prominent examples of successful P2P
systems.
    Bauwens explores the material and cultural prerequisites for P2P systems
to flourish, drawing similar conclusions to Benkler and Lessig about the
centrality of digital-networked computational systems to the viability
of P2P, while also exploring ways that a hybrid economy exists whereby
P2P networks are partially dependent on capitalist markets – through P2P
workers’ current needs for a wage supported outside of their P2P activities –
and capitalist markets are increasingly dependent on P2P networks as an
13


external source of value which can be appropriated into monetary value.
However, Bauwens departs from Benkler and Lessig’s positions by positing
P2P as an alternative socioeconomic model to that of the market:

  This still nascent P2P movement … is fast becoming the equivalent of
  the socialist movement in the industrial age. It stands as a permanent
  alternative to the status quo, and the expression of the growth of a new
  social force: the knowledge workers. In fact, the aim of peer-to-peer
  theory is to give a theoretical underpinning to the transformative practices
  of these movements. It is an attempt to create a radical understanding
  that a new kind of society, based on the centrality of the Commons, and
  within a reformed market and state, is in the realm of human possibility.
  (Bauwens )

Considering alternative systems which could support an economy heavily
dependent on a P2P mode of production, Bauwens  suggests that a
universal basic income (UBI) could provide an economic base which then
empowers individuals to contribute value via P2P networks. This position
has in recent years gained traction across a range of left-wing thinkers
and organizations including Stiegler  a); Maurizio Lazzarato ;
Michael Hardt and Antonio Negri  , .and Nick Srnicek and Alex
Williams .
   Proponents of UBI contend that enabling universal access to a basic
wage, alongside the provision of health care and education, would allow
innovation and creativity to flourish by removing the compulsion to work
to make ends meet, while reducing the current threat of precarity which is
likely to be reinforced by increasing levels of automation. Furthermore, a
UBI would require a reappraisal of renumeration for labour. Today, jobs
that are deeply unpleasant, dirty, dangerous or demeaning – such as cleaning
toilets – are currently often poorly paid due to the compulsion for people
to work to make ends meet. If those basic needs were met by a UBI, finding
people willing to perform those tasks would require significant financial
incentivization. Alternatively, that financial incentive could spur the
development of technologies to automate those unpleasant tasks, something
that is unlikely to occur in the current socioeconomic environment, precisely
because they are so poorly paid.
   It must be noted, however, that such claims go beyond those made by the
free software movement, which campaigns around and develops software,
rather than the broader socioeconomic issues advocated by Bauwens
and Stiegler. Stallman’s own political position can best be described as
libertarian, and the FSF has always stated that free software has commercial
potential. However, the dissociation of the open source movement from the
free software movement can be understood as partially resulting from free
software’s connections with radical social movements and the desire on
  the part of liberal–capitalist advocates of open source software to distance
their work from anti-capitalist connotations in order to provide more
corporate-friendly platforms. Indeed, among anti-capitalist tech-activist
communities, projects are typically framed by the rhetoric of free software,
while corporate projects tend to ensconce themselves in the language of
open source.
   The differences surrounding the open source and free software movements
demonstrate the diverse range of views which are broadly supportive of
commons-based peer production. These range from avowed capitalists
and corporations innovating around affordances particular to the network
society, to anti-capitalist activists contending that these formations provide a
glimpse of a hyper-productive economic mode with intrinsic drives towards
an alternative model of organization based upon cooperation rather than
competition. In relation to these debates, the media ecological approach I
wish to develop is situated towards the anti-capitalist end of the spectrum,
contending that sociotechnical assemblages must be re-orientated away from
GDP as the sole source of value, towards promoting broader conceptions of
growth and wealth as the enhancement of ecological systems.
   Hyper-productive digital networks do not, however, inherently address
ethical or ecological problems. As we saw in the previous chapter with regard
to viral marketing, astroturfing and blogging, often those best situated to
benefit from technological innovations are existing elites, so pre-existing
inequalities can be widened rather than narrowed, and this is certainly the
case when analysing how corporations such as Google, Microsoft and IBM
have leveraged open source software to enhance profitability. As we shall
see, community-led development can mean that corporations effectively
benefit from the unpaid labour of volunteers. Despite this, if we are to take
seriously the claims advanced by Stiegler and Bauwens that commons-based
peer production forms a nascent alternative mode of social organization and
economic production, providing potential tools for social and ecological
liberation within control societies, a detailed exploration of various practices,
actions and forms pertaining to FOSS, as the paradigmatic example of this
activity is important in grasping salient features, affordances and affects
related to this mode of production.
   Whereas debates surrounding the discourses and practices of FOSS and
software studies present one scale for examining the ethics and agencies of
software, I now turn to numerous examples which explore a range of scales
within the software ecosystem. These cases treat software as being operative
across numerous registers, ranging from the microscopic materialities
of firmware or specific device drivers, through to the development and
distribution of networking protocols, social media algorithms and search
engines. They will develop an analysis of variable deployments of openness
across an ecology of software, alongside examining the agencies and
technocultural politics associated with various software.
13


      Jailbreaking iPhones and Magic Lanterns
The term ‘firmware’ describes the lowest level of software, usually
consisting of relatively small programs or data structures which enable
device functionality, without which the hardware device in question would
be completely non-operational. Firmware is present throughout the range
of microelectronics present in contemporary technoculture. While the term
has become somewhat fluid as the technologies involved with firmware have
evolved, the loose demarcation between software and firmware references
both the level at which the code operates and the fact that originally firmware
was encoded in read only memory (ROM) on discrete hardware modules.
Updating firmware therefore initially required users to physically alter
the machine, plugging and unplugging the respective firmware-containing
modules into the device.
    Semiconductor-based programmable read only memory (PROM) chips
were eventually superseded by erasable programmable ROM (EPROM)
chips in which memory could be erased through sustained exposure to
strong ultraviolet light, which in turn were replaced by electrically erasable
ROM (EEPROM). Most modern devices contain flash memory, a modern
form of EEPROM first introduced by Intel in (Tal .which permits
up to 1, ,  erase/rewrite cycles. This entails that modern firmware
can be regularly updated by manufacturers in order to add functionality,
remove bugs or improve performance. Firmware may initially appear to be
a utilitarian set of instructions designed to enable the underlying capabilities
of hardware devices; however, firmware has significant impacts on the
affordances of these devices, revealing struggles pertaining to power, control
and agency surrounding firmware access and modification.
    Firmware’s agential affordances are illustrated by the open source Magic
Lantern firmware that was originally designed for the Canon 5D Mark II
(5DMII) and subsequently deployed on other Canon DSLRs capable of
shooting video. Upon release on November , the 5DMII generated
significant interest from videographers due to the inclusion of a high-
definition video mode. Although the camera was primarily designed and
marketed by Canon as a photographic camera, the 5DMII had several
unique features for a video camera priced around £ , largely deriving
from the camera’s full-frame 35mm sensor, which dwarfed the sensors in
video cameras which existed in at a similar or higher price point.
Commonly used professional video cameras in , such as the Sony Z1
and Panasonic HVX , used 1/3-inch sensors, the higher-end Sony EX3
used a 2/3 inch sensor and even the Red One – a professional digital cinema
camera used for feature films such as Peter Jackson’s King Kong and costing
over ten times the price of a 5DMII – used a Super 35mm sensor (whose size
is similar to an APS-C sensor; see     
FIGURE 4.1 Video camera sensor size comparison.


   Cameras containing larger sensors feature enhanced low-light
performance, as the greater surface area allows more light to fall upon
each individual pixel. Another capacity of larger sensor sizes, which is seen
as highly desirable by cinematographers, is that they provide a shallower
depth of field (DOF) at a given aperture. Consequently, cameras with large
sensors afford film-makers a greater degree of control over DOF, allowing
the creation of aesthetics traditionally associated with high-budget 35mm
film productions, whereas deep DOFs are historically aligned with low-
budget or amateur video or 8mm productions. Additionally, the 5DMII
had an interchangeable lens mount, allowing a huge range of optics to be
mounted to the camera, whereas in most camcorder featured fixed
zoom lenses. These jack-of-all-trades optics are designed for versatility, but
interchangeable lens systems allow the use of specialized lenses such as tilt-
shift, macro and wide-aperture fixed focal length lenses.
   Despite these advantages, there were also significant drawbacks when
comparing the 5DMII to professional video cameras. Upon release, the
camera did not allow alterations of the lens aperture while in video mode;
adjustments had to be performed by exiting video mode, resetting the
aperture and then re-entering video mode.3 The camera would only record
video at frames per second, meaning that it could not be used for broadcast
material in Europe, where the PAL standard requires frames per second,


3
    This was changed by a firmware update from May (Canon .
13


and it could not shoot frames per second – the speed of film cameras.4
Furthermore the 5DMII lacked a number of features ubiquitously found
on professional video cameras, such as manual audio gain control, audio
levels and zebras.5 Although the 5DMII’s hardware was able to implement
these features, the Canon firmware which shipped with the camera did not
enable them.
   Following the 5DMII’s release, a group of open source programmers
decided to investigate the potential of modifying the firmware to add
functionality to the cameras. The result was the open source Magic Lantern
software whose first public release was in June . The initial release
included features such as manual aperture control, zebras, manual gain
control and audio levels, and over time Magic Lantern has expanded to
include features such as focus peaking, variable bit rate recording, full HD
output via the cameras HDMI output (the Canon firmware only supports
  × , assignable white balance values in kelvin and an intervalometer.
The addition of this functionality greatly improves the versatility and utility
of the 5DMII as a video camera.
   Magic Lantern is installed through a straightforward process detailed on
the Magic Lantern website and subsequently illustrated by online videos.
Users begin by downloading the software and a firmware update from the
Magic Lantern website, and then load the firmware onto a memory card
which is placed into the camera. The user then navigates through camera
menus, selecting an option to update their firmware – a process congruent
to installing official firmware updates. Once the modified firmware has
been installed, the user must then make the selected memory card bootable,
which can be achieved using a software tool with a GUI linked from the
website. Once this has been achieved, the Magic Lantern software can be
loaded onto the memory card, which when placed into the camera with the
modified firmware will boot the software. This particular practice means
that if the memory card in the camera does not contain the software, or is
not bootable, then the camera functions as though it was unmodified.
   The Magic Lantern group have successfully reverse engineered versions of
the firmware to add similar functionality as to that achieved for the 5DMII
on subsequently released Canon DSLRs such as the 5D Mark III, 6D, 7D,
 D and 70D. Additionally, as of , Magic Lantern added capabilities
for RAW video recording on numerous Canon DSLRs. Whereas the default
video capture from Canon DSLRs records compressed 8-bit video using
the h  codec (either with IPB or with intraframe compression), Magic



4
  Both and frames per second recording capabilities were added by Canon in a firmware
update in March .
5
  Zebras provide a cinematographer with visual feedback indicating the overexposed areas of
an image.
  Lantern adds an option to capture a stream of 14-bit RAW images, which
retain the full level of data recorded by the sensor. The file size for RAW
video capture is thus far larger than for h  and the extra detail and
sharpness in these images significantly increase the quality of the recorded
images. Consequently, Magic Lantern RAW video has been enthusiastically
adopted by independent film-makers.
    The example of Magic Lantern highlights that firmware is not a neutral,
utilitarian set of instructions which simply allows hardware to function but
represents a method for controlling particular kinds of functionality. Access
to firmware becomes crucial to grasping the affective potentials of devices,
what they are capable of doing and which actors are afforded the capacity
to explore and modify these boundaries. Interestingly, when a photography
blogger contacted Canon to ask whether installing Magic Lantern would
void his camera warranty (Tirosh . the response he received advised
that while any damage caused through the alteration of the firmware would
not be covered by the warranty, damage from other sources (such as buttons
failing) on cameras running modified firmware would be replaced under the
Canon warranty.
    This position, where Canon are happy for customers to customize their
devices but will not be held responsible for damage which occurs through
the process of modification contrasts sharply with the behaviour of certain
other companies faced with open source developers attempting to enable
features on their devices. A prime example of this is Apple’s reactions to iOS
jailbreaking, the process of gaining root access to a device running Apple’s iOS
operating system which is found on iPhones and iPads. Attaining root access
enables users to remove numerous limitations and restrictions built into the
software, including allowing the installation of software from sources other
than the official Apple App Store, customizing the user interface, setting
different default applications for specific tasks and various other ways of
customizing functionality which is prevented by iOS. Prior to iOS version
4.0, the Apple software also prevented devices from multitasking, another
feature which was enabled by jailbreaking iOS devices.
    The ability to download applications from outside of the official
Apple App Store allows users to decide which applications they wish to
run, rather than relying on the applications deemed appropriate by the
hardware manufacturer, whose policies over application censorship have
been criticized in numerous cases. In December , Apple rejected an app
created by Pulitzer Prize-winning satirist Mark Fiore on the grounds that
it contains satirical content which violates ‘Section 3.3.14 from the iPhone
Developer Program License Agreement which states: Applications may be
rejected if they contain content or materials of any kind (text, graphics,
images, photographs, sounds, etc.) that in Apple’s reasonable judgement
may be found objectionable, for example, materials that may be considered
obscene, pornographic, or defamatory’ (Singel . The same section of
13


Apple’s guidelines was cited in the removal of an application from the German
tabloid newspaper Bild, deciding that it contained overly sexual material for
the Apple platform, leading to claims of censorship from Bild’s publisher,
Springer (Gebauer and Patalong . Apple has also been criticized over
decisions to ban an application from WikiLeaks (Helft .and to prevent
charity and non-profit organizations from providing applications which
allow users to donate to their organizations (Strom .
    To circumnavigate restrictions put in place by Apple, users can choose
to jailbreak their device. While accurate numbers of jailbroken devices are
hard to source, in over seven million devices installed the Evasi0n
jailbreaking tool within its first week of public release (Greenberg .
denoting that significant numbers of iPhone users override the restrictions
put in place by Apple. Apple’s response to jailbreaking has been attempts
to prevent it through technical and legal means. iOS jailbreaking relies on
the user utilizing one of several known firmware exploits which allows the
user to attain root access to the device. An exploit can be broadly defined
as taking advantage of vulnerabilities in the device’s firmware, affording the
user the elevated privileges associated with a super-user or root account.
The term ‘root’ references the file system on Unix-style operating systems,
whereby the root directory is the top-level system directory, so root access
refers to a user having privileges to access all areas of the system and to set
access permissions for other users.
    Jailbreaking usually requires users to enter the device firmware update
mode on their device, which is achieved by turning the device off, connecting
it via USB to a computer with iTunes running and then holding down the
home and power buttons for ten seconds, before releasing the power button
while continuing to hold down the home button. If this procedure is correctly
applied, the user receives a message that iTunes has detected a phone in
recovery mode. Device firmware update mode allows jailbreaking software
such as RedSn0w to install firmware onto the iOS device, as the firmware
and OS are not loaded by devices booted into this mode, which is intended
to recover devices when the firmware or OS has been corrupted and requires
reinstalling. Once connected to the jailbreak software, the device has the
modified firmware applied, allowing the user to install a modified OS.
    The process of jailbreaking devices is enabled by the existence of
online how-to guides and instructional videos, which provide step-by-step
instructions for jailbreaking. As the act potentially results in permanently
rendering the device inoperable if incorrectly applied, provision and
straightforward access to clear instruction is crucial to jailbreaking. Dozens
of YouTube tutorials delineate how to jailbreak various versions of iOS,
with the most popular tutorials having over a million views each. Not only
is the networked infrastructure of the web the means for disseminating
jailbreaking software, but broadband internet capable of streaming video
becomes crucial for its adoption. Here we see a way in which entanglement
  between scales of content, software and hardware is made visible: digital
content is used to instruct users how to jailbreak hardware by means of
installing customized software. Without the web, users would lack both
access to the jailbreaking software and the knowledge of how to execute the
jailbreaking process.
    To combat these exploits, Apple frequently releases firmware updates
closing vulnerabilities used by the jailbreaking community. After each
exploit is closed, the jailbreaking community finds new vulnerabilities
to continue jailbreaking devices. Apple also stops signing old firmware
versions in order to prevent jailbreakers downgrading their firmware to
earlier versions which had been successfully jailbroken and have released
iOS firmware updates such as version iOS 4.02 for the iPhone and 3.2.2 for
the iPad which add no new functionality (Patel . but do close known
jailbreaking exploits, demonstrating Apple’s commitment to preventing
jailbreaking. Consequently, jailbreaking iOS devices is a cyclical process
whereby products are released, exploits are found, implemented, published
and subsequently patched by new firmware releases, which in turn leads to
the search for novel firmware exploits.
    In addition to utilizing technical means attempting to prevent iOS
jailbreaking, Apple have sought to criminalize the act itself. In , Apple
submitted materials to the US Library of Congress Copyright Office,
requesting that the circumnavigation of restrictions built into mobile
telephony devices be reviewed under the Digital Millennium Copyright Act
(DMCA). Apple argued that explicitly exempting jailbreaking from DMCA,
which was originally designed to prevent circumvention of copyright controls
commonly known as digital rights management (DRM) would ‘destroy the
technological protection of Apple’s key copyrighted computer programs in
the iPhone device itself and of copyrighted content owned by Apple that
plays on the iPhone, resulting in copyright infringement, potential damage
to the device and other potential harmful physical effects, adverse effects on
the functioning of the device, and breach of contract’ (Apple . In ,
the US Library of Congress Copyright Office dismissed Apple’s arguments,
ruling that jailbreaking was a private decision for individuals, who were
free to decide which applications they wished to run on hardware they had
purchased and that this was covered under existing legislation around the
fair use of products (Albanesius . Although Apple were ultimately
unsuccessful in their efforts to render jailbreaking illegal, they continue to
deploy technical measures attempting to prevent jailbreaking and maintain
that jailbreaking voids any product warranty (Satriano and Shields .
    This case presents an example of a multinational corporation having a
conflict of interest with a significant minority of its customers: while Apple
prevent users from utilizing features their devices are capable of – notably
downloading applications from locations other than the official Apple App
Store, from which Apple take per cent of any monetary transactions – a
14


fraction of Apple’s customers seek the freedom to use the applications they
wish and to download them from their location of choice. This conflict of
interest has seen the creation of tools which provide users with root access
to their devices, and Apple’s response has been attempts to criminalize their
customers and to prevent jailbreaking through technical means.
   This conflict of interest highlights what is at stake with regard to
controlling access to firmware. Firmware enables or denies access to various
elements of device functionality, delimiting particular possibilities of the
device/user assemblage. Firmware, then, becomes a terrain of conflict for
competing groups who lay claim to ownership of devices and desire the
agency associated with ownership to designate how devices can be utilized.
Consequently, we can understand that firmware relates to both power
relations and agencies within media ecologies: how parties can or cannot
connect to, alter and modify firmware affects agential capacities regarding
ways that user/device assemblages function, and conflicts over controlling
access to these connections illustrate differential power relations between
actors. Furthermore, the persistence of jailbreaking over a protracted
temporal duration denotes the inability of Apple to entirely control their own
code, evidencing one way that it possesses its own agencies and affordances.
   Responses to firmware/product hacking have differed widely, with the
two examples outlined here demonstrating this via the contrast between
Canon’s statement that they will repair modified devices under warranty (so
long as firmware modification has not caused the fault) and Apple’s attempts
to criminalize their customers. This divergence can largely be explained by
the economic aspects of each case. Magic Lantern adds value to Canon’s
customers, providing them with enhanced functionality, but Canon itself
stands to lose nothing through users modifying hardware they have already
purchased. Magic Lantern effectively enhances the value of Canon’s products,
while Canon themselves invest no time, energy or capital in this process,
and are under no obligation to support users whose hardware encounters
problems from these modifications. By contrast, while iOS functionality is
also enhanced through jailbreaking, the ability for users to escape the walled
garden of Apple’s App Store and instead use alternatives such as Cydia
to download applications entails that Apple loses revenue. The negative
economic connotations for the manufacturer presented by jailbreaking can
thus be understood as a primary motivation for Apple’s stance.
   This account of firmware and product hacking produces productive
homologies with accounts of computer game modding practices, which
have long been seen as key indicators of paradigmatic changes to media
culture in the early twenty-first century:

  Game modifications do suggest that the era of media as software will
  produce new legal relationships between consumers and producers.
  However, these relationships are not the precondition for a utopian
    democratization of creativity – they still exist within the prevailing
  economic nexus. Game modders provide the industry with free research
  and development of new ideas and sometimes whole new titles. … The
  mod community now provides a reliable source of labour for the industry,
  with very low or no training investment. (Dovey and Kennedy  34)

While game publishers have gone to great lengths to attract modding
communities, modding practices are viewed as highly attractive so long
as value is added to the existing commercial framework laid out by the
software developers: ‘The point at which modding becomes competition
rather than brand development and viral marketing is very carefully policed’
(Dovey and Kennedy  34). This closely corresponds to Apple’s
behaviour surrounding iOS jailbreaking, as the ability of jailbroken devices
to download apps outside of the official App Store represents a challenge to
the monopoly system Apple otherwise operates.
   Consequently, we can suggest that open source communities bear some
semblance to game modding communities, both of which see ‘consumers’
taking increasingly active roles in shaping the development of media
platforms. While there are divergent reactions in specific cases, these
predominantly arise from potential threats or boosts to profitability. Notably,
these heterogeneous responses demonstrate that a one-size-fits-all approach
to understanding firmware/product hacking would prove overly reductive
and that according to the economic specificities of the situation, corporate
actors may embrace or vehemently reject open source modifications to
devices.


              Drivers, antifeatures and DRM
An operating system (OS) is the software that controls numerous crucial
functions of a computational device: resource (hardware) management,
interfacing the core system with connected hardware via device drivers, disk
access, file systems, user interface (commonly a graphical user interface)
and networking protocols, while also acting as an intermediary between
higher-level software applications and the system’s hardware. The section
of the OS which deals with resource management is the kernel. The kernel
manages hardware resources such as the CPU, GPU, RAM and any input/
output (I/O) devices, such as disk drives, displays, printers and input devices,
and allows other software applications to run by allocating system resources
among them. The kernel communicates with hardware via a combination
of firmware and device drivers: software whose function is communicating
between the hardware and OS, allowing the OS to manage the device and
allocate its functionality to higher-level applications. While inter-device
communication and allocating resources may sound like a utilitarian task,
14


there are instances where the types of agency inherent to these functionally
crucial but often unseen forms of software make themselves visible.
   This is exemplified by implementations of proprietary binary drivers for
graphics cards under the free/open source GNU/Linux OS.6 Device drivers
are OS specific, and different OSs utilize divergent models for independent
hardware producers to create device drivers. The Microsoft Windows model
sees Microsoft publish a stable set of Application Binary Interface (ABI)
calls which provide hardware vendors with a selection of services that are
made available to device drivers by the OS. The ABI effectively decouples
OS and driver development, as Microsoft maintains the closed-source OS
and the published ABI, while the hardware producer maintains the closed-
source device driver, with the ABI presenting the stable interface between
repositories of proprietary software. One negative aspect of this model is
that Microsoft has at times altered the ABI between successive Windows
releases, entailing that hardware producers must rewrite their drivers to
be compatible with the new ABI. This led to widespread problems with
Windows Vista as many vendors had not rewritten their drivers when the
OS was released, leading to users being unable to access peripheral devices
(Montalbano .7
   Within Linux, the preferred approach is for hardware producers to
provide open source drivers which are subject to a public peer-review
process, before being accepted into the mainline kernel for the OS. Once
drivers are accepted, they are maintained by developers working on the
OS, so drivers continue to work with future releases: ‘The key strength
of this approach from the user’s viewpoint is that, in happy contrast with
proprietary operating systems like Windows Vista, once a device is working
on a given version of Linux support continues through all future versions. In
Linux, hardware support only gets better; it never gets worse’ (Kohn .
From the hardware producer’s perspective, this model is beneficial, as once
a driver is accepted into the mainline kernel, they do not update or rewrite
the driver.
   The difficulty with the Linux model arises when vendors provide closed-
source drivers (as they would do with Windows or OSX). As the community
does not maintain the driver, it must be updated by the hardware vendor
alongside each kernel update, leading to compatibility issues. Within the
sphere of graphics card drivers, this has been the strategy pursued by


6
  The OS derives from Stallman’s initial call for the GNU operating system; however, this project
was completed with the Linux kernel developed by Linus Torvalds. Subsequently, the OS has
commonly become known as Linux, although FSF materials always refer to it as GNU/Linux.
From here on, I will use the term Linux, as this will be the name recognized by most readers.
7
  Subsequent to the Windows XP/Vista fiasco, Microsoft has implemented a backwards
compatibility mode for device drives to ensure that hardware devices do not fail to function
when upgrading OS.
  Nvidia, whereas Intel and ATI (the other two major players in the graphics
card industry) have both released open source drivers. Linux users with
Nvidia graphics cards have experienced significant difficulties, with the
driver regularly being recorded in the top oopses (Bottomley .–
deviations from the correct kernel behaviour which produce an error log.
While many experienced Linux users consequently choose to purchase Intel
or ATI graphics cards, Linux Foundation technical advisory board chair
James Bottomley  contends,

    Most of the reported oopses are coming from less experienced or even
    novice Linux users. The problem here is that these people quickly get
    frustrated with the problems which they will ascribe to Linux in general,
    not the problem binary driver in particular. Even worse, they may report
    the problem to a Linux forum only to be told that it’s a binary driver
    issue and can’t be fixed, thus leaving the reporter with few options to
    try a working Linux system beyond an expensive graphics hardware
    replacement. These users aren’t likely to continue their experiment
    with Linux; nor will they recommend it to their friends. In fact, they’re
    probably turned off Linux for a considerable period (if not for life). This
    last is an illustration of the active harm binary modules do to the Linux
    ecosystem: Linux gets classified as unusable because of a problem in a
    binary module which no open source developer can fix.

This highlights the impact which device drivers can have on user experience,
while also demonstrating some of the unplanned affordances of software:
Nvidia have not conspired to create oopses for customers opting to use
Linux. While the relatively small market share of personal computers
running Linux8 may entail that the company expends less resources on driver
implementation than on its Windows counterpart, Nvidia still attempt to
create a stable device driver. However, issues that arise surrounding system
stability have widespread ramifications for users, in some cases rendering
hardware unusable with a particular OS. This not only foregrounds the
different affordances of open and closed models of software creation but
also highlights one way that unintended impacts arise from software usage,
denoting the presence of a form of nonhuman agency.
   The solution Bottomley and Kohn provide – hardware producers writing
open source drivers which can be integrated into the mainline Linux kernel –
does not itself eradicate bugs in drivers or incompatibilities introduced
by kernel updates. By opening problems up to the Linux development


8
 Among servers, or other computational devices such as networked attached storage and smart
TVs where Linux market share is significantly higher, dedicated graphics cards tend not to be
used.
14


community, however, they allow a large networked community to investigate
and correct problems. Opening access to the driver’s source code entails that
user and development communities are no longer beholden to an external
entity to resolve issues but are empowered to directly intervene and address
issues. This approach utilizes the increase in connectivity afforded by the
contemporary networked media ecosystem to crowdsource solutions to
compatibility issues, demonstrating a practical instantiation of the open
source mantra known as Linus’s Law: ‘Given a large enough beta-tester and
co-developer base, almost every problem will be characterized quickly and
the fix obvious to someone’. Or, less formally, “Given enough eyeballs, all
bugs are shallow”’ (Raymond  0).
   It must, however, be emphasized that this strategy is not guaranteed to
reveal errors or exploitable vulnerabilities in code, particularly when the
codebase grows over a prolonged temporal duration. This is well illustrated
by the Heartbleed security issue found in the OpenSSL cryptographic
library, a widely used open source implementation of the transport layer
security and secure sockets layer protocols that are designed to provide
secure connections across computational networks. Heartbleed has
been described as ‘a serious vulnerability. Some might argue that it is the
worst vulnerability found (at least in terms of its potential impact) since
commercial traffic began to flow on the internet’ (Steinberg . with
over half a million servers affected, including high-profile companies such
as Pinterest, Amazon Web Services, SoundCloud and Wikipedia. Although
the exploit was patched as soon as it was discovered, prior to public
disclosure the vulnerability could have been used for up to two years to
obtain passwords and other sensitive data, such as the 4.5 million patients’
health records which were obtained from Community Health Services, the
second largest for-profit hospital chain in the United States (Frizell .
That said, the episode around Heartbleed, which revealed the underfunded
and understaffed process around OpenSSL development has subsequently
led to significant improvements in its maintenance, precisely because of its
interrogatability as an open source project.
   Although questions of compatibility surrounding how differing devices
connect and communicate with one another are common to a vast array of
technological systems, answers have tended to be framed within the context
of political economy, and specifically by approaches to competition and
regulation undertaken by corporate and governmental actors. What the
Linux community offers is an alternative model predicated upon utilizing
distributed networks of self-aggregating peers to examine openly published
standards. The result is that the open source community provides a workable
alternative to developmental methodologies based upon intellectual
property, enabling co-creatively designed systems to provide long-term
driver support. This open source methodology presents an example of how
peer-to-peer systems enabled by the material infrastructure of the network
  society can intervene into processes which have erstwhile been dominated
by multinational corporations.
   Allowing individuals with technical competencies and access to
networked digital computers to participate in the development of tools and
technologies crucial to the functioning of a technocultural milieu in this way
exemplifies how the collaborative practices of peer-to-peer systems challenge
previous models based upon the separation of producers and consumers.
Stiegler foregrounds this as a critical distinction between the contemporary
technological ensemble and those of industrial society: ‘The internet age is
an age of hypomnesis constituting itself as an associated technical milieu.
It marks the end of the era of dissociated milieus – the escape from milieus
that separate the functions of producers and consumers, deprive both
of their knowledge, and consequently strip their capacity to participate
in the socialization of the world through its transformation’ (Stiegler
 b: 83). Stiegler argues that associated milieus built upon networked
digital technologies afford the construction of an economy predicated on
contribution to the commons, rather than the communal loss of knowledge
associated with a producer/consumer dichotomy. Whereas for Stiegler the
dissociated milieu of industrial culture is characterized by knowledge being
displaced into technology or elites, the associated milieu of contemporary
media ecologies allows an alternative system based upon cooperation and
contribution to supplant the proletarianization of neoliberalism.
   This example differs from firmware/product hacking, as it involves the
creation of systems which function as competition to existing proprietary
OSs, which arise from a peer-to-peer developmental methodology. As we
have seen, advocates such as Stiegler and Bauwens argue that this mode of
production points beyond the hypercompetitive market-dominated model
of neoliberalism, towards a post-capitalist alternative which provides social
benefits through positive externalities based upon the formation of associated
milieus. We should, however, remind ourselves that FOSS is also proposed
as a hyper-efficient developmental methodology which forms ‘mutually
reinforcing relationships with market-based organizations’ (Benkler :
 ), rather than an alternative socioeconomic system. Indeed, the degree
to which multibillion-dollar corporate entities such as IBM, Google and
Amazon leverage Linux-based systems and contribute to Linux development –
allowing them to co-design the open source systems they benefit from –
should give pause for thought when considering claims that FOSS gestures
towards a departure from contemporary modes of capitalism.
   Another set of differences between FOSS and proprietary software
which manifest at the level of the OS are antifeatures: ‘functionality that a
technology developer will charge users to not include’ (Mako-Hill .
This is distinct from the traditional process of charging for features which
enhance functionality or user experience, which logically derives from
a labour theory of value, whereby as coding features takes time, labour
14


and resources on the part of the programming team, users effectively
pay for the labour time of the coders. Antifeatures invert this logic, with
customers paying programmers not to include features designed to inhibit
functionality; in these cases, the customer pays more money for less labour
as the features in question are restrictive or malicious. Antifeatures are
frequently employed within digital rights management (DRM) systems,
software which prevents certain usages of digital content. DRM has proven
a contentious topic, with proponents claiming that DRM provides essential
security for copyright holders while opponents contend that DRM systems
go beyond the scope of copyright laws in preventing users from accessing
content they have purchased, inhibiting fair uses of products.9
   Windows Vista provides an example of an OS whose DRM
implementation received heavy criticism. The Protected Video Path DRM
which debuted in Vista and has been included within more recent versions
of Windows constantly monitors user activity in order to police the outputs
of certain types of digital media designated as ‘premium content’, primarily
high-definition video, whose digital transmission is encrypted by Windows
in an attempt to prevent the content from being copied and shared. In
order to achieve this, the OS has to expend resources both monitoring the
content and providing the encryption, and this was one of the reasons why
Vista’s minimum recommended systems specification of a 1GHz processor
and a gigabyte of RAM far exceeded its predecessor, Windows XP, which
recommends aMHz processor andmegabytes of RAM. DRM
within Vista disables hardware devices that do not include approved
digital content protection facilities. For example, the Sony/Phillips Digital
Interface Format (S/PDIF) high quality audio output contains no digital
content protection and is disabled by Vista when the OS detects that a
system is playing premium content (Gutmann . DRM technologies
built into Windows Vista not only impede the performance of hardware
but additionally inhibit user freedom with regard to digital media content;
making copies for personal use is considered fair use by copyright law but
disallowed by Vista’s draconian DRM.
   Similar complaints regarding DRM have been levelled at devices such as
the Apple iPad and Nintendo 3DS. Like other iOS devices, the iPad uses DRM
to prevent users downloading applications which have not been ratified by
Apple and has consequently been criticized for being the most locked-down
general-purpose computing device available (Anderson , Defective by
Design . The Nintendo 3DS is a mobile gaming device that wirelessly
transmits information about the user’s activity to the manufacturer, gives
the manufacturer a royalty-free licence to use any content created by the


9
 My usage of the term ‘fair use’ here technically refers to US legislation, but is intended to
include related legal constructs such as UK laws pertaining to ‘fair dealing’.
  user (the device includes a camera, so Nintendo claim the right to use any
pictures you take with your device) and scans the system for any software
modification, which if detected can lead to the manufacturer remotely
rendering the system permanently unusable (Defective by Design .
    Antifeatures are a phenomenon exclusively restricted to proprietary
software. The act of adding malicious code to FOSS programs in order to
limit functionality would simply lead to members of the FOSS community
forking the project and creating a branch with the antifeatures removed
from the code base. This highlights one of the key differences between the
philosophies of open and proprietary software: whereas proprietary software
is created to generate profits, which can include deliberately sabotaging the
functionality of some versions of the software with antifeatures, FOSS is
based around communal innovation attempting to create the best possible
software solution. While FOSS may include numerous different versions of
a particular form of software – such as the many different implementations
of Linux – each version is designed to afford users different useful features,
not to include malicious code designed to hamper user experience. There is
a crucial distinction, then, in the value systems evident within the respective
developmental modes: proprietary software reflects an ethic whereby
economic profits are the primary imperative, echoing the neoliberal position
whereby GDP is the sole determinant of wealth, whereas within FOSS
economic benefits are seen as supplementary to the primary goal of creating
platforms which enable various forms of value to flourish, arguably positing
an alternative value system which resonates with the ecological ethic outlined
in Chapter whereby creating connections and allowing systems to grow in
various directions and dimensions supplants economic determinism.


                        Formats and protocols
The file formats, codecs and protocols10 which comprise the outputs and
standards used by applications and networks can again be either open or
closed. While there is no prescriptive definition of an open format analogous
to the FSD, open file formats are generally understood as publicly published
specifications for storing data that are usually maintained by a standards
organization. Open formats are thus available for implementation by
anyone using either proprietary or free/open software, whereas closed
formats involve methodologies for data storage which are the intellectual
property of the entity who maintains legal ownership (via copyright or
patent) of the file format. This gives the owner exclusive control over the


10
  For a comprehensive examination of the role protocols play within networked technoculture,
see Galloway .
14


format, entailing that other software cannot be compatible unless it has
been licensed or successfully reverse engineered. Proprietary software
can produce formats which are either open or closed, FOSS exclusively
produces open formats.
   Notable examples of open file formats include the Joint Photographic
Experts Group’s (JPEG) lossy compression format for digital photography
and images, which is maintained by the International Standards Organization
(ISO) and the International Electrotechnical Commission (IEC); the WebM
audio/video format for use with HTML5, which is developed by Google, the
Portable Document Format (PDF), which was initially designed as a closed
format by Adobe but was subsequently published as an open standard
in and is now maintained by the ISO, and the Hypertext Markup
Language (HTML) and Cascading Style Sheets (CSS) used to author and
style pages on the World Wide Web, which are maintained by the World
Wide Web Consortium (W3C) and the ISO.
   HTML and CSS present particularly pertinent examples when discussing
the merits of open formats, as the design and construction of the web
around exclusively open formats, standards and protocols was a key choice
in shaping its character as an informational network. Discussing computer
networks which existed before the web, Tim Berners-Lee, the British
engineer and computer scientist generally credited with inventing the web,
argues,

  In , the world still suffered from incompatible networks, incompatible
  disk formats, incompatible data formats, and incompatible character-
  encoding schemes. This made any attempt to transfer information
  between systems daunting and impractical. This was frustrating because
  people were increasingly using computers to handle information, a large
  amount of important information was already stored in computers, and
  many of the computers were networked. However, these systems in use,
  including those that were proprietary and those that physicists wrote for
  their own use, were incompatible. (Berners-Lee )

One of the primary ways that Berners-Lee sought to address these
incompatibilities was through employing open formats and protocols for the
web. Utilizing standards which did not rely on proprietary methodologies
enabled users on any computational platform to engage with the web.
Indeed, this openness, which Berners-Lee describes as a primary design
principle of universality, is ascribed as being of central importance to the
web’s utility and popularity.
   This reveals the importance of the structural design of platforms and
the way in which openness can be pivotal to the success of a platform:
‘People seem to think the Web is some sort of piece of nature, and if it starts
  to wither, well, that’s just one of those unfortunate things we can’t help.
Not so. We create the Web, by designing computer protocols and software.
… We choose what properties we want it to have and not have’ (Berners-
Lee . It is important to restate that contrary to Berners-Lee’s usage,
here, within an ecological framework, ‘we’ refers not only to the humans
involved in design but to a sociotechnical assemblage incorporating multiple
nonhuman actors. People may aspire for the web to have terabit/second
connections; however, if the physical carrying capacity of the global networks
of fibre-optic cables or the end-user networking devices and protocols do not
support these desires, then the actual system presents a negotiation between
agencies, desires and capabilities within the system as a whole. The open
protocols of the web additionally provide a useful reminder that technology
does not evolve according to a teleological pathway; the web arose from
particular decisions, constraints and actions; it was not and is not the only
possible outcome.
   Bearing in mind the centrality of open protocols to the successes of the
web, we should note that the development of the latest set of standards for
the web, HTML5, has been marked by an increasing volume of corporate
involvement in the drafting of the standard (Daubs and Manzarolle .
with Apple and Google contributing thirty-seven representatives to the
HTML working group between them. The consequences of this can be seen
in the incorporation of DRM protocols into the HTML5 standard via the
Encrypted Media Extension (EME) specification, a move which has sparked
significant controversy. Proponents of EME claim that HTML5 requires
DRM to maintain the open web as a viable alternative to both the walled
gardens of iOS and Android apps, and the closed-proprietary web-based
usage of systems such as Adobe Flash (Meyer . Opponents of the
incorporation of DRM into the HTML5 standard, such as the EFF, who
in filed a formal objection to the proposal and in resigned from
the body who maintain web standards – the W3C – over EME’s adoption,
contend that it ‘will shut out open source developers and competition,
throw away interoperability, and lock in legacy business models. This is the
opposite of the fair use model that gave birth to the Web’ (EFF . Cory
Doctrow  has highlighted that EME violates the W3C’s own policies,
which requires that standards are not burdened by patents and that the
inclusion of DRM is thus fundamentally incompatible with the principles
of openness and inclusion which underpinned the formation of the web
and W3C. Consequently, Daubs and Manzarolle  surmise that while
HTML5 has been heralded in some areas of the technology press as an
antidote to the proprietary duopoly of Apple and Google, ‘the proposed
inclusion of DRM protocols suggest HTML5 apps will be just as limiting
and closed as their native counterparts on iOS and Android devices. HTML5
may provide new technical capabilities, but only insofar as these capabilities
15


link back to forms of accumulation and commoditization of creative labour
characteristic of app-centric media.’
   The case of HTML5 and EME demonstrates that the protocols and
standards which underpin the web are still developing and that present
moves around their commercialization depart from the openness upon
which the web was originally built. Far from being utilitarian and neutral
tools, or a fundamental part of the nature of networked telecommunications,
these standards are an ongoing site of struggle in which activists from
organizations such as the EFF and FSF contest the increasing corporatization
and enclosure of digital networks. Within this space, the ideology of
openness, which was a guiding principle for the initial development of the
web, has been eroded over time through the incorporation of corporate and
commercial interests, with debates over DRM and HTML5 elucidating one
area where open protocols are being enclosed at the behest of multibillion-
dollar technology corporations.


                        Crawling the web
One of the defining aspects of contemporary computational technologies
is their ability to communicate with other computers over networks. While
this is partially predicated on a global assemblage of hardware, software
also plays a critical role, with shared protocols enabling networking and
various forms of software utilizing networking capabilities. Forms of
software pivotal to networking include the device drivers for network cards
and their interactions with the OS, which itself will include networking
protocols such as TCP-IP (transmission control protocol/internet protocol),
FTP (file transfer protocol) and SSH (secure shell). As we have seen, the
openness of these protocols has been crucial to how networked computing
and the web have evolved, allowing devices to connect with one another and
exchange data irrespective of their hardware platform. Today, a multitude
of applications depend upon networking capabilities, and many websites
employ numerous forms of software to accomplish a broad spectrum of
activities pivotal to the functioning of networked media ecologies.
    The algorithms and programmes that underpin search engine software
present a useful example that illustrates some of the agential and political
issues embedded within networking software. Search engines are a key form
of software, as their results direct traffic around the web, determining what
sources we are shown for specific queries, thereby constructing what users
tend to imagine the web to be. A search engine is predicated upon an index
of web pages, an enormous database detailing what pages exist and the
key information they contain, which is obtained by web crawlers, pieces
of automated software which create the index by following hyperlinks and
continuously reporting the results of the crawl back to the database.
     This immediately implicates a degree of machinic agency, as not all web
pages are indexable; it is estimated that between and per cent of pages
are indexed by major search engines (Gull and Signorini . While this
cover virtually all preferentially attached sites that do not require a login,
vast numbers of pages are rendered invisible at any given time by search
engines. Although accurately mapping a dynamic system such as the web
is impossible due to its constant mutation, the ways that crawlers index
the web are not neutral but depend upon how crawlers are programmed.
In addition to material hidden on the darknet behind a dynamic .onion
address (Gehl . search engines do not typically crawl certain types
of content, such as sites built with Adobe Flash or Microsoft Silverlight.
Other issues which can affect the indexing behaviour of crawlers include
web pages using frames, temporary faults with the site’s server during the
index, temporary faults with the crawler during indexing, the page requiring
a login or the page being dynamically generated. In each of these cases, the
alleged objectivity and neutrality of the indexing process is shown to in fact
be partial and contextually dependent on forms of agency pertaining to the
crawler itself, the technologies used to generate the web page and the way
that their respective capacities interact during the indexing process.
   A further issue that contradicts the notion of algorithmic neutrality is
the manner in which search engines produce their results. Google’s search
engine famously utilized the PageRank system, with Lawrence Page and
Sergey Brin describing the algorithm in as ‘a method for rating Web
pages objectively and mechanically, effectively measuring the human interest
and attention devoted to them’ (Page and Brin  . The PageRank
algorithm was thus designed to provide a universal, objective ranking
system; however, due to the unequal diffusion of web connectivity allied
to the attractor of preferential attachment, results frequently presented
pages from the United States as disproportionately relevant to search terms
(Vaughan and Thelwell  Van Couvering  61). Consequently,
Page and Brin’s claims pertaining to PageRank’s objectivity are dubious at
best. That this bias skewed relevance in search terms towards geographical
regions with pre-existing economic and informational privileges evidences
that far from being neutral and objective agents, algorithms possess forms
of agency and that these do not necessarily stem from their design: Google
did not intend to design an algorithm which favoured US-based results,
consequently reinforcing existing hierarchies surrounding material wealth
and access to cutting edge technologies, but this is how PageRank acted.
   Consequently, Google updated the PageRank algorithm to present users
with data that factored local geography into results, so users from other
countries would not receive US-biased search results. In order to achieve this,
the search engine’s software has to log the Internet Protocol (IP) address of
the user, allowing them to be geographically identified. By storing the results
of search terms attributable to unique IP addresses, search engine operators
15


are able to compile complex profiles of users’ online activities, which has
become a valuable commodity, allowing the search engine operators to profile
user types, and then use this data to produce highly targeted, user-specific
advertising. While Google had been implementing forms of personalized
search results for users signed into Google accounts since (Kamvar
 ), in December Google extended their personalized search results
to include users who were not account holders or logged in to services: ‘This
addition enables us to customize search results for you based upondays
of search activity linked to an anonymous cookie in your browser’ (Horling
and Kulick .
    Current search results from Google are far from the objective and
universal reproduction of knowledge which Page and Brin originally sought
to produce. They involve the implementation of complex personalization
algorithms which modulate search results dependent on the user’s previous
behaviour. The practice of personal algorithmic filtering is not exclusive
to Google’s search engine; other websites such as Facebook, YouTube and
Amazon use similar forms of algorithmic filtering to present personalized
material. Eli Pariser  contends that these practices create filter bubbles,
in which the web becomes an echo chamber, whereby users are provided
information which acts to support and reinforce confirmation biases, while
failing to challenge their preconceptions. danah boyd  describes this
as the ‘psychological equivalent of obesity’, whereby companies which have
a financial interest in capturing attention achieve this through presenting the
informational equivalent of fast food: celebrity gossip, sports and sexualized
imagery. The popularity of this digital junk food with certain user groups
leads personalization algorithms to subsequently filter out alternative search
results which present more serious discursive frameworks.
    When considering the agencies of software, what is notable here is that
these functions are performed by algorithms, which essentially become
the new gatekeepers of information. Whereas in the mass media age
professional journalists acted as the filter through which information had to
pass in order to reach the public, this role is now accomplished by software.
‘Search engines construct net reality. They are not just technical tools in the
hand of the user. They have a significant impact on the image users have
of web content and its patterns of relevance’ (Schultz, Held and Laudiene
 . Consequently, an analysis of the emerging online practices of
gatekeeping via algorithmic filtering requires an investigation of ‘the politics
of code’ (Goode  , considering the nonhuman agencies embedded
in every layer of the software ecology, alongside exploring the connections
between human and nonhuman elements of the complex techno-social
assemblages within which contemporary software is situated.
    Paralleling other cases explored within this chapter, the FOSS community
has responded to perceived threats to digital freedoms, this time to online
privacy and anonymity via the array of personal information held by
  Google, creating technical systems enabling the circumvention of Google’s
data mining strategies. Three such systems are Startpage, Duck Duck Go
and Tor. Startpage is a search engine which applies a proxy system to utilize
Google’s search results without the user having to directly engage with
Google, thus allowing anonymized search while using the Google system.
Instead of travelling directly to Google, which could identify the user based
on their IP address and browser cookie, the request is sent via the startpage.
com proxy server, whose identity Google associates with the request. The
proxy then retrieves the search results and forwards them to the user. The
system uses a number of proxies and each successive request from a user is
passed onto different servers, entailing that Google is unable to identify a
single user as the originator of the agglomeration of search terms, so they
are unable to build a dataset of terms associated with a single user, which
is key to their statistical profiling techniques. This method also entails that
Google cannot provide personally filtered search results, as they cannot
access a personal search history with which to enact filters relevant to the
user’s previous interactions with the search engine.
   Whereas Startpage offers an alternative search facility which leverages
Google’s results to provide a privacy-oriented search engine, Duck Duck
Go provides an alternative search engine to Google which has marketed
itself as an option for users who are concerned about privacy. Duck Duck
Go does not track the activity of users and thus is designed to avoid the
issue of creating filter bubbles based upon personalized search results and is
designed to prevent search histories becoming valuable data for marketing
companies. Espousing a cyberlibertarian ethic which resonates with early
discourses of online freedom, Duck Duck Go does not store any information
about users’ IP addresses or user agent (the web browser/OS and other data
about the machine accessing the server) in order to provide an anonymous
mode of search.
   Whereas Startpage and Duck Duck Go both specifically focus upon
search, Tor – an acronym of The Onion Router, a reference to the layered
structure of the vegetable – is a broader privacy-related project which can
be deployed to similar effect with regard to anonymous web searches. The
system behind Tor was originally developed by the US Naval Research
Laboratory for protecting governmental communications (Tor a)
but has been developed into widely used FOSS tools for anonymous web
browsing. Tor functions by connecting users to a distributed peer-to-peer
network, and all internet traffic is routed via this network. The pathway
data is channelled through regularly changes, entailing that requests and
traffic are not traceable to the originator, so they remain anonymous and
therefore impervious to personalized filtering algorithms. Although the final
node in the network that the request is routed through, which is known as
an exist node, can be readily identified, this does not allow the pathway
back through the Tor network to be traced.
15


   The advantage of using Tor over other anonymizing tools is that the user’s
entire internet traffic is anonymized, not just search queries, meaning that
Tor has far broader utility in online activism. For example, Tor was used
widely during the uprising against the Mubarak regime in Egypt during the
Arab Spring of when the government attempted to cut the protesters’
channels of communication by ordering ISPs to cut off internet access to
Egyptians. Whereas some ISPs revoked their Border Gate Protocol routes,
rendering connections via Tor impossible, other ISPs such as Noor and
Etisalat only implemented IP filtering; software filters enabled to prevent
Egyptians communicating online by redirecting any traffic associated with
an Egyptian IP address. One of the responses by those inside Egypt was
to utilize the Tor network to anonymize their IP addresses and therefore
bypass the filtering restrictions constructed by ISPs (Ioerror . This led
to a huge spike in the traffic within the Tor network, resulting in Twitter
users across the world requesting that politically sympathetic users set up
additional Tor relay nodes to accommodate the extra traffic (Finley .
   Startpage, Duck Duck Go and Tor demonstrate that the FOSS
community have created various tools to contest issues surrounding
privacy and anonymity on the web, and that these solutions are capable
of circumnavigating numerous issues, such as Google’s data retention and
personalized search filters. It must be noted, however, that the usage of
these FOSS hacktivist tools is miniscule in comparison to Google’s overall
traffic; as of , Startpage has around 5.5 million searches per day and
Duck Duck Go receives over million daily queries, while Tor has over 2
million daily users. While these numbers sound impressive in isolation, they
are dwarfed by the 3.5 billion search queries which Google receives daily.
There becomes a significant risk when foregrounding such activist/privacy-
led alternatives, that we lose sight of the overwhelming scale of corporate
dominance.
   Services such as Duck Duck Go, Tor and Startpage can easily become
ways for a technologically literate, ethically concerned elite to escape the
issues of Google’s corporate dataveillance and filter bubbles while building
alternative bubbles of their own in which their techno-social networks
employ an array of privacy enhancing tools but do little to effect broader
social change. That is not to say these tools cannot be useful for activists
who wish to maintain private and secure communications in their ongoing
attempts to organize a range of activities but that the key is trying to mobilize
wider networks, rather than simply bolstering one’s own online privacy. Put
another way, media ecology advocates using these technologies to promote
the formation of communities whose actions are geared towards solidarity
and sustainability, not just individual freedoms.
   While these examples only scratch the surface of debates surrounding the
ethics and politics of internet privacy, anonymity, data mining and filtering,
they do elucidate key features of these debates, notably the demarcation
  between commercial entities whose economic viability is predicated upon
selling targeted advertising based on the possession of a vast quantities of
data and FOSS/hacktivist communities who are concerned with protecting
privacy and anonymity. In light of the Snowden revelations, we should
note that many of the long-recommended security practices advocated by
hacktivists, such as using Tor and PGP,11 have largely been validated as ways
of maintaining anonymity or encrypting data (Taffel . Understanding
debates over user privacy – what information is being harvested by whom,
for what purpose and with what degree of transparency – becomes crucial
when exploring the ethics of network driven software. In this sense, finding
ways of providing users with feedback about the digital footprints they
leave is an important task (Pasquale . The current situation sees the
majority of internet users unaware of the information contained about their
IP address and user agent allied with the ways that this data is used by
predicative systems of corporate and state dataveillance, so finding ways
of making these processes visible may be one avenue by which activists can
productively engage with issues surrounding data security and privacy.
   The European Union’s General Data Protection Regulation (GDPR),
which came into effect in May , is one prominent way that these issues
have been approached through multinational regulation. The wide-ranging
legislation is designed around the concept of data protection by design and
default, whereby businesses that handle personal data must build systems
that are designed to provide safeguards to protect data, such as using the
highest-possible privacy settings by default. GDPR includes clauses that
include the ability to challenge automated decision-making systems, a right
to erasure of personal data, a right to access of copies of personal data
that is held and a requirement for informed consent to be provided before
businesses can collect and process data. GDPR also has significant penalties
for non-compliance; parties who violate GDPR can be fined up to €20
million or per cent of annual worldwide turnover, whichever is greater.
In the case of major digital platforms such as Google and Facebook, this
amounts to several billion euros per year.
   On the surface, this would appear to be a significant milestone in regional
regulatory action that contests poorly understood issues surrounding data
harvesting and extraction. There have, however, been significant concerns
raised about where GDPR places the burden of responsibility in many of
these instances. Trawling through lists of data firms and cookies, obtaining
rationales for specific automated decisions or copies of data to inspect,
all falls squarely upon the shoulders of individual data subjects. As Adam
Greenfield  51) notes, the underlying logic is ‘thoroughly consonant


11
  PGP (pretty good privacy) is a public key cryptography standard which is used to send
encrypted emails within a web of trust.
15


with the neoliberal practice of governmentality which tends to individualize
hazards and recasts them as issues of personal responsibility or moral failure
rather than structural and systemic issues’. Resonating with the previous
chapter’s focus on the poverty of time within a commodified economy
of attention, it is then unsurprising that for many people, the reaction to
GDPR’s rollout has been dominated by anxiety. Every website and digital
platform simultaneously required users to read through lengthy end-user
licence agreements and check or uncheck boxes for each of the third parties
that data was previously shared with. This process was made particularly
time-consuming and tedious by many sites breaching the regulations by not
defaulting to the most stringent privacy settings and refusing to include an
untick all option, so users had to manually click on a seemingly endless list
of boxes to opt out of sharing data with third parties.
    Consequently, Max Schrems, an Austrian data activist has filed
complaints about how Facebook, WhatsApp, Instagram and Google have
implemented GDPR compliance, arguing that these companies are in clear
breach of the spirit and letter of the regulation; their new terms of service
are a form of ‘forced consent’, where users either accept them wholesale or
stop using the service. It will be interesting to see how this develops, as the
filing can potentially lead to multibillion-euro fines for each entity. While
GDPR’s neoliberal mode of governmentality should be foregrounded as an
issue that limits its efficacy, especially for time-poor subjects who are often
those most vulnerable, it does still have the potential to mitigate some of
the worst elements of data and algorithmic opacity, especially if authorities
decide to impose heavy financial penalties upon corporations who breach
its guidelines.


      Conflicts and agency in code and design
This chapter has examined entanglements within ecologies of software,
exploring how disputes and struggles over ethics, agency and power occur
within these technocultural assemblages. Approaching software through an
ecological framework entails that ‘no piece of software is a singular entity’
(Yuill  7), software is always dependent upon a multiplicity of other
forms of software, protocols, hardware and social structures; it functions as
part of dynamic technocultural assemblages that collectively evolve through
intra-actions, rather than as isolated individual commodities. Within these
assemblages, network effects see particular forms crystallize into platforms,
well-established large entities which exert selection pressures that draw
new actors into their basin of attraction. As we saw with the example of
HTML5 and EME, the ongoing corporatization and enclosure of digital
spaces within what has been described as ‘platform capitalism’ should be a
serious concern for proponents of social and environmental justice.
      In this chapter, there have been two tensions which have dominated
proceedings. The first of these is between open and closed systems of
software, whereby there are ongoing struggles over agential, political and
ethical dimensions of software development and maintenance. It would be
wrong, however, to present this as a binary opposition, where openness is
understood to effectively mean good and necessarily point towards a post-
capitalist future. For example, product hacking presents instances where
companies such as Canon have allowed user communities to customize and
enhance products, as this behaviour effectively increases product value with
no necessary capital outlay from the manufacturer. In cases such as iOS
jailbreaking though, where economic models are challenged by alternatives
generated by distributed peer-to-peer networks, we see the deployment of
various legal and technical measures attempting to preserve profitability.
Outside of a company’s key economic interests, however, they may choose
to deploy FOSS to a significant degree, such as Google’s embrace of FOSS
outside of its search engine, in software including the Android OS and
Chrome web browser.
    In some ways, this denotes the variable affordances of FOSS,
highlighting the differences between anti-/post-capitalist and free software
advocates on the one hand and proponents of efficiency and open source
on the other. Numerous cases explored within this chapter demonstrate
that while open approaches may provide beneficial outcomes, FOSS can
simply present a hyper-efficient model of production for corporations, for
whom increased efficiency denotes increased profitability. This competitive
advantage has nothing to do with the potential ecological benefits of
commons-based peer production, instead demonstrating how powerful
economic actors can exploit the free labour of precarious workers.
Arguments surrounding economic efficiency are far removed from claims
that FOSS are emblematic of an associated milieu and the economy of
contribution, whereby contributing to culture is linked to taking care of
the community and moving beyond a politics of competition, austerity and
precarity.
    The pharmacological context of technics must again be emphasized. To
foreground this, I have attempted to outline how open systems are leveraged
in different ways within technocultural assemblages with divergent political
and social outcomes. The spaces and practices which are deterritorialized
by the affordances of a new technocultural milieu are subsequently
reterritorialized and colonized by powerful actors seeking to exploit these
developments to enhance their own positions. Just as in the previous chapter
we saw that networked digital ecologies afforded the formation of new
modes of communication and participation alongside the creation of novel
modes of hierarchy, we can understand that the affordances of open source
have had a similar effect, both challenging previous economic dogmas
regarding the efficiencies of markets and positing commons-led alternatives,
15


while simultaneously allowing powerful economic actors to leverage
crowdsourced, precarious labour to enhance their economic standing and
inflict further defeats on organized labour movements.
   We have also seen conflicts between corporate actors and heterogeneous
FOSS communities relating to issues surrounding intellectual property and
privacy. These issues raise questions surrounding which actors have agency
to modify, alter, enhance and hack devices. In cases such as the creation
software-enforced monopolies such as the Apple App Store, or the insertion
of DRM into web standards such as HTML5, we see corporations attempting
to dictate the range of acceptable usages of digital assemblages largely based
on economic outcomes. By contrast, FOSS and hacker groups contend
that having purchased hardware or software, users are within their rights
to utilize these materials as they see fit, not as the manufacture instructs.
Examining network-based software such as search engines, browsers and
social networking software additionally reveals tensions pertaining to user
privacy, whereby FOSS groups question current practices surrounding data
mining, profiling users and leveraging user’s personal data for profit and/
or corporate/governmental surveillance. However, as cases such as Magic
Lantern and Android development denote, when companies see functionality
added to their devices at no economic cost to themselves, they are likely to
embrace open source communities.
   Openness alone is not sufficient to realize an ecologically inflected post-
capitalist economy, as is evidenced by the various enthusiastic corporate
adoptions of open source. Here my thinking resembles some of the recent
discussions which have been conducted through the P2P foundation, which
outline sustainability, openness and solidarity as the three fundamental
building blocks of a commons-based alternative economy. Transparency,
openness and collaboration are thus understood as being beneficial when
their aims are aligned with social solidarity and ecological resilience,
rather than being a positive thing in and of themselves. The key here is the
relational context of openness, so that it becomes more than just a way of
magnifying existing inequalities, enabling corporate reterritorialization and
enclosure or assisting individuals in enhancing their own situation while
leaving broader power structures unaffected.
   The second major tension which runs throughout this chapter relates
to distributions of agency throughout the techno-social assemblages of the
network society. In numerous places, the agencies of software have been
foregrounded, examining ways that software develops along trajectories
unforeseen by those responsible for creating it. Examples of this nonhuman
agency explored in this chapter include the presence of glitches, bugs and
exploitable loopholes in code, the material consequences of algorithmic
informational gatekeepers, and the inability of companies to control their
own code. Consequently, we see that software possesses forms of agency,
rendering an ecological account of software distinct from humanist accounts
  of technology, whereby technologies become neutral, agency-less tools in the
hands of human subjects. Forms of nonhuman agency differ from those
associated with human actors, but nonetheless they clearly set limits and
define spaces of potentiality. In particular, the ability of software to automate
tasks, such as crawling the web or filtering particular types of search results
or social media streams, and to take decisions based upon machine learning
from past behaviours demonstrates the agential mode which Mackenzie
 .describes as ‘secondary agency’, one which is differentiated from
that of both humans and other nonliving entities.
   Nonhuman agencies contrast with certain ways that agency is discussed
with regard to design at various points throughout this chapter; designers
are portrayed as being able to significantly determine aspects of systemic
development through their choices, particularly with regard to openness
and freedom within computational systems. An ecological conception of
software design requires a reorientation of the designer; whereas social
constructivist accounts would place the human agent – the subject capable
of free will, creativity and innovation – as the designer, an ecological account
emphasizes that software design is never undertaken solely by humans.
Rather than individual designers, contemporary software design requires
assemblages of humans, computers (themselves assemblages of various
components composed of a multitude of elements), networks of fibre optics,
modems, internet exchange points, input devices, scripting languages,
protocols and syntaxes alongside the existing software ecology, all of which
influence and constrain what is and is not possible.
   In other words, it is not the human designer who wields agency in the design
process, but the entire designing assemblage whose agencies are mobilized.
Indeed, the agency of the human designer only exists precisely because of
the complex array of interconnections with the nonhuman nodes of the
assemblage: without the pre-existing work that has gone into collectively
evolving an advanced informational environment the human capacity to
design software would resemble that of our cave-dwelling ancestors. As
such we cannot understand technical evolution without exploring both
the cultural and technical conditions which afford the design of software,
hardware and mediated content. My argument here is that doing so requires
a fundamental rejection of the atomized individualism of neoliberalism,
instead foregrounding the collective entanglement of assemblages. The
following chapter continues this theme, through an exploration of the
political ecology of digital hardware.
                Materiality and digital
                 infrastructures



The language employed to describe digital media includes numerous terms
that conceal the materiality of technology. The term ‘virtual’ gestures
towards the somehow not quite there, an ethereal spectre which grasps at
but never quite reaches reality. While there is a more concrete definition of
the term ‘virtualization’, which specifically refers to the software-generated
simulation of one computational platform by another, the commonplace
notion of digital technology as relating to virtual spaces, virtual communities
and virtual reality is a discursive practice that obfuscates the complex and
often poorly understood materiality of microelectronics. Similarly, the now
unfashionable contrast between cyberspace and actual space effectively
masks the underlying materiality of global networks of computational
technologies, optoelectronics, cellular networks and global positioning
satellites which enables the creation of the novel spatial engagements
associated with a pervasive digital technoculture.
    Immaterial labour is frequently proclaimed to be the paradigmatic mode
of labour associated with digital technologies; we may quite reasonably ask
if it would it be possible to posit a concept further removed from matter or
materiality? Whereas Maurizio Lazzarato’s  original essay bearing
that title features some incisive analysis regarding post-Fordism and the
neoliberal economy, including a delineation of various material practices
and structures that have transformed labour practices, the connotation that
working with digital technologies involves a departure from materiality
is quite understandably how many have interpreted the trope. The recent
designation of vast server farms providing data storage and computational
processing power for ultra-portable thin client devices such as smartphones
and tablets via networks of undersea and underground fibre optics and 4G
cellular networks as ‘cloud computing’ denotes the continuation of this
trend, with the employment of a descriptor which evokes nebulous floating
  vapour, rather than the witches’ brew of toxic chemicals, heavy metals
and plastics found within microelectronics. The notion of the cloud, then,
continues the history of discursively situating digital technologies as some
form of quasi-magical departure from material reality.
    This encourages us to conceive of these technologies as disposable
commodities, ‘smart’ isolated objects that appear as if from nowhere to be
bought and swiftly discarded, rather than encouraging an engagement with
the material flows of metals, minerals, plastics, energy, labour and waste,
and the political economies and ecologies from which these commodities are
produced and through which they circulate. A central contention of media
ecology is that when considering the ethical and political affordances and
impacts of media systems, we must not only address the final communicational
outputs designed to be read by humans – the content of media – but also
additionally explore the architectures of software and hardware which were
necessary to produce and distribute that content. From this perspective,
the material impacts of media infrastructures cannot be separated from
the cultural connotations of content.1 Whereas the previous two chapters
have explored the scales of content and software, this chapter investigates
concerns centring on computational and communicational hardware.
    The flows of matter and energy which transform ores, earths and fossil
fuels into assemblages of digital microelectronics involves a multiplicity of
materials and actants; producing a smartphone requires around seventy
of the eighty-four stable (non-radioactive) elements (Rohig . Ores
mined in one country will often be processed into pure metals and minerals
elsewhere, before being shipped to new locations for processing into alloys,
transformation into components, assembly into devices, and packaging and
branding, before travelling to the country where the device will be sold, used
and discarded. Having been thrown away, electronic waste – or e-waste as
it is commonly known – is commonly sent around the world once again
for disassembly and processing, as specific valuable materials contained
within devices are recovered and sold for reuse. Each stage in the life cycle
of microelectronics is entwined with ethical imperatives regarding social
and environmental justice, and this chapter aims to delineate many of these
concerns. Alongside these issues, the chapter foregrounds interventions into
systems designed to either limit detrimental impacts or promote positive
alternatives to current methods. Analysing these interventions form a


1
 One question raised by discussing the material impacts of hardware is whether the term
‘impact’ is itself too molar, suggesting pre-formed entities which collide with one another, rather
than a process of assembling. My usage of impact here is homologous to thinking about affect,
how particular entities connect with and transform the affective capacities of one another,
with positive impacts being ones which enhance the capacities of assemblages and negative
ones being those which diminish them. Impacts, then, are ways of describing the evolution of
dynamic assemblages, rather than the percussive encounters of stable subject/objects.
16


crucial part of an ecological approach, which, following the schizoanalytic
methodology employed by Deleuze and Guattari, must go beyond the
destructive task of critique which highlights negative impacts associated with
current practices, suggesting lines of flight that posit potential bifurcations
into more equitable and resilient ways of living.
   Structurally, the chapter breaks up the life cycle of electronic goods into
a series of chronological stages: product design, extracting and processing
raw materials, manufacture, usage, and recycling/disposal. Although these
sections could be depicted as a strictly linear succession of events, from
the inception of the product’s design and the unearthing of the necessary
raw materials for its manufacture, through to the disposal or recycling of
the hardware, these stages are in fact heavily dependent upon one another.
Consequently, when seeking to intervene in the systemic processes involved
in the production cycle, there are frequently strategies applied within one
stage which create changes elsewhere in the life cycle, rather than simply
affecting localized changes to a particular stage.
   This is especially true of the design stage. As Jim Puckett  26) states
while examining the effects of dumping e-waste in Asia, ‘True solutions to
our toxics crisis lie not in recycling wastes downstream, rather in eliminating
them through “green design” upstream.’ By designing products which reduce
or eliminate toxic materials from their construction, which are built to last,
modular and designed to be easily and safely recycled, ecologically inflected
design has an important role to play in creating safe, socially beneficial and
ecologically sustainable digital technologies. While we should raise a note
of caution surrounding design-centric ‘solutionism’, which can provide
a depoliticized, technocratic approach to problems which can never be
extracted from power relations, this chapter explores design-based strategies
last, as strategies surrounding design frequently apply to transforming the
entire structure of the life cycle of microelectronics, rather than simply
impacting one constituent stage. Focusing on design, then, allows a second
scale surrounding hardware to be addressed. Whereas the previous examples
deal with particular phases within the life cycle of microelectronics, design
additionally affords an overview of the flows comprising the entire process,
raising a series of additional problems.


                Blood coltan and rare earths
The ‘raw’ materials for microelectronics are the various ores, fossil fuels and
earths that typically are found beneath the surface of the Earth. Consequently,
in order to access these substances, they must be extracted from the planet.
This removal of vast quantities of matter can logically be understood as one
of the first steps in the life cycle of microelectronics. Considering processes
of extraction connects the ‘virtual’ reality of digital networks to not only the
  geology and geography of the earth but also the globalized flows of energy,
human labour and matter, that compose the twenty-first-century mining
and processing industries. The scale of these flows is immense, with large-
scale mining operations moving over  ,  tonnes of rock on a daily
basis. These industries undoubtedly produce beneficial effects, providing
materials crucial to creating and powering contemporary infrastructures,
while conferring employment opportunities and associated material wealth
upon communities. Without the benefits of the extraction industries,
contemporary society would be dramatically altered; however, the processes
of extraction used to obtain materials vary widely, and in many instances,
incur various social and environmental costs.
   For example, open-cast mining which is frequently employed to extract
elements such as copper and zinc involves stripping away the topmost layers
of earth to reveal the ores below – a process which necessarily involves the
destruction of whichever habitat previously occupied the site. Furthermore,
mining operations can produce adverse effects including the contamination
of local groundwater and erosion, such as in Tar Creek, Oklahoma, where
local lead and zinc mines left the area so badly polluted and at risk of
structural subsidence that the Environmental Protection Agency (EPA)
ordered an evacuation, declaring the town uninhabitable and enacting a
buyout of citizens in the area (Oklahoma Department of Environmental
Quality  Roosevelt .
   The diversity of materials required for microelectronics is a consequence
of the specific properties of those materials. For example, lanthanides,
commonly referred to as rare earth elements (REE), create the strongest type
of permanent magnets: ‘Small, lightweight, high-strength REE magnets have
allowed miniaturization of numerous electrical and electronic components
used in appliances, audio and video equipment, computers, automobiles,
communications systems, and military gear. Many recent technological
innovations already taken for granted (for example, miniaturized multi-
gigabyte portable disk drives and DVD drives) would not be possible without
REE magnets’ (US Geological Survey . Despite the connotation of their
name, REE are not uncommon elements; however, they are extremely difficult
to extract and refine as they tend to be found in very low concentrations,
alongside one another (their similar atomic structure makes separation a
difficult task), in sites also containing radioactive elements. Consequently,
the production of each tonne of REE additionally produces a tonne of
radioactive waste water and 10,  to 12,  cubic metres of waste gas
(NASA . The tailings pond at Bayan Obo, China’s largest REE plant,2
holds aroundmillion tonnes of toxic waste from the mine. Since ,


2
 China produces around per cent of these strategically valuable materials, and this has
caused geopolitical concerns over their future availability. For more see Taffel b.
16


the human population that previously dwelt in the area has been relocated
by the government due to high incidences of cancers and growth defects
arising from proximity to the plant (Kaiman .
   From an eco-ethical perspective, these cases prompt us to ask questions
about digital microelectronics as the extraction of materials used to create
digital infrastructures causes serious harms to human and other biotic
communities. To what extent can we justify damages to these ecological
systems based on the socioeconomic benefits that digital culture brings? Can
the economic benefits of mining be seen to compensate for social impacts?
What kind of status do we afford the nonhumans whose ecosystems are
damaged by these activities, and how can we evaluate nonhuman costs in
relation to social benefits?
   A further example that brings the stakes of digital materiality into a
sharper political and ethical focus while also foregrounding the historical
and enduring colonial dynamics of extraction is the procurement of coltan
ore for the extraction of tantalum. Tantalum is primarily used as a powder
inside capacitors (Cunningham . due to its volumetric efficiency – a
crucial quality for technologies such as mobile phones and laptops where
size is a paramount design concern – and durability, as unlike electrolytic
capacitors, tantalum capacitors do not lose capacitance over time. The
usage of tantalum capacitors within mobile phones and associated portable
computing platforms has been important for reducing the size of these
devices in the twenty-first century, which is a significant factor with regard
to their growth in functionality and popularity. While tantalum is mined in
Australia, Brazil and China, controversy arises from the coltan3 industry
in the Democratic Republic of Congo (DRC). Whereas in the DRC
produced just per cent of global tantalum supplies (US Geological Survey
 ), by this had risen to around per cent, entailing the DRC
had become the single largest national source of tantalum (US Geological
Survey .4
   According to the International Rescue Committee , a decade of
international and civil conflict in the DRC resulted in approximately 5.4
million deaths, the majority of cases being indirect deaths resulting from
treatable diseases, which flourished following the collapse of the economy
and healthcare system, with children under the age of five accounting for


3
  Coltan is a contraction of columbium-tantalum, the tantalum bearing ore found in the DRC.
Columbium is an alternative name for the element niobium which has similar physical and
chemical properties to tantalum.
4
  While many online activist campaigns claim that per cent of the world’s tantalum/coltan
is derived from the DRC, and such claims have made their way into the academic literature
(e.g. Meikle  iii), the source of this dubious figure is untraceable, and potentially refers
to the particular tantalum bearing ore mined in central Africa, rather than global tantalum
production.
  nearly half these deaths. While this conflict raged, exports of coltan from the
DRC continued, notably including those from mines controlled by armed
militias whose continued military activity is financed by coltan and other
conflict minerals such as tin and tungsten. Additionally, the neighbouring
states of Rwanda and Uganda, neither of which have significant domestic
tantalum reserves (Anderson . began exporting coltan, having
exploited Congolese reserves while their armed forces controlled parts of
the DRC, with Ugandan coltan exports increasing from 2.5 tonnes before
the war in , to tonnes by (Essick . President Kagame of
Rwanda described the war in the DRC as self-financing (Hara .chiefly
due to mineral exploitation, with the Rwandan army having seized over $62
million worth of coltan from the DRC in alone.
   The war in the DRC was instigated by a multitude of factors including
the legacies of racial segregation and the ensuing preferential treatment
of certain groups by European colonial regimes, the Mobutu dictatorship
which was overthrown in , and an influx of refugees from Rwanda, a
significant minority of which were implicated in the Rwandan genocide of
 . As Michael Nest  6) explains,

  While political and strategic factors were important at the start of the
  conflict, all armed groups turned to revenue raising activities to finance
  their costly military campaigns. As the conflict wore on, economic
  interests became a major reason to continue fighting. … the Congo war
  became a conflict in which economic agendas became just as important
  as other agendas, and at times more important than other interests.

Consequently, conflict minerals such as coltan have been essential to both
the international and enduring civil conflict within the DRC.
   In addition to the human costs of the conflict, there are numerous
environmental issues associated with coltan mining in the DRC. The Okapi
Wildlife Reserve, a world heritage site, suffered incursions from thousands
of artisanal coltan miners, who decimated the animal population after staff
were evacuated (Essick  United Nations Environment Programme
 ). Illegal mining camps in national parks have also led to miners
hunting endangered species such as Grauer’s mountain gorilla (Taylor and
Goldsmith  21) and elephants (UN .as food. Miners hunt these
animals due to a lack of alternative food sources; given the choice between
bushmeat from endangered species and starvation, it is hard to blame
miners for feeding themselves. Starvation here is linked to land use change
surrounding the conversion of agricultural land into artisanal coltan mines:

  In Numbi in Kalehe the massive destruction of former grazing land is
  catastrophic. Soil which has been used for unplanned prospection and
  artisanal coltan mining is no longer usable for agriculture. Entire hills
16


  and valleys have been turned into giant craters, turning the landscape of
  the region into an expanse of naked earth, at the bottom of which flow
  rivers and streams which were diverted for the requirements of coltan
  mining. (Tegera  0)

This exhibits one way that social and environmental ecological impacts form
an entangled meshwork, with social malaise resulting in environmental issues,
which feeds back into further social issues, particularly when ecosystems are
damaged through deforestation and soil erosion, which prevents any future
return to agricultural activities.
   Primarily due to the activities of NGOs such as Raise Hope for Congo,
Global Witness, Amnesty International and Action Mondiale Pour le
Congo (Global Action for the Congo), who used the tantalum-containing
microelectronics devices whose production they sought to alter to create
social media campaigns, online petitions and documentary films, the
ethical issues surrounding the usage of Congolese conflict minerals has
received considerable media attention. Consequently, numerous electronics
companies have sought to publicly distance themselves from tantalum
originating from central African nations; however, the complexity of the
global tantalum trade, whereby coltan mined in the DRC passes through
at least ten intermediaries between extraction and consumption (Essick
 ; Ma . entails minimal transparency and accountability. Each
transaction represents another avenue for plausible deniability on the
part of the new owner, and consequently it can be extremely difficult
for electronics companies to distinguish whether or not they are using
conflict-related tantalum. One strategy advocated by NGOs which would
assist transparency is for electronics corporations to publish their supply
chain; however, corporations have been adamant that such activity would
adversely affect their ability to obtain competitive price advantages,
highlighting a tension between ethically motivated action and the pursuit
of economic profit.
   The mediated attention surrounding mineral extraction from the DRC also
translated into lobbying pressure seeking to enact legislation entailing that
action was legally mandated, rather than merely an option for corporations.
The outcome of this action was section of the Dodd-Frank Wall Street
Reform and Consumer Protection Act (commonly referred to as the Dodd-
Frank Act), a wide-ranging post-recession US law which introduced a series
of modifications to financial regulations in the United States. Section 
specifically dealt with issues surrounding the procurement of Congolese
conflict minerals and explicitly requires American companies to take steps
to determine whether their products contain minerals from the DRC or
neighbouring nations. President Obama signed the law into effect in July
 , and in August the Securities and Exchange Commission (SEC)
published the rules and regulations which companies had to adhere to with
  regard to their efforts to trace minerals. In an attempt to avoid regulation,
the National Association of Manufacturers, Chamber of Commerce of
the United States and Business Roundtable filed a lawsuit against the SEC
contending that section was a transgression of the first amendment
of the US constitution; however, this case was dismissed in July (US
District Court .
    Despite the intention for section to improve the situation in DRC
by reducing the availability of funds to militias, the impacts of the bill have
been heavily criticized. By requiring that Congolese tantalum be certified
as conflict-free, major electronics corporations simply turned away from
purchasing Congolese tantalum, tin and tungsten, with reports that even
before SEC reporting was introduced, legal exports of tantalum from the
DRC fell by over per cent (Drajem, Hamilton and Kavanagh . While
this was accompanied by an increase of per cent in illegally smuggled
tantalum, the net result was that thousands of artisanal miners who had
previously survived on a meagre income from legitimate mining operations
were no longer able to obtain revenue from their labour, entailing that the
effect of this legislation had been to inflict further suffering on Congolese
citizens while boosting the black market trade in smuggling minerals out of
the DRC (Dizole  Aronson . Western lawmakers were accused
of over-simplifying the complex process of mineral procurement and
detrimentally impacting upon the livelihoods of the people they sought to
assist. Because of the globalized production process, it was simply easier for
corporations to purchase materials from other sources, thereby avoiding the
criticisms associated with using conflict minerals and the additional cost of
certification.5
    An alternative approach to mineral procurement has been undertaken by
Fairphone, a company which originated from a Netherlands-based NGO,
the Fairphone Foundation, which was established to research and campaign
around ethical microelectronics. The Fairphone project evolved because
its members felt that making a commercial product which embodied the
values of the foundation expounded the best way to verify the viability of
alternative methods of producing microelectronics. The first Fairphone was
a smartphone featuring a capacitive touchscreen which runs a version of
Android. The company crowdfunded the production costs of the phone,
using an online advertising campaign that informed potential customers
that production would be economically viable once 5,  Fairphone orders
had been placed, illustrating that in the twenty-first century, in order to
produce ethical hardware, an assemblage involving software and content


5
In February , President Trump announced a suspension of Dodd-Frank section .
This was described as the right decision for the wrong reasons (Stoop, Verpoorten and van der
Windt .
16


are essential components. In an initial production run of 25, 
Fairphones was ordered and the first phones were delivered to customers.
   Fairphone sources materials from partner institutions based in the DRC,
such as tantalum from the Katanga-based Solutions for Hope initiative
(Ballester .and tin from the Kivu-based conflict-free tin initiative (van
Abel . These local initiatives certify particular mines as being conflict-
free, and tag all minerals leaving these mines, meaning that they remain
traceable and thus can bring economic benefits to Congolese miners and
associated workers without funding warlords and militias. Consequently,
Fairphone can be understood as a venture which leveraged ecologies of
content and code to impact upon the production of microelectronics in ways
which sought to produce outcomes that demonstrably improved the lives of
those involved in producing the materials necessary to construct the devices.
   While the project highlights that a more ethical microelectronics
production model is possible, 25,  Fairphones contrast with a broader
industry which saw 1.4 billion smartphones sold in (Gartner .
Fairphone therefore involves a tiny fraction of devices being produced
in a more ethically responsible fashion. Recalling the critique of ethical
consumerism that was advanced in Chapter 2, ethical consumerism
alone cannot provide answers to issues around the ecological impacts
of microelectronics manufacture; it merely represents the abdication of
regulatory governmental practices by devolving ethical responsibility onto
the individual, and Fairphone could be seen as indicative of this. Ethical
microelectronics could simply become a lifestyle choice practised by a
tiny minority of consumers, creating minimal disturbance to mainstream
corporate practices while exemplifying the central mantra of neoliberal
consumer culture, the right of the atomized individual to choose.
   Furthermore, a combination of economies of scale and the speed
of upgrade culture within smartphone production created issues with
Fairphone’s attempts to produce a device that is repairable and has long-term
support. In July , just three and a half years after the original Fairphone
launched, the company discontinued support. The annual upgrade cycle of
smartphone components meant that by numerous suppliers no longer
produced the parts used by Fairphone, and due to the small scale of the
company it was not economically feasible to manufacture new batches of
components. Despite the best intentions of the company, the speeds and
structures of the broader industrial microelectronics ecology prohibited the
device being supported for as long as less-ethically designed smartphones
such as the iPhone or Samsung Galaxy devices. The Fairphone, then, is no
panacea for issues surrounding microelectronics. However, by highlighting
the potential for product design and mineral procurement to productively
engage with artisanal mining communities, it delineates a less reductive and
damaging way forward than the Dodd-Frank section .
     Examining the relationships between digital culture and the extraction
industries illustrates that microelectronics have severe consequences for
people and other living systems and that digital infrastructures are entangled
with the flows of materials, energy and finance that comprise contemporary
globalized capitalism. While digital culture has often been presented as a
dematerialized postmodern public sphere or as an associated milieu allowing
commons-based peer-to-peer alternatives to emerge, the technologies
that currently support the network society are heavily implicated in the
exploitative, inequitable and unsustainable dynamics of global supply
chains. Within these networks, however, not all flows are equal, especially
when considering their social and environmental implications. Digital
technoculture is part of the exploitative history of colonialist and capitalist
social relations. In the case of coltan, we see how the legacy of colonialism
influences current practices and how colonial practices of exploitation are
echoed in the digital age.
   While issues surrounding the procurement of the materials necessary
to construct the contemporary digital media ecology are reported within
NGO and activist circles, supranational institutions such as the United
Nations, and industrial and political discourses, they have largely been
absent within discussions of media and/or mediation that concentrate on
discourse as language and meanings that are tied to content. My argument
here follows recent developments in critical infrastructure studies in
suggesting that when approaching digital media as material systems,
it is crucial to include analyses of the flows of matter and energy, the
assemblages of minerals, metals, human labour and toxic by-products,
that are necessary for the production of digitally mediated content. While
this suggests a decentring of the social constructivist focus upon content
and information, it by no means requires replacing this with a formalism
which relegates the importance of content. Indeed, as we see when
approaching assemblages such as the Fairphone, content, software and
hardware are not discrete domains which can be understood in isolation
but form entangled ecologies which coexist and co-evolve as dynamic
meshworks.


                       Virtual? sweatshops
Once the raw materials for the production of ICT hardware have been
extracted from the Earth and refined into elements and alloys, the next step
in their reconfiguration into iPhones and ultrabooks is the manufacturing
process. By the early twenty-first century, microelectronics production had
shifted away from being primarily conducted by corporations who brand
and sell products, towards a globalized model where ‘manufacturing is no
17


longer considered a core competency for market control’ (Luthje :
22). Brand-name electronics companies are now largely fabrication-
free entities which outsource manufacturing to vast complexes in low-
cost, newly industrializing countries in Asia, Latin America and Eastern
Europe.6 Although this shift in industrial microelectronics production can
be understood as part of the ongoing processes of globalization, situating
manufacturing in this way provides a useful counterpoint to claims that
information technologies are post-industrial or dematerialized.
   The transition to a globalized industry based on subcontracting has been
accompanied by marked decreases in wages and safety conditions, alongside
increased environmental damage caused by companies externalizing
costs onto local ecosystems (Price . Indeed, the lack of adequate
environmental or labour-related regulations are reasons why regions
provide attractive locations, as the financial cost of adhering to stricter
environmental and labour laws reduces profitability. Perhaps unsurprisingly,
the industry has aggressively sought to resist regulation from governments
and unionization or similar forms of collectivized worker representation.
The levels of environmental damage caused by ICT manufacture can be
grasped by considering the twenty-nine sites around Silicon Valley designated
as Superfund priority locations by the US Environmental Protection Agency,
the highest concentration of seriously contaminated areas within the
United States (Gabrys  . These sites arose due to chemicals used in
microelectronics manufacturing seeping out from underground tanks into
local ecosystems, poisoning soils and water supplies and creating a toxic
legacy which will remain hazardous to biotic systems for decades.
   The most prominent examples of labour rights abuses in ICT
manufacturing pertain to Foxconn, the world’s largest manufacturer of
electronics components, and the Longhua Science and Technology Park
situated in Shenzhen, China, which employs between  ,  and  , 
workers in conditions that have seen them labelled as iSlaves (Qiu .
The scandal began with an exposé in the British Mail on Sunday
newspaper (Joseph .which revealed that workers manufacturing iPods
laboured for twelve to fifteen hours a day, earning £54 per month, with
half that figure deducted by the company for food and accommodation.
Inside the on-site accommodation, one-hundred workers occupied each
dormitory and visitors were forbidden. These conditions, and the labour
cost of manufacturing an iPod nano − £4.20 – were contrasted with the
retail price in the United Kingdom of up to £ . The controversy provided
widespread media coverage (Frost and Burnett .which concerned


6
 However, there are exceptions to this, such as Sony and Samsung, both of whom not only
manufacture their own devices but produce components for many competitors, with Samsung
producing screens for Apple, and Sony manufacturing most cameras found in smartphones.
  Apple, prompting the company to launch an audit of labour conditions at
the plant. This found violations of local labour laws or Apple’s corporate
code of conduct pertaining to the number of hours undertaken by workers,
as the limit of sixty hours of work per week ‘was exceeded thirty-five
percent of the time and employees worked more than six consecutive days
twenty-five percent of the time’ (Apple . the amount of living space
inside dormitories; methods of reporting overtime; dispute processes for
workers whose overtime had been underpaid; and ways that management
disciplined workers. Apple subsequently pledged to remedy these infractions
via a press release; however, as the scandal denotes, many of the
violations of local labour laws and Apple’s corporate code of conduct were
not rectified.
   The subsequent scandal surrounding Foxconn in was prompted by
the actions of fourteen employees who committed suicide between January
and May . These acts were committed by young migrant workers, who
like the majority of microelectronics labourers in China had left agricultural
home towns for manufacturing centres such as Shenzhen to find better-
paid work.7 Confronted with the reality of working excessively long hours,
while living in crowded dormitories under a fiercely disciplinary regime,
these workers aged between eighteen and twenty-five jumped to their deaths
to escape the misery of manufacturing the material infrastructure of the
attention economy. Unlike the scandal, the controversy focused
primarily upon Foxconn rather than Apple or other contractors (Blanchard
 ; Hille . although academic criticism traced accountability back
through the networks of production, arguing that ‘leading international
brands have adopted unethical purchasing practices, resulting in substandard
conditions in their global electronics supply chains’ (Chan and Pun .
Companies like Apple typically sign exclusive contracts with subcontractors
to manufacture the entire stock of new product lines, which the contract
stipulates must be available by a certain date. The penalties for not fulfilling
the specified quota of devices by the deadline are severe, so the subcontractor
typically enacts extremely high levels of compulsory overtime in order to
fulfil the contract.
   In response to the adverse publicity generated by the scandal, Foxconn
announced a series of measures designed to improve worker morale and halt
the suicides. The basic rate of pay was increased fromto yuan
in June, and then from to yuan in October (Blanchard
 ), the number of compulsory overtime hours was reduced (SACOM
 ), and workers were asked to sign pledges that they would not kill


7
 While historically this labour was largely undertaken by women, by per cent of
Foxconn employees were male, partially as a result of female infanticide in China, meaning that
there are fewer young women available as workers (Chan, Pun and Selden .
17


themselves. The wage increase announced in June can be largely attributed
to a rise in the legal minimum wage in Shenzhen to yuan, meaning that
Foxconn’s wages remained just above statutory minimum levels (SACOM
 ); however, the subsequent rise to almost double the legal minimum
wage suggests that under the scrutiny of sustained international attention,
Foxconn felt forced to act before the negative coverage translated into
contractor action.
   One noteworthy aspect of this case which parallels the actions of NGOs
campaigning around Congolese conflict minerals is that the mobilization of
mediated attention which was partially successful in addressing concerns
revolved around the usage of various digital technologies whose conditions
of production were the issue at hand. This point invokes Stiegler’s argument
that technics constitutes a pharmacological situation, whereby contemporary
technologies are simultaneously poisonous and represent the remedy to
their own toxicity. Examining the material impacts of these technologies is
revealing here, as what could otherwise appear to be a metaphor referring
to the toxification of subjectivity is realized in a strikingly literal way through
the exposure of workers to poisonous materials commonplace throughout
the life cycle of microelectronics.
   Although Stiegler does not explicitly engage with the ecological costs
of digital media technologies, the notion of replacing the current mode of
production and the separation of producers and consumers which frequently
sees negative social and ecological impacts of technologies borne by people
and ecosystems far removed from sites of consumption does present a
potential alternative. Indeed, Stiegler’s focus on an economy of contribution
as a system predicated upon inclusive systems of care and the formation
of positive externalities can be seen as a remedy to the current tendencies
towards separation between producers in Congolese coltan mines and
Chinese sweatshops and consumers spatially and affectively removed from
the impacts of the production process. Examining the globalized ecologies at
play here reveals the production of negative externalities which impact upon
vulnerable people and ecosystems by a system designed around economic
growth rather than ecological well-being. However, it must be emphasized
that the disparities of wealth and associated communicative power within
an information society entail that often those human and nonhuman actors
who bear the brunt of these negative externalities are unable to mobilize
attentional flows via social media networks.
   The impacts of using media technologies to campaign for changing the
conditions under which media technologies are produced also demonstrates
why the symbolic and material aspects of technology cannot be effectively
separated into disparate realms. Symbolic communication is never
immaterial, as it performatively shapes material processes and actors; in
this example the production of media and attention affects the labour
conditions under which tools necessary for mediated communication are
  produced, forming a feedback loop whereby semiotic and material systems
are symbiotically entangled.


                        Powering digital culture
Stating that microelectronics require electricity to power them borders upon
tautology, but electricity has to be generated, which within the contemporary
cultural context predominantly requires the combustion of fossil fuels,
entailing the release of carbon dioxide and other greenhouse gases into the
atmosphere thus contributing to anthropogenic climate change, alongside the
localized ecological costs of fossil-fuel extraction, which were dramatically
highlighted by events surrounding the BP-owned Deepwater Horizon oil rig
in which saw 4.9 million barrels of oil spilt into the Gulf of Mexico
(US Coast Guard  3). The energy requirements of powering ICTs
is substantial and rapidly rising, presenting questions surrounding the
ethical and political status of these technologies. Explicating the scope of
these issues is required in order to better comprehend what is at stake: by
paying attention to the problems posed, we can begin to consider the ethical
problematics and formulate programmes designed to remedy issues.
   The global ICT industry was responsible for approximately per cent
of global electricity in , and this is expected to grow substantially due
to the expansion of the Internet of things and server infrastructure, with a
worst-case scenario that ICT could require half of all electricity by 
(Andrae and Edler . The energy requirements of ICTs are not limited
to powering products, as manufacturing microelectronics hardware requires
high volumes of energy. Producing a  -gram iPhone X requires around  
kilogrammes of raw materials – so less than per cent of the overall material
required is present in the final device – and is responsible for the emission
of approximately kilogrammes of greenhouse gases. Manufacturing a
iMac Pro desktop computer releases over kilograms of CO2e8.
For computers and smartphones, the vast majority of life cycle energy costs
pertains to production with the remainder split between transportation and
customer usage. While we think about the energy footprint of these devices
in terms of our actions of plugging them in to charge their batteries or
powering them via mains electricity, this is a small fraction of the total energy
required.9 This is because of the complexity of manufacturing processes and


8
  CO2e denotes an atmospheric forcing equivalent to an amount of CO2. The release of other
greenhouse gases are factored into such equations to provide an overall atmospheric forcing.
9
  There are, however, spectacularly wasteful practices surrounding the electricity usage of ICT,
notably the mining of the cryptocurrency Bitcoin. Bitcoins are produced through ‘mining’,
which involves solving a complex mathematical problem to validate each new section of the
blockchain ledger, requiring billions of calculations to be performed. The Bitcoin network
17


the variety of materials required but also results from the highly limited
lifespan of media hardware within the cultural context of upgrade culture.
    While microelectronics manufacturers have trumpeted claims regarding
improvements in carbon efficiency per transistor within the manufacturing
process, the overall increase in transistors used typically negates these
efficiency savings. This is an example of Jevons paradox, whereby any
increases in efficiency are negated by increased demand for and consumption
of that resource. Similarly, the production of devices which require less power
to operate does not negate the costs of producing these devices in the first
place or the proliferation of networked microelectronics devices. Whereas
in the s a household may have owned a single desktop computer, they
are now likely to possess numerous laptops, tablets, smartphones and other
‘smart’ devices such as televisions, fitness tracking bands, internet radios,
smartwatches and networked attached storage devices.
    The rapid pace of contemporary technological change entails that
obsolescence is often perceived before devices fail to function. For example,
research synthesizing analyses of computer lifespans (IVF  Hoang et
al. .calculates that the average life of a laptop is between two and half
and four years, with desktops lasting only slightly longer. Although most
computers function beyond these durations, users upgrade to newer, more
powerful machines, a process which is frequently driven by the introduction
of newer versions of software, which demand higher system specifications,
resulting in consumers perceiving their hardware to be obsolescent. This
is especially relevant for tablets and laptops, as upgrading hardware often
requires considerable technical expertise, meaning that when one key
component (such as the CPU or graphics card) does not meet the owner’s
requirements, the entire system requires replacement, partially explaining
why laptops have shorter lifespans than desktops. The difficulty in upgrading
components additionally entails that if a key component fails and the system
is outside of warranty, the entire system is usually replaced.
    Perceived obsolescence is also an issue with mobile phones, which are
commonly sold on twenty-four-month contracts, at the end of which the
consumer is offered a new contract, with another free phone as an incentive.
Consequently, the average life cycle of a phone in the United States,
European Union and China is between seventeen and twenty-one months
(Balde et al.  1). Despite the old phone still functioning adequately, it
is either thrown away or left in a drawer to gather dust. In ,million


self-adjusts the complexity of these calculations, so that new blocks are on average created
once every minutes. Due to the value of Bitcoin, which in December reached over
US $19, , astronomical amounts of computational power are expended by Bitcoin miners
performing this task. Consequently, it is estimated that in electricity usage associated with
Bitcoin was over TWh, higher than that of entire nation-states such as Ireland and Austria
(de Vries .
  mobile phones were disposed of in the United States alone, only twelve
million of which were recycled (US EPA . The vast majority of these
devices remain functional, but the economic and cultural imperatives of
networked capitalism and upgrade culture entail that they are perceived as
obsolete. Advertising constantly bombards us with messages that frequently
upgrading our digital devices will make our lives more efficient, enjoyable
and entertaining. Given the ecological costs of producing these devices, the
practice of discarding functional hardware must instead be understood as
ecologically unsustainable and ethically dubious.
    Examining the life cycle energy costs of ICTs proves useful in delineating
that the vast majority of energy is spent producing hardware, partially
due to the limited lifespan of these technologies. Bearing this in mind, it
is unsurprising that extending the lifespan of hardware is among the
most promising approaches to mitigating energy costs associated with
microelectronics. By simply not discarding functional devices, consumers can
significantly reduce the ecological footprint of their technology. However, as
illustrated by the example of mobile phones – where models are annually
replaced and each revision is declared to be a revolutionary innovation –
the corporations producing these technologies are eager for consumption to
speed up rather than slow down, deploying a combination of perceived and
planned obsolescence in order to reduce product lifespans, which increases
ecological costs but raises short-term profitability.
    This highlights the divergence between corporate imperatives driven
by economic profitability and the ecological epistemology presented by
Bateson. The unit of survival for corporations is the individual company,
which competes with rival corporations, viewing social and environmental
commons as assets to be appropriated and exploited, or an othered outside
onto which ecological costs can be externalized in the quest for profitability.
By contrast, Bateson’s epistemology calls for the implementation of economic
models which value wider ecological contexts within which the corporation
is embedded. In the ever-intensifying battle between corporate actors for
consumer attention and economic growth, broader notions of ecological
growth are marginalized, allowing the type of destructive short-termism
criticized by Stiegler to flourish. Indeed, from the longer term, ecological
perspective, such behaviour is ultimately self-defeating, as individual
purposive systems that quarrel with their own ecology inevitably harm their
own futures. Contributing to global ecological crises such as anthropogenic
climate change will eventually prove detrimental to corporate profitability;
however, the short-term structural directives of market valuations effectively
prevent corporations from taking ecologically effective action.
    It must, however, be emphasized that critique which solely focuses upon
the energy costs associated with individual laptops, smartphones or tablets,
merely repeats the misidentification of the individual object as the relevant
scale for analysis. When using modern computing systems, we are very
17


rarely exclusively engaged with the individual microelectronics device with
which we have immediate bodily contact; a vast array of cloud data storage
and processing, fibre-optic cables, 4G cellular towers, Wi-Fi access points,
GPS satellites and other hardware provide the infrastructure necessary for
networked, mobile and pervasive computing. Indeed, the trend seen over
recent years, whereby desktop computers sales have significantly declined,
whereas devices such as tablets, smartphones and ultrabooks have become
increasingly popular, is dependent on most storage and processor-intensive
computation being located elsewhere, while the end user accesses these
resources through ultra-portable thin client devices. Although this situation
provides the illusion of small, smart, green information technologies, the
reality is very different when we consider the magnitude of the back-end
infrastructure necessary for this arrangement, but which is often out of
sight and out of mind. Cubitt, Hassan and Volkmer  highlight this
situation through a discussion of the energy costs and ecological impacts
of the gargantuan server farms built by Google to host the data uploaded
to sites such as YouTube, Google Docs, Gmail and Google Drive. These
facilities draw overmegawatts each, and Google is only one of the
major players in server farms and cloud computing, alongside companies
such as Amazon, Dropbox and Microsoft.
   While Cubitt et al. argue that data servers are likely to be the sector
within information and communications technologies with the most rapid
increase in carbon emissions over the coming years, this claim is disputed
by material from the Centre for Energy Efficient Telecommunications
(CEET) at the University of Melbourne, whose report entitled ‘The
Power of the Wireless Cloud’, contends that much of the focus upon the
environmental impact of data servers has failed to acknowledge the rapidly
growing energy costs of wireless access networks. Especially with regard
to the introduction of the Internet of things, whereby an array of devices
from fridges to watches, televisions to traffic lights are becoming wirelessly
connected to the internet, CEET  .estimates,

  By , [the] wireless cloud will consume up to TWh, compared to
  only 9.2 TWh in , an increase of  %. This is an increase in carbon
  footprint from megatonnes of CO2 in to up to megatonnes of
  CO2 in , the equivalent of adding 4.9 million cars to the roads. Up
  to 90% of this consumption is attributable to wireless access network
  technologies, data centres account for only 9%.

What we see from the work of Cubitt et al and CEET is that the
environmental impacts of information technologies reach far beyond the
devices we see, touch and think of as contributing to ‘our’ individual carbon
footprint. Indeed, the vast assemblages of hardware required to construct
today’s digital world can be conceptualized as resembling an iceberg; the
  hardware we are directly in contact with is just the tip of the technocultural
infrastructure, with the vast majority residing out of sight. This demonstrates
the importance of making the often-imperceptible assemblages of hardware
become visible, which means adopting a stance which explicitly combats the
fuzzy notions of immateriality, virtuality and informationalization which
often accompany discourses of cloud computing.
    An important point which arises from this material is that proprietary
strategies which rely on exclusivity and restriction of access are only likely
to compound the environmental costs of the wireless cloud. When mobile
telephony providers install access points which only benefit the users
of their network, what we see is the replication of infrastructure, which
multiplies the environmental impact of networking technologies. Such
replication is not, however, a technical necessity, and open-access digital
resources can provide examples whereby communally accessible resources
are far more efficiently stored than was previously possible. In order to
access a print copy of Encyclopaedia Britannica, an individual typically had
to own the text, which entailed a substantial economic outlay, alongside
the environmental costs associated with sourcing and bleaching the paper,
inks and printing machinery. Similarly, Encarta, the Microsoft-published
encyclopaedia for computers, required each individual user to purchase the
CD-ROMs on which the data was stored. By contrast, no single end user is
required to download the entire Wikipedia, nor would this be desirable even
if this wasn’t rendered impossible by Wikipedia’s dynamic and metamorphic
nature. The status of the project as open content entails that rather than
replicating data, it can be stored centrally (or at least in a polycentric way
across several mirrors located across the globe) and accessed by hundreds of
millions of users, rather than requiring each individual to possess their own
copy of the ecologically costly artefact.
    Questions over data storage, then, not merely are instrumental and neutral
but contain political affordances which are connected to the sociocultural
usages of these technologies, with models based upon openness at the levels
of content and software, resulting in hardware platforms which could
significantly reduce the environmental impacts of hardware systems, in
contrast to those based upon exclusivity and competitive individualism.
Again, what we see here emphasizes that when we think about the impacts
of digital technology, hardware is fundamentally inseparable from thinking
about software and content, further strengthening the case for thinking
about them as entangled, rather than interconnected phenomena.


                 The global trade in e-waste
Once digital devices reach the end of their useful life, through either
damage or obsolescence, they must be disposed of, but safely disposing of
17


microelectronics is not straightforward, as devices contain numerous toxic
materials. The multiplicity of materials within microelectronic devices
typically include acetone, lithium, hydrochloric acid, benzene, arsenic and
plastics that contain phthalates and brominated flame retardants, all of
which are hazardous to humans and other living system. Until their usage
within consumer microelectronics was banned in under the European
Union’s Restriction of Certain Hazardous Substances in Electrical and
Electronic Equipment (RoHS) directive, lead, mercury, cadmium, hexavalent
chromium and polyvinyl bromides were also ubiquitously utilized within
ICTs. Consequently, e-waste accounts for ‘forty percent of all lead, seventy
percent of the heavy metals and a significant proportion of the pollutants in
US dumps’ (Byster and Smith  10). In , 44.7 million metric tonnes
of e-waste were generated globally (Balde et al.  8). Consequently,
both the toxicity and volume of e-waste present further examples of hidden
ecological harms associated with digital technologies.
    As with other Anthropocenic environmental issues, e-waste production is
highly heterogeneous, with nation-states such as the United States, United
Kingdom and Australia generating over kilograms per inhabitant per
annum, whereas the continental average for Africa is under kilograms
per inhabitant annually. Currently, wealthier nations send large volumes of
toxic e-waste to poorer countries, where they are manually treated, with
few safeguards in place to protect the health and safety of workers or local
environments. This externalization of environmental harm from rich nations
to poorer ones has been the subject of various campaigns since the turn of the
century, and while these have had some successes, in , it was still found
that around per cent of e-waste sent for recycling in the United States
was illegally shipped to the developing world for ‘recycling’ (Hopson and
Puckett . We should note how the rhetoric of recycling and associated
connotations of environmental concern and sustainability have been co-
opted into a system that breaks international law to externalize ecological
costs onto impoverished people. These flows of toxic waste denote another
level at which digital colonialism operates; whereas raw materials and data
are extracted from the global poor to enrich global elites, toxic waste is then
sent back to impoverished areas.
    Under the Basel Convention, adopted in and amended in , it
is illegal for most OECD nations to export hazardous waste and for non-
OECD countries to receive it (although the treaty was not ratified by the
United States almost all developing countries have done so, entailing that it
illegal to ship e-waste to those places). More recently, the European Union’s
Waste Electrical and Electronics Equipment (WEEE) directive requires all
e-waste within the European Union to be recycled at local facilities and
that retailers must allow consumers to return old electronics products when
purchasing replacements. While the WEEE directive and Basel Convention
exist to prevent a global trade in e-waste, these laws are routinely
  circumvented through waste management companies shipping e-waste
labelled as working, second-hand goods for sale in non-OECD countries:
‘In Lagos, while there is a legitimate robust market and ability to repair and
refurbish old electronic equipment including computers, monitors, TVs and
cell phones, the local experts complain that of the estimatedforty-foot
containers shipped to Lagos each month, as much as seventy-five percent of
the imports are “junk” and are not economically repairable’ (BAN .
Part of the difficulty results from the vast global flows of material through
major shipping routes. Effectively policing the movement of e-waste would
require significant investment in port authorities, who would have to
not only check whether these devices were functional but be sufficiently
knowledgeable to adjudicate whether faults were easily repairable.
    Once e-waste arrives in processing centres, it is manually disassembled,
with valuable materials such as copper, gold and silver gathered for re-sale.
The techniques employed to recover valuable materials by artisanal e-waste
recyclers in developing nations would typically be illegal in developed
nations where many of these devices were used, as they poison workers and
local ecosystems. For example, wires are often burnt, melting the plastic
casing to revealing the valuable copper contained inside. As the insulating
plastics often contain PVC and brominated flame retardants, the smoke
emitted contains organically persistent and highly toxic compounds such
as brominated and chlorinated furans and dioxins. Acid baths are used to
recover the gold used in pins which connect silicon chips to circuit boards,
with the corrosive leftover products of nitric and hydrochloric acids mixed
with the dissolved remnants of the components routinely finding their way
into local water tables, rendering the water undrinkable. CRT monitors
have the lead-laden glass cracked open so that the valuable copper yoke
can be removed, resulting in lead leaching into the ground and water table.
    Consequently, when the Basel Action Network tested water from the
river in Guiyu, China, a major e-waste processing area, they found it
contained lead levelstimes higher than the maximum level safe for
human consumption prescribed by the World Health Organization (BAN
and STVC  4). Subsequent research has found elevated levels of lead
in the blood of children of Guiyu (Guo et al. . As lead poisoning in
children is associated with impaired cognitive development, dealing with
toxic high-tech trash serious damages the health and future available to
these people. Soil from Agbogbloshie, Ghana, another major e-waste
processing area presented similar findings, with lead levels being over  
times the background level, alongside elevated levels of cadmium, antimony,
phthalates and chlorinated dioxins (Greenpeace . Most of the  , 
tonnes of e-waste in Ghana comes from Europe and North America (Itai et
al. .
    Workers in e-waste processing zones frequently include children,
sometimes as young as five years old (Greenpeace . Wages are often
18


in the region of US$1.50 a day (Roman and Puckett  . ‘Workers and
the general public are completely unaware of the hazards of the materials
that are being processed and the toxins they contain. There is no proper
regulatory authority to oversee or control the pollution, nor the occupational
exposures to the toxins in the waste. Because of the general poverty people are
forced to work in these hazardous conditions’ (BAN and STVC  6).
The global trade in e-waste involves affluent countries externalizing
ecological costs onto the peoples and ecosystems in impoverished areas of
the world. In this way, the material architecture of the attention economy
detrimentally impacts upon the digital have-nots, as vast quantities of
hazardous materials are sent across the world to be manually ‘recycled’ by
people who do not comprehend the harms to themselves, their communities
or local ecosystems wrought by their actions. Consequently, both drawing
attention to this inequitable situation and finding ways of enforcing existing
legislation which renders exporting e-waste illegal become crucial tasks if
the inequities and ecological harms currently caused by e-waste are to be
addressed.
   One strategy widely championed as an effective method for dealing
with problems caused by e-waste is extended producer responsibility
(EPR) (Raphael and Smith  Huisman and Magalini .allied with
legislation mandating EPR schemes. EPR is designed to make producers
responsible for the disposal of devices, requiring that companies take
products back once they break or become obsolete, or that they pay another
company to undertake local recycling, thereby avoiding the exportation of
toxic material to areas where it will be manually disassembled. EPR requires
producers to internalize ecological costs, ideally leading to producers
redesigning hardware so that many of the current hazards are eliminated
through sustainable design:
  Rather than seeking remedies at the end of a product’s life, we need a
  new political and economic infrastructure to support the development
  of products that are safe and sustainable throughout their life-cycle. This
  policy must prevent pollution and waste rather than just controlling it.
  Just controlling waste, which has been the approach society has relied on
  since the Industrial Revolution, is no longer adequate or acceptable. (EPR
  Working Group 
The European Union sought to implement a comprehensive system enforcing
EPR through the WEEE directive, which was passed into law in ,
requiring that by August all companies which supplied electronic
equipment within the European Union were to establish programmes to
collect and safely recycle e-waste.
   Before implementation, the WEEE directive was widely lauded as an
excellent piece of legislation; in practice, however, less than per cent of
end-of-life EEE is being returned and treated under these schemes, leaving
substantial room for improvement (Balde et al. . One report into the
  efficacy of the WEEE directive concludes, ‘Increasing consumer awareness
is a necessity for an eco-efficient WEEE implementation with maximized
environmental results (collect more) and increased costs efficiency (treat
better)’ (Huisman et al. . This foregrounds the role that mediated
communications can play in publicizing existing schemes by raising
awareness of their existence. Without effective publicity (i.e. media content)
leading to an informed populace, the WEEE directive will continue to make
only a fraction of its potential impact. Without effective communication
and molecular activity, the molar strategy of international legislation
does not necessarily realize its aims. What we see here is not an argument
for either molar or molecular strategies, for regulation or activism; eco-
effective outcomes are most likely to arise from actions which combine
these approaches, exemplifying the synthetic logic of the AND proposed by
Deleuze and Guattari.
    A further problem with the WEEE directive is that the existing global trade
in e-waste masquerading as functional second-hand goods still functions,
presenting companies an avenue that avoids paying for WEEE processing.
An investigation into e-waste being illegally shipped to Nigeria found that
29 per cent came from the European Union (Odeyingbo, Nnorom and
Deubzer  . despite this breaching both the WEEE directive and Basel
Convention. Again, this highlights the weaknesses of unenforced molar
strategies; in this case, the risk/reward scenario for companies surrounding
the cost of legally processing e-waste and the risk of being detected while
shipping defunct items as second-hand goods frequently sees laws broken.
While a total system of surveillance and control over global flows of matter
is not economically viable, this does suggest that significantly increasing
penalties for those found exporting hazardous materials such as e-waste
may alter the risk/reward calculation.


       Linear and cyclical models of production
Alongside the WEEE directive, the European Union passed the Restriction
on the Use of Hazardous Substances in Electrical and Electronics Equipment
(RoHS) directive, which focused upon eliminating several of the most harmful
substances which were previously ubiquitously used within microelectronics
manufacturing: mercury, hexavalent chromium, lead, polybrominated
biphenyls and polybrominated diphenyl ether – making it illegal for new
devices to contain more than trace amounts of these substances.10 RoHS
has proved a highly effective piece of legislation because it targets design;
banning the usage of toxic materials entails that issues relating to toxicity do


10
  In RoHS was amended to include four additional substances, all of which are phthalates,
organic compounds that are used as plasticizers.
18


not exist later within a product’s life cycle. Design, then, potentially becomes
a crucial point in the life cycle for the implementation of ecologically
beneficial alternatives to current practices.
    A first major issue with the current design process is the structure of the
‘life cycle’ of microelectronics produced under the logic of industrialism.
Currently, this proceeds as a near-linear chain of events which begins with
product design and extracting raw materials, and ends with disposal. A
limited amount of down-cycling occurs during e-waste processing whereby
certain valuable resources are extracted from used products for reuse. This
process is realistically described as down-cycling rather than recycling as
only parts of the product are re-used, and these typically create materials of
lower quality than the original. This does not form a cyclical process where
‘waste’ becomes resources for next-generation materials, it merely extends
the limited lifespan of certain parts of these products.
    The near-linear progression from design to the dump that is characteristic
of current industrial processes contrasts sharply with cyclical processes
found within ecology and life sciences, which are sustainable and resilient.
In these cases, ‘waste’ becomes ‘food’, outputs become inputs for the next
iteration of the process, meaning that these systems can run continuously for
millennia without accumulating problematic stockpiles of ‘waste’ material.
Below are simplified diagrams of the nitrogen cycle   , an example
of a sustainable process and the industrial model of production   .
    Whereas every stage within the nitrogen cycle is infinitely repeatable
with no identifiable beginning or end point, the industrial production model
inexorably leads towards landfill or incineration, with the outputs not
being recoverable into fresh resources for the next generation of products.
Problems with this mode of design are compounded by the incorporation
of substances which are toxic to organic systems; not only is there an
accumulation of waste material, but this stockpile is hazardous to life.




FIGURE 5.1 The nitrogen cycle.
  
FIGURE 5.2 Industrial production model.


   The nitrogen cycle includes multiple feedback loops, a systemic
arrangement that builds systemic resilience as it allows for a certain amount
of slippage within the system to be absorbed without the entire process
collapsing. This stands in contrast to the model of the linear production cycle
which is streamlined for efficiency, leaving as little redundancy as possible.
In ecological and evolutionary systems, ‘the key is flexibility, not admirable
precision’ (Gould  4). Ecological and biological systems are resilient
precisely because they incorporate redundancy, affording flexibility in an
uncertain and changing world, whereas this redundancy is conspicuously
absent from the optimized, efficient linear production models of just-in-time
capitalism. While this presents a somewhat crude and reductive approach to
approaching cyclical processes within ecological systems, we can juxtapose
the geological durations of the nitrogen cycle, which was relatively stable
for two billion years (until the onset of the Anthropocene), with the two
centuries of industrial civilization, which has produced multiple concurrent
Anthropocenic ecological crises.
   Problems regarding the linearity of industrial design are addressed by
the cradle-to-cradle movement and related concepts of zero waste and the
circular economy. Founded by Michael Braungart, an industrial chemist,
and William McDonough, an architect, the cradle-to-cradle movement
explicitly seeks to remodel the entire system of industrial manufacture
according to cyclical processes. Braungart and McDonough argue that the
linear cradle-to-grave model which has dominated design and production
since the industrial revolution is extremely wasteful:

  More than ninety percent of materials extracted to make durable goods
  in the United States become waste almost immediately. Sometimes the
  product scarcely lasts longer. It is often cheaper to buy a new version of
  even the most expensive appliance than to track down someone to repair
  the original item. … What most people see in their garbage cans is only
  the tip of a material iceberg: the product itself contains on average only
  five percent of the raw materials involved in making and delivering it.
  (Braungart and McDonough  7)
18



FIGURE 5.3 Revised industrial production process.


Indeed, bearing this in mind, it is worth revising our notion of the industrial
production model    to account for the fact that waste is generated
at every stage throughout the process. This is particularly evident when
dealing with portable microelectronics such as a smartphone where, as we
have seen, over per cent of waste is generated in production, rather than
in end-of-life disposal.
   Consequently, Braungart and McDonough contend that design and
manufacturing require re-orientating towards an ethos of eco-effectiveness;
whereby the logic of economies of scale and the widespread deployment of
toxic materials are supplanted by designs where every material employed is
entirely recoverable and reusable. This reorientation of production stands in
contradistinction to contemporary environmental approaches that portray
humanity as inevitably destructive towards natural systems, proposing
that such approaches stem from erroneous epistemological premises which
accept the linear model of industrial production as the only valid system of
production. Consequently, environmentalist actions are delimited in scope
to lessening inevitable harm(s) perpetuated by human actions, rather than
transforming the logic of production to generate positive environmental
contributions. The cradle-to-cradle approach therefore proposes a process
of industrial re-evolution:

  Our products and processes can be most deeply effective when they
  most resemble the natural world. … Natural systems take from their
  environment but they also give something back. The cherry tree drops
  its blossoms and leaves while it cycles water and makes oxygen; the ant
  community redistributes nutrients throughout the soil. We can follow
  their cue to create a more inspiring engagement – a partnership with
  nature. We can build factories whose products and by-products nourish
  the ecosystem with biodegradable materials instead of dumping, burning
  or burying them. (Braungart and McDonough  55/ )

The cradle-to-cradle movement has enjoyed several notable successes in
implementing this type of change to specific industrial processes, particularly
  within the textile industry. Working with Victor-Innovex, a Quebec-based
fabric producer, Braungart and McDonough developed eco-intelligent
polyester, a fabric designed to be completely recyclable and which avoids
many of the toxic materials traditionally used in polyester production. For
example, antimony, a catalyst regularly used in polyester fabrication, is a
carcinogen and is poisonous to various human organs including the heart,
lungs and skin. Conventional methods of recycling polyester involve a high-
temperature process releasing antimony-trioxide into the air. McDonough
and Braungart’s design replaced antimony with a titanium and silica-based
catalyst which lacks the toxicity of antimony. Furthermore, McDonough
and Braungart  a) ‘analysed all the dyes and auxiliaries Victor used
in the manufacture of polyester, trimming a list of fifty-seven chemicals to
fifteen. Of those, several were replaced with more environmentally sound
chemicals’. Utilizing innovative green design to replace toxic materials
from industrial processes points to an example of how implementing ethics
via design can create ecologically beneficial alternatives to certain current
practices.
    The restoration of Ford’s Rouge River manufacturing complex in Dearborn
Michigan is another case where Braungart and McDonough’s principles
have been applied. While transforming an ageing 1, -acre industrial site
into a modern complex, featuring a living roof, improved insulation and
the utilization of natural skylights (McDonough and Braungart b) can
all be understood as positive environmental contributions, McDonough
and Braungart’s contentions that this demonstrates Ford’s commitment
to environmentally sound production, with the possibility of cradle-to-
cradle style automobile manufacturing in the future, seems hopelessly naive
when considering that the complex is used to manufacture mile-per-
gallon F  trucks. Similarly, Coca-Cola promote their green credentials
for incorporating cradle-to-cradle carpeting in their Pemberton Place site
in Atlanta, United States; however, the company has been subject to an
ongoing boycott from human rights campaigners in relation to numerous
incidents across the corporation’s globalized workforce. Incidents cited by
activists include financing the murder of trade union leaders by right-wing
paramilitaries in South America (Killer Coke a), firing workers for
attempting to unionize at numerous plants across several continents, and the
overexploitation and pollution of groundwater in India (Killer Coke b).
While the complexities of globalized capitalism entail that all companies are
implicated in exploitative practices – such as those delineated throughout
the course of this chapter – not all actors within these networks are ethical
equals; not all corporations stand accused of hiring paramilitaries to execute
workers for attempting to unionize.
    Heralding companies such as Ford and Coca-Cola as harbingers of an eco-
effective re-evolution is problematic, insofar as despite conducting beneficial
actions in particular areas of environmental policy, these corporations
18


frequently breach labour rights or create environmental degradation
elsewhere. Presenting their vision of a sustainable social ecology, Braungart
and McDonough present a triangular diagram with the points representing
ecology (by which they meant environmental ethics), economy and equity.
However, their uncritical support for corporations whose actions towards
their workforce and environmental impacts are so deeply inequitable
undercuts their vision of a future which is not merely sustainable but claims
to encourage positive growth across ecology, equitability and economy.
   Recalling the model of the three ecologies, effective ecological action
is not just aimed at the environment but is designed to create beneficial
conditions for social and psychological relations, not sweatshop labour,
union busting and gas-guzzling motor vehicles. Providing positive public
relations for corporations to present themselves as ethically concerned
businesses, despite their dubious human rights and environmental justice
practices, merely creates positive brand associations that corporations are
often more concerned about than underlying ethical concerns (Klein 
Peretti . Although the cradle-to-cradle movement presents useful ideas
with regard to removing toxic and non-biodegradable substances from
product design, their overarching vision for a positive future is seriously
tarnished by an uncritical celebration of corporations whose labour and
environmental justice records are severely problematic.


               Beyond planned obsolescence
Alongside issues surrounding the cradle-to-grave life cycle of microelectronics,
a second major problem surrounding the overall structure of microelectronics
life cycles relates to planned obsolescence. The idea was initially proposed
by Bernard London, who in argued that a major factor perpetuating
economic depression in the United States was that citizens were increasingly
repairing and reusing goods and products, extending their lifespans. London
argued that this created a lack of consumer spending, supressing economic
growth and recovery. To combat this, London proposed:

  I would have the Government assign a lease of life to shoes and homes
  and machines, to all products of manufacture, mining and agriculture,
  when they are first created, and they would be sold and used within
  the term of their existence definitely known by the consumer. After the
  allotted time had expired, these things would be legally ‘dead’ and would
  be controlled by the duly appointed governmental agency and destroyed
  if there is widespread unemployment. New products would constantly
  be pouring forth from the factories and marketplaces, to take the place
  of the obsolete, and the wheels of industry would be kept going and
  employment regularized and assured for the masses. (London )
  While London’s plans for a centrally mandated form of planned obsolescence
never materialized, many digital devices incorporate elements of planned
obsolescence into their design.
   Devices such as the Apple iPhone, iPad and Google Pixel devices feature
non-removable lithium-ion batteries, whose performance degrades over
time; most Li-ion batteries deliver aroundfull charge/discharge cycles
(Buchmann . entailing that these batteries typically last between two
and four years. Eventually, the battery will hold only a fraction of its original
capacitance, denoting that for portable devices such as smartphones and
watches, the battery is essentially useless. While manufacturers could design
products with removable batteries, so that once the limited lifespan of the
battery expires it could simply be replaced, products such as the iPhone are
deliberately designed so that users cannot easily achieve this, with the use
of nonstandard screw threads and the battery being held in place by strong
adhesive. In some cases, it is possible for consumers to send devices back to the
manufacturer to have batteries replaced; however, the monetary cost is often
substantial, for example, replacing an iPhone battery in this manner costs
£45 to £79. By contrast, replacement batteries for the LG G4, a smartphone
featuring a removable battery, costs around £10. Additionally, the practice of
telecommunication providers giving away ‘free’ phones with new contracts is
further designed to make discarding old but functional phones an attractive
option for consumers, combining planned and perceived obsolescence.
   Designing products that require replacement every few years – once the
supplied battery fails to hold an adequate charge – makes economic sense
for the companies involved, as if products have relatively short lifespans,
consumers will purchase new hardware regularly, rather than simply
replacing the inexpensive battery and continuing to use older devices.
Indeed, from the perspective of upgrade culture, if consumers were not
prepared to regularly invest in new hardware, then electronics corporations
would become less profitable and begin to slow down the R&D process and
product cycle to adjust to consumer demand. If this occurred, the price, as
opposed to the ecological cost, of these items would likely rise, as the rate of
consumption presents a feedback loop which drives R&D and keeps prices
down, in turn driving further consumption. From an ecological perspective,
however, it makes little sense for people to discard functional hardware
because a solitary, cheap-to-manufacture, and potentially easy-to-replace
component no longer works adequately. As we have seen, the short lifespan
of consumer electronics is a significant contributor to the extremely high
energy costs, so one effective strategy for reducing these costs is extending
product lifespans. This again returns us to ethology and the politics of
speed in digital capitalism; curbing the destructive velocity of consumption
becomes key to ecologically beneficial outcomes.
   A useful example showcasing how networked digital technologies
enabled productive engagements with planned obsolescence is provided
18


by iFixit, a website and online community which provides a communally
generated repository of repair guides. As of , the site lists over thirty
thousand repair manuals featuring devices from tablets to toasters. Each
guide is available as a HTML webpage, via an iOS and Android application,
or as a downloadable PDF file. In each case, the contents are released under
a Creative Commons 3.0 Attribution, Non-commercial, Share Alike licence,
meaning that anyone can reproduce, remix and republish the content, so
long as the resulting works are issued under the same licence, the derivative
works attribute the original source material as coming from iFixit, and
those remixed works are not sold. In addition to repair manuals, the iFixit
site features a space for community members to ask questions, allowing new
and less technically adept members to draw upon the broader community’s
expertise, with the site containing around  ,  of these questions and a
YouTube page with video repair guides and device disassembly guides which
have over fifty million views.
   iFixit additionally maintains a blog which provides a manifesto arguing
why repairing devices is ecologically and socially beneficial. These arguments
include explorations of issues surrounding e-waste and ethical mineral
procurement, alongside individual factors such as consumer freedoms and
cost savings, and social issues such as the creation of local jobs. These issues
are outlined in    the Self-Repair Manifesto, and the iFixit blog
explores each point in detail, using source material from NGOs such as
the Basel Action Network and the Institute of Electrical and Electronics
Engineers to substantiate the claims being advanced.
   It is worth noting the homology which exists between the triumvirate
of levels at which iFixit claims that repairing electronics presents positive
outcomes, which are identified as the individual, society and environment,
and the three ecologies outlined by Bateson and Guattari as mind, society and
environment. While there is clear overlap among the categories of society and
environment, it is worth highlighting the discrepancy between the enclosed
and atomized notion of the individual consumer, and the distributed and
connectionist conception of the mind which Bateson forcefully argues
is immanent outside of the body. Indeed, for Bateson  68), the
reduction of the mind to the level of the individual represents a fundamental
epistemological pathology which has serious evolutionary repercussions for
(post)modern humans:


  As you arrogate all mind to yourself, you will see the world around you
  as mindless and therefore not entitled to moral or ethical consideration.
  The environment will seem to be yours to exploit. Your survival unit will
  be you and your folks or conspecifics against the environment of other
  social units, other races and the brutes and vegetables. If this is your
  estimate of your relation to nature and you have an advanced technology,
  your likelihood of survival will be that of a snowball in hell.
  
FIGURE 5.4 Repair Manifesto: source iFixit.com. CC BY-NC-SA 3.0.


For Bateson, then, the three ecologies cannot be reduced to individual,
society and environment, as there are no enclosed and bounded individuals.
Indeed, this alteration entails that rather than an entangled meshwork,
there are three relatively autonomous domains, which we can comprehend
as being interconnected, but which are discrete and separable, marking a
critical departure from an ecological onto-epistemology.
19


   iFixit could, however, be seen as making an important intervention around
Stiegler’s argument that digital mnemotechnics – exteriorized technologies
which comprise a crucial element of human memory – can form associated
milieus which resist the communal loss of knowledge which Stiegler,
following Marx but departing from conventional Marxist readings, terms
‘proletarianization’. Whereas most Marxist accounts of proletarianization
focus upon the transition of the rural peasantry into the urban working class,
Stiegler contends that what underpins this transformation is the relationship
between the worker and the product of their labour. Whereas the rural
agricultural worker understood the production processes surrounding the
growth of crops and their conversion into saleable goods, the urban factory
worker is merely one cog in the factory machine, and so becomes alienated
from the product of their labour as each worker is deprived of knowledge
regarding the process of production. Stiegler argues that this knowledge
becomes externalized within the machinery of the factory, which effectively
becomes an exteriorized form of technocultural memory.
   The emphasis on the externalization of memory denotes the continuity
between Bateson and Stielger’s schemata of the distributed nature of being.
Where Stiegler goes beyond Bateson, however, is in his detailed delineation
of the ways in which mnemotechnics entail that collective memories become
subject to forms of biopolitical control, exercised by the elements within
a social ecology which have control over the means of mnemotechnical
production. Whereas the centralized structure of the mass media led to an
industrialization of memory and the formation of a dissociated milieu in
which production and consumption remained distinct activities, whereby
production was only accessible for a tiny socioeconomic elite, Stiegler
 b: emphasis in original) contends that contemporary digital
technologies point towards a pivotal change in this dynamic as digital
technologies enable the collaborative construction of communal knowledge
and memory. Here the Self-Repair Manifesto’s point that repair ‘makes
consumers into contributors’ would appear to be emblematic of Stiegler’s
arguments surrounding an economy of contribution and the formation of
collectivized forms of exterior memory.
   However, as we have seen, the digital age far from guarantees the end of
the separation between producers and consumers, especially when we look
beyond content towards systems of software and hardware. Indeed, when
comparing the reparability of devices, the move towards ultra-portable thin
client devices such as tablets and smartphones, sealed units designed not
to be disassembled by end users, demonstrates a further loss of knowledge
when we consider our understandings of, and ability to maintain, the
mnemotechnical devices which accompany us through life. Viewed in this
way, digital technologies may not present the end of the era of dissociated
milieus as Stiegler suggests but their intensification. Alternatively, while sites
  like iFixit may enable a technologically literate elite to repair their devices,
this could become another minority lifestyle choice, whereby a handful
of geeks can feel good about their ethical repair-based activities, but the
vast majority of humans simply keep consuming black-boxed devices at an
increasing pace.
   iFixit does, however, demonstrate that there are ways of combatting
attempts to remove the communal knowledge to maintain and repair
microelectronics hardware which involve leveraging the cooperative and
communicative affordances of networked digital technologies. We again
see the merit in Stiegler’s demarcation of technology as pharmakon,
simultaneously poison and cure, as the technologies whose design veers
towards planned obsolescence and a raft of negative consequences that
permeate across the three ecologies are also the means by which a distributed
community is able to collaboratively produce a common resource which
addresses issues surrounding planned obsolescence and proletarianization.
Such a pharmacological approach precludes Stiegler’s position from
becoming a simple claim for the revolutionary positive effects of digital
technologies, as, while he clearly acknowledges the potential formation of
associated milieus which produce an economy of contribution and an ethic
of care, this must always be contrasted with the toxic reterritorializations
which accompany such possibilities.
   An alternative, though related, method for redressing the problematics
associated with planned obsolescence is found in the open-source hardware
(OSH) movement. Also referred to as open hardware and open design, the
OSH movement is based around the principles of free and open-source
software. A working definition of OSH, derived from the Open Source
Initiative’s definition of open-source software explicates that ‘Open Source
Hardware is a term for tangible artefacts – machines, devices, or other
physical things – whose design has been released to the public in such a way
that anyone can make, modify, distribute, and use those things’ (OSHW
 ). A crucial point regarding OSH, then, is that the blueprints for making
hardware are freely and publicly available, with the licence granting users
the freedom to utilize and alter designs.
   The emphasis on design is paramount, because by opening up the way
products are made to innovation over distributed networks of users via the
internet, OSH implements a methodology of creating rivalrous, physical
objects along similar lines to those by which non-rival FOSS are created.
Importantly, this signifies a shift in the form of production where peer
production presents an alternative to market-based production. Goods are
described as rivalrous assets if their material embodiment prevents them
from being simultaneously used by multiple parties. For example, if I am
using your digital camera, then you cannot simultaneously be using the
same device elsewhere. Consequently, we can be conceptualized as rivals
19


to use the device, whose distribution is governed by scarcity. By contrast,
informational goods, such as ideas, languages and affects, tend to be non-
rival, insofar as any number of people can simultaneously access words and
thoughts without the originals being depleted.
   Crucially, the advent of networked digital computers with vast amounts
of storage space altered the status of certain kinds of information from
rivalrous to non-rival goods, so long as certain preconditions are met
(Benkler  5–6). Whereas copying a vinyl record to an audio cassette
tape entailed degrading the quality of information, copying music between
computers produces duplicates that are indiscernible from the original, at
virtually zero cost, due to the proliferation of high-capacity digital storage
devices. The materiality of computational technology does not, however,
equate to immateriality; the specific material capacities of digital computers
afford lossless copying and storing vast quantities of data on ever-smaller
devices. Furthermore, the networked nature of twenty-first-century
computing technologies afford the ability to share information across
massive distributed peer-to-peer networks, entailing that information can
be shared among millions of peers rather than relying upon face-to-face
interaction.
   While the internet provides a relatively straightforward method of
copying and distributing various forms of content and software, this
mode of digital translation does not initially appear to apply to hardware;
physical objects such as washing machines, computers and cars remain
rival resources, which cannot be translated into binary code. Until the early
part of the twenty-first century, this difference was perceived as sufficiently
critical to prevent the emergence of an open hardware movement paralleling
the FOSS movement:

  Because copying hardware is so hard, the question of whether we’re
  allowed to do it is not vitally important. I see no social imperative for free
  hardware designs like the imperative for free software. Freedom to copy
  software is an important right because it is easy now – any computer
  user can do it. Freedom to copy hardware is not as important, because
  copying hardware is hard to do. Present-day chip and board fabrication
  technology resembles the printing press. Copying hardware is as difficult
  as copying books was in the age of the printing press, or more so. So
  the ethical issue of copying hardware is more like the ethical issue of
  copying books years ago, than like the issue of copying software
  today. (Stallman )

Where OSH intervenes is not the physical act of copying hardware, but the
process of designing hardware. By making the blueprints for various types
of device openly available for anyone with a networked digital computer to
peruse, download, examine and modify, OSH presents an alternative to the
  ‘manufacturer-centric innovation development systems that have been the
mainstay of commerce for hundreds of years’ (Von Hippel  .
   One device developed subsequent to Stallman’s contention that OSH
is untenable with contemporary technologies, and whose popularity and
versatility suggests otherwise, is the Arduino. Launched in , Arduino
is an open-source, electronic microcontroller which consists of a CPU,
memory, circuit board and inputs/outputs whose precise specifications
depend on the particular kit. Arduinos are programmed via a Universal
Serial Bus (USB) port, which is connected to a personal computer, using
the Arduino programming language, which is based upon Wiring, an open-
source programming framework for microcontrollers. The low cost of the
board – about $35 for a basic Arduino branded kit11 − ensures that economic
barriers to entry are minimal for individuals who already have access to a
computer and network connection. The hardware designs are issued under
a Creative Commons licence, and the software used to interface with the
device is issued under an open-source GPL licence.
   The growing Arduino community has used the versatile microcontroller
for projects from robotics to electronic textiles, controlling art installations
to creating inexpensive MIDI outputs for accordions. Looking just at
Arduino user-community-developed innovations with a specific focus on
environmental themes, there are numerous systems whose designs are now
available for others to use or modify. These include Open Energy Monitor,
a project aiming to develop open-source systems for energy monitoring,
control and analysis, alongside tools designed to increase energy efficiency
and promote the usage of distributed renewable micro-generation. Like
many FOSS projects, a large OSH project such as the Open Energy Monitor
brings together a geographically distributed group of developers, who can
allocate tasks in a modular fashion, allowing tasks to be divided among
the distributed community (Benkler  Holland, O’Donnell and Bennett
 ). The Open Energy Monitor project itself consists of a modular
array of sensors, displays, microcomputers (which act as data servers)
and EmonCMS, a custom-built open-source content management system
which logs, processes and visualizes the data collected by the Open Energy
Monitor system.
   Gardenbot is another Arduino-based project that aims to affect
environmental sustainability. Users embed sensors in their garden, enabling
Gardenbot to ‘show you charts of the conditions in your garden – so you
can see the world the way your plants see it’ (Freuh . This highlights
that sensor systems can make nonhuman perspectives visible in ways that
are designed to mobilize practices of caring for nonhumans. By making


11
  But as an OSH device there are community-designed alternatives based on the original
schematic which can be found for or less.
19


the perspective of plants tangible, we can act in ways designed to enhance
their well-being. There exist numerous other Arduino-based systems
geared towards building environmentally conscious technology, such as
self-powering systems to enable solar panels to track sunlight, systems to
increase battery life through monitoring sleep/standby functions, systems
for monitoring water usage to help reduce unnecessary water wastage,
and self-watering gardening systems which sense when soil is dry and then
irrigates arid areas. The diverse range and scope of peer-produced commons-
based hardware projects emanating from the nascent but growing Arduino
community demonstrates the viability of OSH projects.
    Utilizing low-cost technology alongside open design and licensing, these
projects provide ecologically beneficial, user-generated systems available
to anyone with the requisite technicity, whose designs are freely available
for others to innovate with and build upon. Advocates have suggested
that the OSH movement therefore presents a possible starting point for
an alternative to industrial hardware production. Whereas profit-driven
corporations have an economic imperative to produce goods designed to
become rapidly obsolescent, OSH can be designed in a modular manner,
geared towards extending the duration of usage, user/producers are
not typically interested in having to regularly replace hardware. These
differences are part of what Bauwens  describes as the positive
externalities of commons-based peer production, which contrasts sharply
with the negative externalities of late-capitalist production, whereby, as
we have seen, detrimental effects are frequently externalized onto workers
or the environment.
    An important difference between FOSS and OSH remains the fact that
whereas FOSS is generally available for no economic cost OSH requires
users to purchase the materials to assemble the hardware.12 It should be
noted, however, that in manufacture-centric production systems, the end
user pays for the cost of materials, in addition to the intellectual endeavour
in designing and patenting the product. Additionally, many OSH projects,
such as the Open Source Washing Machine – an OSH project which uses
an Arduino to create a low-cost, low-energy washing machine for people
currently unable to afford a commercial device – are designed to use recycled
or low-cost, readily availability materials in order to prevent the cost of
materials from becoming prohibitive. However, current practices, where
users purchase the materials for assembly, do present material barriers to
participation in OSH communities in addition to the requirements of access
to a networked computer.
    One OSH project which sought to reduce or remove this material barrier
to access is RepRap. Initiated by Adrian Bowyer of the University of Bath


12
 Or purchase a pre-built kit as with the example of the Arduino.
  in , RepRap – a contraction of (self)Replicating Rapid Prototyping
machine – is an open-source device issued under the GPL licence, which aims
to create a machine capable of constructing the necessary parts to produce
a working copy of itself; a self-replicating machine. While self-replicating
machines have been theorized since the early days of cybernetics (Von
Neumann . the RepRap project provides a practical example of an
OSH project which not only is designed to generate replacement parts so the
machine can repair itself, but could afford individuals and communities the
capacity to manufacture a wide number of items, using either custom-made
designs or pre-made open source patterns, including additional RepRap
machines. Consequently, much of the hype surrounding RepRap and related
3D printing technologies has invoked the replicator found in Star Trek, a
device capable of producing any desired object, which in its most utopian
accounts can act as a panacea for material scarcity.
   RepRap is built around an Arduino circuit board, and the device uses
a process called Fused Filament Fabrication (FFF), a form of 3D printing,
to manufacture objects from blueprints created using open-source CAD
software. The FFF process involves printing an object using layers of
thermoplastics, with the object being built up in three dimensions over a short
period of time. The most commonly used materials for FFF are acrylonitrile
butadiene styrene (ABS), a petroleum-derived plastic, and poly lactic acid
(PLA), a biodegradable material produced from cornstarch or sugar cane.
Although ABS has a higher melting point and tensile strength, PLA has
the advantages of not being produced from fossil fuels or contributing to
global crisis of non-biodegradable petroleum-derived plastics (Michaels
 3). Immediately, we see that there are potentials pitfalls as well as
positive outcomes associated with 3D printing. Depending upon the specific
materials used, this can transform fossilized prehistoric life into ‘disposable’
objects that have afterlives measured in millennia, as they slowly degrade
into microplastics that attract persistent organic pollutants, become ingested
by organisms and which mimic hormones and so disrupt their endocrine
systems. Which materials are employed matters when it comes to the
ecological impacts of technologies.
   The first 3D printers were produced in the s; however, as recently
as , the cheapest commercial 3D printer cost over £12,  (Sells
 ). The open source approach pioneered by RepRap was pivotal to the
introduction of low-cost 3D printers, with the edition of RepRap,
the Mendel, costing between just £  and £ . Around this time, a
number of affordable, consumer-orientated pre-assembled 3D printing
kits were released, including the MakerBot and Ultimaker, both of which
were inspired by the RepRap and whose designs were originally released
under open source or Creative Commons licences. While purchasing
standalone pre-built 3D printers has proved to be a far more popular route
for making consumer-level 3D printers than RepRap’s community-building
19


self-replicating machines, in , the legacy of the RepRap’s commons-
orientated approach can be seen in top-selling 3D printers such as the
Original Prusa i3 continuing to use open-source designs.
    By re-inventing the manufacturing process as a system which could be
almost infinitely customizable yet maintained from front rooms, RepRap
and 3D printers provide a glimpse of an alternative model of production
to centralized, manufacturer-centric systems that have dominated industrial
production since the s. This model that has been described as fully
automated luxury communism (Frase . whereby automation is able
to increasingly replace the necessity of labour, thereby allowing humans to
spend their time caring for others and the environment instead of working.
While a distributed community of peers producing physical objects using
self-replicating machines may sound like a utopian fantasy, RepRap and
Arduino demonstrate that the contemporary technocultural milieu affords
commons-based peer production of physical objects that potentially form
a bifurcation from a model of industrial production. It would, however,
provide an empirically dubious teleology to simply proscribe progression
from an environmentally and socially destructive model of consumer
capitalism to cybercommunism predicated upon commons-based peer
production. If the OSH movement is to succeed in revolutionizing the
model of production, it will do so via multiple pathways through dynamic
complex social structures filled with uncertainty at every step. Indeed, Chris
Anderson’s  depiction of the maker movement as a force which
heralds a new industrial revolution predicated upon invention enabled by
the affordances of networked computing, 3D printing technologies and
entrepreneurial business models demonstrates one avenue through which the
radical potential of OSH is currently being reterritorialized and reintegrated
into neoliberal capitalism.
    The degree of product customization and individualization of goods
afforded by 3D printing technologies can be understood to embody the
neoliberal celebration of unique individual consumers, and the maker
movement enables the development of micro-entrepreneurial ventures
which will potentially provide immense material gains for a miniscule
elite comprised of well-educated, technologically adept, business savvy
individuals. Equally, as Greenfield  astutely notes, economies
of scale currently produce consumer goods at such low prices, that it is
hard to see 3D printers being able to economically compete with mass-
produced products anytime soon. Additionally, the use of fossil-fuel-derived
thermoplastics in some 3D printing contributes to ecological harms in very
tangible ways that illustrate that this new design paradigm is not intrinsically
environmentally benign. As we have seen elsewhere, we can envision futures
whereby pharmacological elements of existing technocultural systems lead
to collectivized, ecologically inflected notions of resilience and growth, while
these same technologies simultaneously contain alternatives tendencies
  which see atomized consumerism, overconsumption and the reduction
of growth to economic profitability continue to dominate technocultural
assemblages.



            Externalized harms and feedback
Surveying the ecological impacts associated with the life cycle of
microelectronics hardware reveals that far from being predicated upon
dematerialization or virtuality, the networked digital economy is entangled
with the globalized systems of contemporary capitalism. As we have seen,
contemporaneous practices surrounding mining, factory labour and e-waste
frequently involve the systemic externalization of harms onto communities
and ecosystems in impoverished areas far from the affluent districts where
digital technologies are predominantly used. Addressing the ecological
costs of microelectronics requires a thoughtful consideration of the ethical
questions raised by current practices. A liberal ethics based on the Rawlsian
notion of justice and rights-based discourse would undoubtedly criticize
contemporary labour practices that depend upon enormous material
inequalities and which involve workers – frequently including children –
poisoning themselves for scant financial recompense. However, as we saw
in Chapter 2, deontological approaches are poorly situated to address
the commensurability of benefits to humans and harms to nonhumans.
Consequently, an ecological approach to ethics has been posited here as a
way of approaching the ecological costs of microelectronics hardware.
   Within this context, ethics moves from considering ‘good’ and ‘bad’ acts
based upon essentialized morals to contingent and contextual truths which
are actualized within material assemblages. This does not mean eschewing
differences between good and bad acts, but relating ethics to ethology and
the affective and material coupling between assemblages. Resultantly, an
ecological ethics pertains to practical transformations, to ways that actions
bring the dynamic relations constitutive of an ecological assemblage into
composition with those of other beings and networks, with positive actions
augmenting the agential and affective capabilities of actors, whereas
negative actions reduce them. Undoubtedly, the deleterious practices we
have encountered throughout the life cycle of digital technologies reduce
the capabilities of differing forms and scales of actors, including individual
workers poisoned by treating e-waste, local ecosystems ravaged by open-
cast mining and the net contribution to climate change that arises from
the energy costs of producing hardware and powering server farms and
the Internet of things. Crucial to ecological ethics, then, is the move from
focusing on human subjects to considering entangled relations between
human and nonhuman actors.
19


  When considering methods to address these problems via mobilizing
political action, Guattari  20) contends,

  The ecological crisis can be traced to a more general crisis of the social,
  political and existential. The problem involves a type of revolution of
  mentalities whereby they cease investing in a certain kind of development,
  based on a productivism that has lost all human finality. Thus the issue
  returns with insistence: how do we change mentalities, how do we reinvent
  social practices that would give back to humanity – if it ever had it – a
  sense of responsibility, not only for its own survival, but equally for the
  future of all life on the planet, for animal and vegetable species, likewise
  for incorporeal species such as music, the arts, cinema, the relation with
  time, love and compassion for others, the feeling of fusion at the heart
  of cosmos?

This highlights the importance of altering mental ecologies via engaging
with systems of distributed cognition. An ecological ethic, then, does not
consider the material implications of digital media ecologies to be separate
from economies of attention and modes of subjectivity, it contends that only
by reconfiguring attention and subjectivity across multiple entangled scales
can these issued be substantively addressed.
   When searching for modes of activity to reduce these ecological harms,
several promising avenues arise from mobilizing attention towards the
current ecological costs of ICTs achieved by utilizing ICTs. The surge in
global attention towards Foxconn and Apple regarding working conditions
in Shenzhen led to concrete improvements to the lives of workers. Similarly,
Huisman et al.’s  claims that the WEEE directive’s impact could
be vastly enhanced by increasing consumer awareness demonstrate that
communicative action that highlights the material impacts of microelectronics
can be a useful strategy in mitigating certain ecological costs – so long as this
goes beyond mere awareness raising to provide forms of collective action
such as lobbying for legislative action or enacting boycotts. The examples
of iFixit and Fairphone provide further evidence whereby the technologies
which are currently associated with a range of detrimental practices and
impacts are leveraged in order to provide novel outcomes that are designed
to mitigate these impacts.
   Citizen-led activism directed at raising awareness and altering corporate
actions through voluntary corporate social responsibility ventures is,
however, unlikely to ever rival the efficacy of enforceable national and
international legislation, especially within the context of communicative
capitalism. Indeed, devolving responsibility onto consumer groups and
the conscience of corporations effectively abdicates collective democratic
responsibility to the market. The success of the European Union’s RoHS
directive in removing many of the most toxic substances that were previously
  ubiquitously found in digital devices demonstrates that large-scale collective
actions enacted by states or groups of states that regulate entire industries
rather than individual corporations can significantly address harmful
practices by prohibiting them. This kind of large-scale collective action
to protect human and nonhuman systems is essential to dealing with the
globalized issues that have been the focus of this chapter; however, as Dodd-
Frank and WEEE delineate, legislation must be capable of being
enforced and supported by those it is designed to defend.
    A further area where harms can be minimized surrounds innovation
in the design process. Designing products that do not contain hazardous
substances, which can be easily and safely recycled, which are built to
last, and modular in composition, can substantially reduce ecological
costs associated with the life cycle of ICTs. RoHS presents an example of
design-orientated legislative change which has significantly impacted upon
microelectronics production practices, having effectively eliminated the
usage of some of the most toxic materials that were previously ubiquitous
within digital devices. Design has particular utility insofar as it presents
a way of not only addressing problematics which arise at the various
component stages of microelectronics life cycles, but altering the structure of
the process itself. That said, we should be wary of design-centric solutions,
such as elements of the cradle-to-cradle approach, whereby the detrimental
impacts of contemporary technoculture can simply be written off as a
case of poor design, rather than anything to do with political structures,
legislation and discourse. Here, focusing upon design becomes a way of
cancelling attention to politics, asking us to explore social and ecological
problems via a perspective drawn from engineering that problematically
assigns itself a veil of political neutrality which disguises its technocratic
ideology.
    The cradle-to-cradle movement and OSH communities gesture towards
potential future methods of creating hardware in ways which reduce or
remedy many of the detrimental ecological impacts incurred by contemporary
practices, particularly those surrounding planned and perceived obsolescence.
Indeed, the shift from a consumer-based mass-manufacturing system to the
distributed peer-to-peer architecture of OSH is one way of transitioning from
a system of commodification to an economy of contribution. OSH provides
a particularly pertinent example, as it provides evidence whereby the mode
of distributed, peer-to-peer production which originated with free software
and is claimed by information exceptionalists to relate solely to non-rival
goods, is applied to rivalrous goods ranging from washing machines to
3D printers. Additionally, by providing open access to design-related data,
OSH resists the struggle against the loss of communal knowledge of how
to create and sustain sociotechnical systems which otherwise become the
intellectual property of multinational corporations, whose economic interest
is to produce information without knowledge: the contemporary situation
  

of information excess accompanied by a lack of awareness regarding the
detrimental ecological impacts of digital infrastructures.
    Both the formation of the peer-to-peer networks which comprise
OSH communities and the strategies for utilizing networked digital
telecommunications to garner attention and raise awareness of issues
surrounding the ecological costs of ICTs, highlight the pharmacological
context of contemporary technics, whereby the devices which ostensibly
are the causes of these detrimental ecological impacts, simultaneously
present the most promising pathways to alleviating these same problems.
It is worth contrasting this picture, whereby activists utilize the very tools
they are campaigning to change, with Audre Lorde’s  12) famous
statement that ‘the master’s tools will never dismantle the master’s house.
They may allow us temporarily to beat him at his own game, but they will
never enable us to bring about genuine change.’ Within the contemporary
context, it becomes unclear as to how campaigns could effectively avoid
using tools connected to the flows of matter and energy that comprise
globalized capitalism. Even if such actions were viable, they would require
segregating oneself to such a degree that the types of communicative and
organizational strategies which are effective at reducing negative impacts
would be impossible.
    One of the central concepts explored within Chapter 2’s delineation of
an ecological onto-epistemology was the nonlinear impacts of feedback
within complex systems. The examples explored within this chapter
suggest that meaningful feedback regarding the ecological costs and ethical
implications of current practices surrounding the life cycle of hardware
are largely lacking from contemporary systems of digital media. Would
people knowingly discard functional microelectronics if they realized the
ecological costs involved in their production, energy usage and disposal?
How would behaviour change if people better understood, or more directly
felt, the impacts of their actions? While digital media systems have greatly
enhanced systems of feedback and connectivity surrounding many facets
of people’s lives, particularly regarding informational flows, there exists a
distinct lack of feedback and systemic understanding with regard to the
material consequences of our mediated actions, of how these actions always
go beyond ‘virtuality’ in connecting with multiple and complex flows of
energy and matter deeply embedded in the systems of globalized capitalism.
Part of this discrepancy in feedback, visibility and attention relates to
the communicative capacity of the agents involved: whereas we receive
an avalanche of information about celebrities, friends, co-workers and
acquaintances that are preferentially connected within our social networks,
impoverished workers, poisoned fish and carbon dioxide molecules all lack
the capacity to garner attention.
              Enacting change across
            digital media ecologies



This book has explored entanglement in contemporary digital media
ecologies, focusing upon flows of energy, matter, information, data, code
and attention. Doing so marks a departure from approaches that employ the
term ‘media ecology’ as a metaphor, either to denote environmental concern
or to examine the existence of a cultural ecology distinct from ‘natural’
ecology. The three preceding chapters have in turn explored content,
software and hardware within media assemblages. The act of separating
these areas echoes the approach to scale found within the science of ecology
which explores flows of energy and matter within and between individuals,
populations and communities, and the homologous approach to mental,
social and environmental ecologies advocated by Bateson and Guattari.
   This approach has allowed certain factors specific to each scale to be
examined, as well as demarcating particular ways that information, code and
hardware cannot be understood in isolation from one another. However, the
danger is that this method could be taken as presenting content, code and
hardware as autonomous phenomena that can be examined as discrete levels
rather than entangled meshworks. Consequently, this concluding chapter
begins by presenting two examples which draw together particular strands
that have been explored individually over the course of the preceding three
chapters. This is designed to further entrench the sense of inseparability
across these apertures, before I move on to present some conclusions which
assemble some common threads surrounding digital media ecologies.


                            Phone story
Phone Story is a game developed for use on touchscreen devices using the
Android or iOS mobile platforms by Molleindustria, an Italian collective
  

of artists, programmers and designers who produce video games to foster
critical perspectives on globalized capitalism:

  Games are an integral part of the global cultural industry, and they are
  in a strategic position in the ongoing processes of media convergence.
  These developments inhibit the political and artistic emancipation of this
  medium: every code line is written for the profit of a big corporation. …
  We can free videogames from the ‘dictatorship of entertainment’, using
  them instead to describe pressing social needs, and to express our feelings
  or ideas just as we do in other forms of art. (Molleindustria )

Phone Story takes the player through a series of events that highlight
ecological impacts associated with mobile phones. The initial scenario,
focused on extraction, sees the player controlling armed militiamen at a
coltan mine, tasked with coercing prisoner-of-war child labourers who
exhibit weariness to continue working. Next, the player controls a mobile
suicide-prevention net and is charged with catching workers jumping off the
roof of an electronics factory. The third level sees players throwing phones to
consumers, while the narration discusses the manufacture of desire, before
the final encounter sees the player take on the role of impoverished e-waste
workers, sorting materials into separate piles of circuit boards, copper wires,
glass and screens, while the voice-over discusses the fact that the majority
of mobile devices will end up being exported and dismantled in ways that
are deleterious to worker health and local environments. If at any stage
the player fails to complete the task in hand, they are greeted with a screen
informing them: ‘Don’t pretend you are not complicit’ in the actions they
are being asked to perform.
   The game is solely playable on mobile devices, thereby reminding users
that they are implicated in the events experienced via the game world through
their ownership and physical connection to a device whose problematic
material construction is explicated in the content. This demonstrates a
powerful way of implicating the user in events, exhibiting one way to realize
the mode of politicized configuration outlined by Moulthrop, in which ludic
action presents insights into complex contemporaneous globalized systems.
The way in which the game – a form of software – uses a combination
of narrative and ludic modes – what we conventionally term ‘content’ and
‘form’ – to reconfigure users’ understanding surrounding the ecological costs
of hardware and their subjective relationships with technological devices
demonstrates how contemporary media experiences can tie together ethical
and political concerns through indicating how users, content, software and
hardware form media assemblages where social inequity and environmental
degradation are rife. Indeed, what makes Phone Story a pertinent example
is how it demonstrates entanglement across scales of content, software
and hardware, making connections visible and tangible to the user, who is
  situated within this assemblage through their complicity in the game world
whose content depicts the real-world harms associated with the smartphone
they are using to play the game.
    Four days after its release, Apple banned Phone Story from their App
Store, contending that the application breached four developer guidelines.
Two transgressions pertained to objectionable content and depicting
violence towards children. The fact that this objectionable content regards
the production of the hardware used to play the game is irrelevant; in the
highly controlled Apple App Store, there is no place for satirical critique
which examines the detrimental impacts of technology or questions
current levels of consumption. The other two broken guidelines relate to
the application’s donations to charities, with Apple’s guidelines forbidding
the collection of charitable donations through any means other than the
Safari web browser or SMS, and forbidding apps which allow donations to
charitable organizations to incur an economic cost. Phone Story does not
allow users to make donations; however, Molleindustria pledged to donate
all funds received from the one-dollar cost of the application (minus the App
Store fee of per cent) to charities who raise awareness of issues explored
in the application, with the initial recipient being SACOM, an NGO whose
campaigning focuses upon labour rights abuses within the microelectronics
industry.
    Apple’s decision to ban Phone Story from the App Store created a story
which was covered by mainstream print media (Dredge . online
technology centred publications such as Wired (Brown .and PcWorld
(Mack .and numerous blogs, entailing that Apple’s decision to
censor the application resulted in the generation of a significant volume of
attention for Phone Story and its themes surrounding the ecological costs
of IT, alongside the authoritarian manner in which Apple polices content
within its App Store. Apple’s censorship ironically resulted in achieving
Phone Story’s aims, as the story regarding Apple’s decision to exclude the
App focused more attention and awareness towards these issues than had
the game simply existed within the long tail distribution of the two million
apps within Apple’s store.
    Phone Story, then, presents a case which exhibits how issues pertaining
to content, software and hardware are not distinct, but form an entangled
media ecosystem. This also demonstrates how engaging with the ethics
and politics of architectures of software and hardware can build systemic
and configurative awareness, locating user actions and agencies within
the complex globalized flows of contemporary systems of production and
consumption. Ecological costs and ethical issues surrounding software and
hardware are often best illustrated by forms of mediated content, which
raise awareness, garner attention and mobilize action via the creation of
systemic awareness of these issues. However, while Phone Story provides
a useful example which delineates scalar entanglement and configurative
  

awareness, creating a performative engagement whereby the power
structures surrounding modes of production reveal themselves via the user’s
explorative and configurative engagement with content, its impacts largely
end at raising awareness. Although $0.70 raised by each sale of the app is
donated to SACOM, this revenue would be highly unlikely to meaningfully
alter the exploitative systems of production the game depicts. The issue
may then be that although users are enraged by their complicity with these
systems through their smartphone ownership, there is no obvious outlet for
practical political action designed to affect this situation.
   We frequently become overloaded with affectively powerful images
representing social and environmental crises from around the globe in
our social media feeds, on the news and elsewhere. War, famine, natural
disasters, drone strikes, climate change, exploitation, child soldiers and
countless other crises seem to be all around us, and all too often we are left
feeling powerless to impact any of these issues and can easily fall into apathy
and defeatism. As per Dean’s critique of communicative capitalism, which
was discussed in Chapter 3, mediated systems need to go beyond just raising
awareness, instead empowering users to take action.


                     Open Source Ecology
The Open Source Ecology (OSE) project is an example which attempts to
translate awareness into action. Founded by Marcin Jakubowski, a physicist
turned farmer, in , OSE aims to create the Global Village Construction
Set (GVCS): ‘a modular, DIY, low-cost, high-performance platform that
allows for the easy fabrication of the fifty different Industrial Machines that
it takes to build a small, sustainable civilization with modern comforts’ (OSE
 ). The GVCS is composed of OSH designs for machines varying from
tractors and compressed earth brick presses to 3D printers, CNC-precision
multimachines and automobiles. The machines are designed to be modular,
cheap to construct, easy to maintain and repair, and built to last rather than
designed to become rapidly obsolete, which as we saw in Chapter are
hallmarks of OSH.
    By , OSE had created functional prototypes of about one-third of the
GVCS, including a tractor, hydraulic power unit, laser cutter, brick press and
CNC torch table. The OSE team work and live at the Factor E Farm, located
near Kansas City, United States, and are supported by a distributed network
of volunteers, some of whom attend dedicated project visits, assisting with
prototyping and manufacturing, infrastructure, and agricultural production
(the farm is a working sustainable farm using the GVCS machines). The
GVCS prototypes designed thus far demonstrate significant economic and
ecological savings over commonly deployed industrial alternatives. Whereas
a commercially available tractor and compressed earth brick press would
  retail between US$40,  and US$55,  apiece, the OSE versions cost
around US$4,  apiece (OSE . For each machine, OSE releases design
rationale, 3D CAD files, 2D fabrication drawings, exploded part diagrams,
circuit diagrams, control codes for automated devices, scaling calculations,
the physics of why the device works, and cost and performance comparisons
to contemporary industry standard machines, entailing that other engineers
or interested amateurs can copy, share, alter and improve their designs. The
documentation is all published online, allowing a distributed audience to
access and interface with their work, and to become participants in the
process of creating and refining the GVCS.
   This further demonstrates how software and hardware form an
entangled meshwork for OSH projects; without the connectivity afforded
by the internet, this type of distributed peer-to-peer collaboration would be
impossible, relegating the scope of the project from collaboratively designing
the GVCS with a distributed community and sharing their work with a
global audience, to a group of talented individuals working on a project by
themselves or with a small-scale network of personally known collaborators,
and whose results would only be accessible to this limited network. The fact
that the design information, which is crucial to the dissemination of the
project, exists as varying forms of content ranging from video documentation
to 3D CAD files also demonstrates how sharing content over the networks
of hardware and software which compose the internet is key to the viability
of the project. Hardware, software and content do not exist separately to
another, but function within a triadic relationship whereby without any one
of these scales, the entire project would effectively fail to function.
   Furthermore, the way in which the project has been funded again
highlights entanglement between content, software and hardware. OSE is
financially sustained through crowdfunding, both carried out through their
own website and via a campaign on the crowdfunding website Kickstarter
in , which raised over US$63,  from contributors. By using
various forms of media content, housed on software platforms including
their own website, the Kickstarter website and Vimeo, which are in turn
predicated upon a vast and spatially distributed hardware infrastructure,
OSE not only communicates the existence of its projects to a distributed
audience, and builds a distributed community of like-minded people to
work on the GVCS projects, but additionally raises the capital required for
the project to sustain itself by leveraging these digital technologies and their
affordances.
   Where OSE goes somewhat further than Phone Story is in implementing
an ecological praxis through producing systems whereby supporters can
become involved in contributing towards a project, which if successful
presents an eco-ethical alternative to contemporaneous forms of industrial
production. In addition to contributing financially towards the project,
supporter/participants can get involved through volunteering on dedicated
  

project visits at the farm; contributing technical expertise towards the
prototyping and fabrication processes; assisting with project management;
peer-reviewing designs; programming and coding elements of the scalable
open-source product development platform; creating documentation for
each of the designs and for the overall project (including tasks such as
distributed off-site video editing); composing artwork animations and other
design assets; and contributing to the OSE website’s forums and wikis. This
allows people with a diverse set of skills and experiences to become actively
involved in the project, and, indeed, the potential success of OSE is dependent
upon a network of individuals being able to competently accomplish these
varied goals and outputs.
   Rather than simply critiquing existing processes – encouraging people to
feel guilty about behaviours and consumptive practices without a means of
addressing them – OSE provides opportunities for individuals to create the
world they would like to inhabit, with these opportunities encompassing
the entangled scales of hardware, software and content. Consequently, the
potential problematic associated with the awareness-raising strategy of
Phone Story – that while people attain insight into the materialities of their
technology, they feel powerless to intervene in ways which create positive
change – is negated by means of encouraging interested parties to become
participants in the process of building ethically orientated alternatives.
Indeed, without the collaborative efforts of a distributed community in
the design, prototyping, refinement, documentation, funding and publicity
surrounding OSE and the GVCS, the project itself would present a near-
insurmountable challenge for the core team. As a result, OSE exemplifies
two thematics which have been central to this book: first, the way that the
domains of hardware, software and content, which are often approached
as separate and bounded wholes in fact compose entangled assemblages,
and, second, projects which resonate with the ethics and politics that
media ecology proposes require forms of praxis that not only encourage
understanding and raise awareness of issues but additionally mobilize
actions designed to enact positive changes across technocultural systems.


         Critical instabilities and lines of flight
Understanding digital media as open, complex ecosystems has been pivotal
to the methodology I have outlined. This is not merely the deployment of
metaphors to draw analogies between ecological and technical systems, but
an exploration of how media systems are nonlinear dynamic systems that
are dependent upon external flows of human attention, (primarily electrical)
energy, and matter in order to continue sustaining themselves. Without access
to these flows, media ecologies would wither and die, just as ecosystems
would perish if severed from the flows of solar energy, water and nutrients
  required for their primary producers (plants) to photosynthesize. Detailed
analysis of the assemblages and entanglements which compose media
systems allows us to explore the conditions necessary for, or conducive to,
emergence, and considering these conditions and their specificities allows us
to better understand the dynamics of media systems and their capacities to
enact changes across various ecological registers.
   Entanglement and synthesis must not, however, be understood as
meaning that anything goes. Indeed, one of the difficulties presented when
working with themes of emergence and complexity are that these tropes are
frequently applied without sufficient rigour or conceptual clarity, invoking a
macro-reductionism via vague invocations that the interplay of determining
forces at work cannot be fully understood within open dynamical systems.
For media ecology, the concept of the assemblage, which rejects both macro-
and micro-reductionisms is therefore useful in mapping of the entanglement
of lines, forces, vectors and affects which constitute media systems.
   Using the language and insights gleaned from complexity theory,
media ecology is interested in exploring the topography of attractors and
bifurcations which determine the trajectories of technocultural systems
in non-teleological ways. This mapping of potentiality constitutes the
understanding of probabilistic conditions of emergence, asking us to
consider questions in terms of points of bifurcations and critical instabilities,
moments of flux where there exist temporal openings for a line of flight
to take a system into a new attractor state. This raises questions, both in
terms of how to recognize such bifurcation points and in terms of what
kinds of organizations and infrastructures are necessary to leverage them. A
pertinent example here may again be the /8 global financial crisis, where
neoliberal models of capitalism were shaken by the thoroughgoing failure of
global financial markets to self-organize, but a lack of credible alternative
political and economic practices saw a retrenchment of neoliberal ideologies
and continuing widespread successes for political parties who had wrongly
placed their faith in the ability of financial markets to regulate themselves.1
Rather than bemoaning this as a missed opportunity, we must ask why
this moment of critical instability was followed by the rise of a politics of
austerity and increasing economic inequality rather than a reversal of failed
market-led policies? What structures were lacking to prevent the left from
effectively forming lines of flight?
   On the one hand, we may point to what theorists such as Jameson, Zizek
and Mark Fisher  have referred to as success of globalized capitalism



1
  While this is partially contested by the rise of supposedly anti-establishment right-wing
populist nationalisms as witnessed by the success of the Brexit and Trump campaigns in ,
we should remind ourselves that the political parties which headed these campaigns, the US
Republican Party and the UK Conservative Party are the traditional bastions of neoliberalism.
  

in fostering a worldview that argues that there is no alternative to the status
quo, that it has become easier to envisage the end of the world (or at least
of human civilization and a large proportion of other presently existing
life forms) than the end of capitalism. This rather monolithic approach to
capitalism contrasts with what Gibson-Graham  describe as a post-
capitalist politics that contests the totalizing nature of the imagined global
capitalist system, instead focusing upon how we inhabit a hybrid economy
where alongside for-profit, private and market-based solutions there exist a
wide gamut of alternative and non-market practices. This includes gifting,
sharing, public (state or local government-based) ownership, co-operatives,
familial networks, domestic (household) work and other practices, a list
to which we may usefully add creative commons-licensed content, FOSS-
licensed software and open-source hardware as modes of commons-
based peer-to-peer production that are prominent within digital cultures.
From this perspective, there exist a multiplicity of non-capitalist modes of
ownership and organization which we encounter throughout everyday life.
Thus, it is less a case of having to articulate a wholly new vision of a post-
capitalist economy, so much as recognizing the latent potential within the
diverse political and economic models active within contemporary hybrid
economies.
    The logic of the AND resonates with this articulation of non-teleological
alternative political and economic models existing as latent capacities
within current technocultural assemblages but which additionally expounds
that such tendencies are frequently reterritorialized and reintegrated
into dominant systems over time. As we have seen within the registers
of content, software and hardware, it is wrong to assume some form of
opposition to the market or neoliberalism is necessarily embedded within
non-market systems. Examples such as corporate open-source software
implementations, the entrepreneurial maker movement and the abilities
of social media platforms and viral marketing companies to monetize
user-generated content exemplify how within each practice there exists a
multiplicity of pharmacological possibilities. Consequently, I have some
concerns over following Gibson-Graham in labelling these practices as post-
capitalist; while they undoubtedly could help bring forth a more ecologically,
economically and socially just and equitable non-capitalist society, the ever-
present potential for reterritorialization makes a departure from capitalism
far from inevitable. In some cases, such as the corporate exploitation of free
labour, they may simply usher in novel forms of exploitation and precarity.
    Understanding media as material technocultural systems with varying
geological, geographical and ecological histories re-connects ‘cultural’
ecology to ‘natural’ ecology, revealing that in fact they are one and the same
material-discursive system, whose separation presents an epistemological
error which gives rise to ill-founded notions of virtuality and immateriality.
As Parikka  a: 34) explicates, contemporary media systems are
  responsible for the global consumption of ‘thirty-six percent of all tin,
twenty-five percent of cobalt, fifteen percent of palladium, fifteen percent
of silver, nine percent of gold, two percent of copper and one percent of
aluminium’. We could add to that list numerous other materials whose
primary contemporary usages are within the global microelectronics
and optoelectronics industries such as tantalum, europium, erbium and
neodymium. Consequently, any serious inquiry into the power relations
associated with contemporary processes of mediation must go beyond the
realm of symbolic representation and reception in order to engage with the
materiality of technocultural infrastructures if they are to conduct a realistic
appraisal of how digital media are entangled with the globalized flows of
contemporary capitalism.
   It is precisely this enormous mobilization of matter on a planetary scale
which Parikka describes as revealing the alternative deep time of the media,
one which departs from Siegfried Zielinski’s work into cyclical patterns
of invention and discovery, instead exploring the geology, geography
and ecology of the nonhuman actants which form contemporary media
assemblages. The process of literally unearthing vast volumes of materials
which have accumulated in deposits strewn around the globe over hundreds
of millions of years and deploying them to construct the material-discursive
systems which underpin the contemporary attention economy should be
contextually situated within the politics of speed of the Anthropocene: a new
geological epoch in which technocultural activity is understood as a defining
departure from the planetary conditions of the Holocene. Anthropogenic
impacts can already be seen in atmospheric greenhouse gas concentrations,
the deposition of radioactive nuclides from nuclear weapons, the production
of hundreds of millions of tonnes on non-biodegradable technical materials
and a species extinction rate which isto1,  times the background
level, a rate of change only present during planetary mass-extinction events.
While such mass-extinction events occur at infrequent intervals throughout
the geological record, with five such extinction events occurring in the
previousmillion years, the fact that technocultural actions are now
causing a sixth requires urgent political mobilization according to any
notional form of ecological ethics and politics.


                          Ecological praxis
Feedback was one of the key concepts that arose from exploring cybernetics
as one of the genealogical predecessors to ecology and complexity theory.
With regard to contemporary digital media ecologies, questions surrounding
feedback were particularly pertinent within the scale of content. Habermasian
and Frankfurt-school critiques of mass media centred upon ways that
media inhibited communication between participants, instead amplifying
  

the voices of powerful economic actors, entrenching the ideologies of the
ruling class. The internet, however, has at face value dramatically altered
this situation, allowing huge numbers of actors previously excluded from
mediated discourse to engage in discussions, leading to claims surrounding
the democratization of communication and the formation of a postmodern
public sphere in which informed citizens would debate issues as equals
within the global space afforded by digital information and communication
technologies. This process correlates to a form of feedback, whereby citizens
are able to interrupt and re-orientate discourse within mediated systems via
participatory processes erstwhile absent from mediated communications.
   As we saw in Chapter 3, while it is correct to emphasize that the internet
has allowed vast numbers of people to engage in mediated discursive
structures, it is erroneous to imply that the removal of barriers to access
equates to creating equal participants within communicative systems.
There exist various new forms of hierarchy within contemporaneous
media systems, notably those surrounding attention, cultural capital and
technicity, which have been accompanied by the introduction of practices
such as astroturfing, trading up the chain and search engine optimization
which currently ensure that privileged actors maintain a high degree of
prominence in digital discourses. Indeed, as we have seen, network effects
create power-law distributions in scale-free networks, systems in which
hierarchy rather than equity is a powerful tendency which structures and
reterritorializes digital spaces which cyber-utopians erroneously expected to
form a rhizomatic realm of democratic debate and deliberation.
   With regard to content, systemic feedback has dramatically increased,
in both volume and depth, with co-creative media such as social media
platforms affording online debate at increasing speeds, with thousands
of responses emerging within hours of stories breaking. These changes
have seen information move from being a scarce and valuable commodity
to the circulatory system of communicative capitalism in which we are
often warned of information overload. The same, however, cannot be said
concerning relationships between feedback and the scales of hardware and
software. Whereas technological infrastructures have been tremendously
enlarged and complexified, affording information processing and algorithmic
manipulation at greatly increased speeds, this has not been accompanied by
a corresponding rise in systemic understandings of the material impacts of
technology. Detrimental impacts throughout the life cycle of information-
processing technologies are routinely externalized onto impoverished areas,
nations and ecosystems, far removed from the affluent urban areas where
the technologies are primarily utilized, entailing that these costs remain
out of sight and out of mind for most consumers. Similarly, at the scale of
software there is a marked lack of feedback with regard to what practices of
algorithmic filtering, surveillance, data capture and protocological control
are in place. Users are often unaware to whom they have given private data,
  how that data is being used to generate profits, how the information they
receive has been filtered by complex personalization algorithms or how
these systems attempt to predict behaviours in order to be able to nudge
them in particular directions.
    Creating active and aware, media-literate citizens require such processes
to be made visible, allowing users to better understand the impacts of the
digital footprints they leave. This in turn raises questions pertaining to the
formation of effective aesthetic and affective strategies for implicating users
within media systems, in order to provide feedback upon mediated actions.
As we have seen, many of the most promising pathways for addressing
these problematics highlight what Stiegler describes as the pharmacological
context of contemporary technics, whereby the digital devices which
ostensibly create these negative ecological impacts simultaneously present
the most promising pathways to remedying these same problems.
    Digital literacy is not, however, enough to guarantee positive outcomes
for societies and ecosystems. Numerous entities encountered across the
book, from algorithmic trading companies to fossil-fuel lobbyists, from
public relations firms to climate sceptics are among the most digitally literate
actors. The issue is that they employ this literacy to benefit themselves to the
detriment of social and ecological justice. Understanding media as open and
dynamic ecosystems has become an increasingly popular approach as media
moved from the seemingly closed and fixed objects of the broadcast era to
open systems such as Facebook, Twitter and Instagram, which depend upon
a constant stream of co-creative user activity. While applying understandings
of dynamical systems to such media projects can prove useful in maximizing
efficiency, increasing popularity and enhancing profitability, media ecology
additionally requires the development of an ecological ethics and politics
which emphasizes mobilizing actions designed to build equity, commons and
mutualisms rather than competitive individualism and economic efficiency.
    Normative ethics – whether deontological or consequentialist – refers to
the value structures by which individuals and societies come to enact moral
decisions, questions pertaining to rights and wrongs. Adopting a Deleuzo-
Guattarian position departs from the universal notions of rights and wrongs
present within deontological ethics, instead presenting ethics as a question
relating to contextual continuums:

  Good has no more sense than Evil: in Nature there is neither Good nor
  Evil. … But because there is no Good or Evil, this does not mean all
  distinctions vanish. There is no Good or Evil in Nature, but there are
  good and bad things for each existing mode. … As Nietzsche put it,
  ‘beyond good and evil … at least this does not mean “beyond good and
  bad”’. … The distinction between good things and bad provides the basis
  for a real ethical difference, which we must substitute for a false moral
  opposition. (emphasis in original; Deleuze a:  / )
  

This provides a perspective in which good and bad exist not as abstracted
essences or moral values but as contingent truths dependent upon the
particular forms and consistencies of the matter in hand. Consequently,
ethics pertain to the ways that actions bring ‘the relations constitutive of the
agent into “composition” with the relations constitutive of another being’
(Butler  41), with good acts being those which augment agential
capabilities – such as symbiosis and mutualism – while bad acts reduce the
capacities or relations of the agents.
    Given the current forecasts for the Anthropocene, an ecological ethics
emphasizes the urgent need for modifying current practices. As Cubitt
 9) astutely observes, ‘Techne is the only route through which we can
now sense the world, most especially the part of the world’s conversations
which are not conducted in wavelengths we can hear, see or otherwise
comprehend.’ Without contemporary information-processing technologies,
humans would not have sufficiently developed understandings of looming
ecological crises to enact urgent calls to action. This is exemplified by the
fact that when considering climatic trends, we are addressing global datasets
with durations measured in decades and centuries – a temporal scale far
slower and a spatial scale far larger than those readily perceptible to humans.
    We can only understand climate change via technological mediations
which allow us to see enduring trends within noisy, chaotic and complex
global climatic systems. This technocultural assemblage includes satellites
that measure tropospheric temperatures, thermal drills that can extract
3-kilometre-deep ice cores from polar regions and the storage facilities
that preserves these geological artefacts, and the supercomputers that
are used to simulate climatic futures. Ecological ethics, then, must not
advocate abandoning mediation and technology as a means of retreating to
a romanticized past but instead requires reorienting modes of production
and consumption towards ecologically beneficial outcomes. Only through
repurposing technics along ecological lines – creating commonwealth rather
than commodities – are potential solutions to ecological crises likely to
become visible; there exists no option to return to a pre-industrial state
before technics supposedly altered the harmonious ‘balance of nature’, as
is commonly suggested by conservationist and deep ecological discourses.
    Applying such an ethics to contemporary media practices means
considering the ways in which media systems at varying scales create novel
connections and forms of commons and public good, and at the same time
involve the usage of energy and materials which close off other avenues via
their ecological costs. Ecological ethics is based on praxis rather than pure
reason, encouraging experimentation and creative interventions designed to
produce positive biopolitical impacts. It entails realizing that while there are
detrimental consequences stemming from currently produced technologies,
there are bifurcation points whereby the agential capacities of assemblages
afford meaningful positive changes to be made to these systems by using the
  offending technologies. This is not hypocrisy, but does require a praxis that
abandons an idealistic purity.
    At the level of content, this means exploring how information flows have
created the commodified economy of attention, problematic discourses of
Big Data and the context of communicative capitalism, while investigating
how ideology, cognitive frames and cognitive dissonance act as brakes which
prevent subjects from altering opinions and effecting change. At the level
of software, this entails considering issues surrounding licensing, software
development and the types of freedom maintained by the free software
movement, alongside issues surrounding surveillance, user privacy and the
contemporary movement towards the commodification of web protocols
as evidenced within HTML5. Within the realm of hardware, this involves
examining the flows of energy and matter which comprise and maintain
the physical architecture of the network society, considering the material
impacts that these flows have on ecological systems at every stage in the
life cycle of microelectronics devices. Across all scales this additionally
requires an examination of the multiple ways that powerful actors seek
to utilize existing and novel hierarchies to perpetuate privileged positions
and technocultural systems that are currently leading us towards social and
ecological catastrophe.
    Media ecology suggests that an experimental praxis with various
commons-orientated projects across the scales of content, software and
hardware is necessary if ecologically resilient and equitable alternatives
are to supplant current practices of mediation. While openness is often
proclaimed to be the defining characteristic of commons-based peer-to-peer
systems, media ecology follows Bauwens in arguing that openness alone
is not enough to guarantee ecologically beneficial outcomes. Alongside
openness, projects require commitments to social solidarity and ecological
resilience if they are to escape reterritorialization.
    Rather than an abstract mode of thought, ecological ethics emphasizes
embodied acts of engagement as a method of becoming-ethical. Praxis
additionally forms a model which resists contemporary processes of
proletarianization, which sees knowledge becomes embodied within
technologies, constituting an industrialization of memory and the
externalization of communal knowledge into corporate technics. Without
some degree of knowledge and experience regarding the complex and
distributed processes of mediation which are increasingly central to
contemporary life, we have little agency to affect and alter these assemblages
along eco-ethical lines. Ecological ethics contends that such knowledge is
derived from material practices rather than abstract speculation; following
Deleuze, we do not know what a digital assemblage can do, but through
experimentation we can produce forms and practices that can help to realize
change. ‘Make a rhizome. But you don’t know what you can make a rhizome
with, you don’t know which subterranean stem is effectively going to make
  

a rhizome, or enter a becoming, people your desert. So experiment’ (Deleuze
and Guattari  77). We cannot exhaustively know the capacities of
systems in advance of this practical activity, so the process of activism, of
creating projects, itself opens up points of critical instability and forms lines
of flight that can lead to more equitable and resilient futures.
    This does not, however, imply that such knowledge leads to mastery and
control. Indeed, a basic comprehension of complex systems entails realizing
that unilateral control is impossible. Unlike humanist accounts of agency
which relegate technics and nature to the status of Cartesian automata,
systems which are teleologically bound to particular pathways, media
ecology contends that agency is a distributed property, existing throughout
assemblages, rather than being an innate quality applying to one particular
type of node. While forms of agency differ between varying types of actor,
the diverse examples explored in this text have tried to elucidate numerous
ways that nonhuman actants realize differential forms of agency.
    Understanding the multiplicity of nonhuman agencies at play in
technological systems – those associated with algorithms, codecs, file formats,
operating systems, minerals, metals and energy supplies – has been posited
as key to understanding ways that these ensembles, along with humans,
work together to form media ecologies. Improving our understandings of
the media systems we are entangled with entails comprehending various
forms of nonhuman agency, how they manifest and what kinds of selection
pressures they apply to the evolution of technical ensembles. Stiegler’s
account of the human as being fundamentally defined by its co-evolutionary
relationships with technologies, which form an exteriorized mode of
distributed and collective memory is useful here, insofar as it reminds us
that ‘we’ do not exist outside of our technical support systems and suggests
that the varying agencies which technologies mobilize produce divergent
ethical and political imperatives. This goes beyond claims that ‘technology
is society made durable’ (Latour . which suggests that society precedes
technology (which allows the former to endure), rather than there being a
mutually constitutive co-evolution of technology and society.
    The appraisal of the positive potentials of complexity theory and
nonlinear dynamics to inform a media ecology where social and ecological
justice are major components denotes a significant departure from Dean’s
 53) claim that

  invocations of complexity induce us, the people, to think that self-
  governance is impossible, too hard, over our heads. It’s like an excuse
  for avoiding responsibility, an infantile fantasy that somehow we can
  escape politics. Global networks, neural networks, financial networks –
  if it’s all just too complex for us to understand we are left off the hook
  for our abdication of political responsibility (no wonder the education
  system has been left to rot; no wonder higher education is a major front
    of political struggle – the more people believe the lie of ‘too complex
  to understand’, the more they concede). Unfortunately, academics
  contribute to the ideological effects of complexity. We emphasize that
  there is always more that needs to be known, that there are unknown
  unknowns and unintended consequences of whatever it is that we end
  up doing. Complexity’s tagging of the multiplicity of interrelated and
  unpredictable effects presents us as so deeply enmeshed in our situations
  that we can’t assess them; we can only react, and just in time, in a 24/7
  ever faster market.

Contrary to Dean’s claims, grasping complexity is precisely what allows
for probabilistic claims to be made at scales that escape the chaotic
unpredictability of short-term noise. I cannot claim to know precisely when
high-frequency algorithmic trading will next assist in the formation of a
flash crash, but the systemic instability that algorithmic trading introduces
entails that they will continue producing such events. Similar arguments
can be made for the difference between the unpredictability of the weather
(short-term noise) and the probabilities of catastrophic climate change. The
point is not that short-term noise makes long-term prediction impossible
and should therefore lead to inaction, but that despite the inescapable
uncertainties produced by short-term noise, complexity theory allows for
precisely the kinds of longer-term predictions that are needed if we are look
beyond the immediate horizon of 24/7 capitalism.
   The focus on specificity, detail, nuance and complexity that has
characterized the multi-scale assemblages of content, code and hardware
that have been explored in Part should not be read through Dean’s aperture
of being too complex to understand, but as resisting the proletarianization
that externalizes and commodifies knowledge. My argument has been not
that specificity mandates inaction but that to assess which kinds of actions
are likely to be beneficial for particular circumstances, we require a nuanced
grasp of both the problems at hand and the weapons that can address them.
Emphasizing dynamism and complexity requires that solutions will always
be contingent, partial and open to reterritorializations, but abandoning the
reductive and utopian dream of the single glorious revolution that resolves
all social and ecological strife everywhere, forevermore, does not mean
abandoning a struggle for social and ecological justice through multiple
molecular revolutions. At the same time, the chaos and uncertainty that are
produced by noise within complex systems should also be read as a way
of resisting the type of systemic stasis that accompanies the central claims
of capitalist realism; that there is no alternative to the inhuman system of
technocratic financial markets and venture capitalist funded tech start-ups
that ‘innovate’ around exploitative models of digital labour. Rather than
providing a justification for conservative inaction, uncertainty should be a
  

call to arms for experimentation and action, of finding lines of flight that
forge more equitable and resilient technocultures.



          A biopolitics for the Anthropocene
Delineating an ecological ethic, a value system which presents an alternative
to economic determinism, cannot by itself mobilize action. For this to occur,
what is required is the transition from ethics to politics, in this case from
an ecological ethics to a biopolitics that approximates Hardt and Negri’s
definition of the term; as both the production of the conditions for life,
affects and the interactions of bodies, alongside the manner by which
biopolitical production produces new forms of subjectivity that resist
strategies of control. The emphasis on a politics concerned with ecological
relationships and affects generates relations between humans, technical
and organic entities which gestures towards futures which are rooted in
contemporaneous modes of biopolitical control but escape the shackles of
capitalist realism. As Cubitt adeptly argues  37):

  Why do people give power to Exxon Mobil? We cannot blame Exxon for
  exercising what we seem to so freely donate. … The political issue is not
  how to get wealth and power from the rich and powerful as if they were
  objects they could own. Money and influence are systemic qualities –
  money is communication, power is communication, not things that can
  be owned. What generates wealth? The ecosystem and the work that
  ordinary people do on it. We have to learn to stop giving our money away
  to nodes of the network where it is amassed, where it stops circulating,
  stops communicating. It is not a question of taking money away from
  the wealthy: it is about stopping giving it to them day after day after day.
  Ditto power and ditto mediation.

The question for a post-capitalist ecopolitics of media, then, surrounds
how to construct assemblages which re-route flows of communication,
power and wealth away from commodities and towards resilient forms of
commonwealth and public good that are designed with ecological and social
justice in mind.
   Many of the examples presented within this book outline groups and
communities building open access, commonwealth-related systems based
on peer-to-peer models of organization, forging models of production
that offer avenues for recuperating aspects of the destructive, profit-
orientated approaches endemic to neoliberal capitalism. Chapter explored
communities of scientists, computer modellers and other concerned citizens
forging participatory and didactic structures to explore and explicate various
  aspects of climate change, including crowdsourcing datasets pertaining to
the global climate, affording others the ability to conduct their own analyses.
Chapter examined ways that the FOSS community have been creating and
refining a software ecosystem predicated on providing open access, defending
user freedoms and privacy. Chapter investigated the nascent open source
hardware movement, which potentially presents an alternative mode of
production to economies of scale and industrial capitalism, predicated
instead upon distributed peer-to-peer networks and commonwealth. These
examples challenge market-based methods of production which draw clear
distinctions between producers and consumers, instead resembling Stiegler’s
notions of associated milieus and an economy of contribution.
    It would be wrong, however, to automatically assume that all commons-
creating activities represent the biopolitical constitution of eco-ethical
alternatives to neoliberalism. As the successful open source strategies of
multinational corporations such as Google, IBM and Microsoft demonstrate,
informational exceptionalism and digital commons can prove a useful
revenue generating resource for market actors. Indeed, capitalism has always
relied on expropriating commonwealth, both ‘natural resources’ such as
land, metals and water alongside cultural commons such as language, ideas
and knowledge in order to function. As ever, there remains the possibility
that attempts to create autonomous peer-to-peer networks will be subject to
reterritorializations which reabsorb such energies into the flows of global
capitalism.
    Of course, commons-based production is far from new, as Gibson-
Graham and others have gone to great lengths to demonstrate, non-
market production has always been a pivotal feature of so-called capitalist
economies. However, the successes of Wikipedia, BitTorrent, Arduino and
other commons-based peer-to-peer systems are worth foregrounding as
cases that challenge the dogma of markets and competitive individualism,
instead positing that forms of mutualism can produce economically
efficient and ecologically resilient structures. Alongside discourses of the
sharing economy, which herald venture capitalist backed multibillion-
dollar corporate ventures such as Uber, Facebook, Kickstarter and Airbnb
as the market entities which have best found ways of monetizing the
productive power of distributed networks, there exist a range of commons-
based organizations which are centrally concerned with social solidarity
and ecological resilience, rather than exploiting precarious labour and
monetizing user attention within the context of post-global financial crisis
austerity politics. That said, commercial start-ups supported by millions of
dollars of venture capital benefit from the fact that this financial investment
is itself a significant driver of the informational traffic, which kickstarts
trending activities and thus begins the consolidation process of network
effects. Within current and future contestations between communal and
corporate peer-to-peer systems what we see is a highly uneven field, where
  

corporate funding buys development time, features, and the publicity and
exposure required to become the next big thing.
   One critical area that shapes the nonlinear dynamics of this technocultural
system comes in the form of governmental and intergovernmental
regulation. As we have seen in cases such as the European Union’s RoHS
directive, legislation can be an effective way that democratically elected
representatives can act collectively to curb some of the most socially and
ecologically toxic and damaging practices which are produced by market-
led drives towards increasing profits through the externalization of costs
onto vulnerable humans and ecosystems. Equally, however, when we
consider the current steps towards the commodification of web protocols
though HTML5 and EME, we see how regulations and standards can be
a way of supporting corporate enclosures. This denotes that the national
and international legislation are key sites of contestation that mould the
configuration of digital ecosystems. While the neoliberal mantra that markets
and competition are the ideal model for driving efficiency have dominated
policymaking for the past several decades, the resurgence of democratic
socialist discourse as evidenced by the rise of the Jeremy Corbyn-led Labour
Party in the United Kingdom, Podemos in Spain and Bernie Sanders in the
United States suggests that the era of neoliberal hegemony may be drawing
to a close. If so, this may afford a regulatory environment in which state
support for cooperative, commons-based digital systems may flourish
while the most antisocial and ecologically destructive elements of digital
capitalism are curbed.
   Such a call for national and international legislative activity to support
a twenty-first-century digital commons is a clear departure from the anti-
statist strategy of exodus that Hardt and Negri  posit. However,
examples such as RoHS and the Montreal Protocol (which enacted a
global moratorium on CFCs, thereby halting and eventually reversing the
destruction of Earth’s ozone layer) demonstrate that global, regional and
national legislative actions are needed alongside activist endeavours if we are
to address the crises of the Anthropocene. There is a need not for top down
or bottom up action, but for actions that take place across many scales,
working from the middle outwards, top down and bottom up. Effective
legislation requires pressure from activists, NGOs, academics and citizens
accompanied by electoral success in removing neoliberal governments.
Applying the logic of the AND to biopolitics entails adopting many tools
and techniques to address ecological crises from many angles.
   While the agencies of digital technoculture afford activists new tools
and technologies to work with, and powerful tools at that, it must be
remembered that they still inhabit political and technocultural fields marked
by huge inequalities with regard to mobilizing resources. Crowdsourcing
and distributed systems of peers have reduced these inequalities in some
important ways, particularly in terms of barriers to access, and the ability
  to form geographically dispersed networks, but in many other capacities,
activist groups currently struggle to compete with multimillion-dollar
marketing, lobbying, research and developmental budgets.
   Although there currently exists a range of commons-based practices across
software, content and hardware, they are frequently confined to the margins
of mediated activity, largely being practised by particular hacker/geek/tech-
activist networks that have problematic affinities with the white, male geek
culture of Silicon Valley-styled capitalism. For positive action to match the
terror-inducing scale of ecological crises forecast within the Anthropocene,
these projects must grow from seeds into vibrant assemblages, moving from
their current position at the margins of neoliberal consumer culture towards
preferentially attached positions. Such a project involves the construction
of a very different digital media ecology to the current model, one that
values commons over commodities, equity over inequalities, collectives
over a misplaced focus upon individuals and resilience over the toxic short-
termism of digital colonialism and platform capitalism.
            